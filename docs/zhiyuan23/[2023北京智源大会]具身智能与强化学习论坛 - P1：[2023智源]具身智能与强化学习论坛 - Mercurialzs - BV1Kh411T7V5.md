# [2023北京智源大会]具身智能与强化学习论坛 - P1：[2023智源]具身智能与强化学习论坛 - Mercurialzs - BV1Kh411T7V5

欢迎各位来到我们今天北京志愿大会，巨深智能与强化学习论坛，我是本论坛的论坛主席和主持人，北京大学助理教授王赫，首先由我来介绍一下咱们今天论坛的一个背景，今天为什么我们要在2023志愿大会上。

畅谈巨深智能与强化学习呢，实际上我们看到在最近的一段时间，从Chat GPT引爆了语言大模型，到GPT-4引爆了多模态的，有图片和文字的大模型，我们的智能体 我们的大模型，不断地在丰富它的能力。

从能流畅地跟人类交流，到理解图片中的世界，并且同时与文字进行交流，我们再问下一步，大模型 我们的智能体应该赋予它什么样的能力，今年2023年应该说是对于巨深智能值得铭记的一年，谷歌是发布了PALM-E。

第一个Embodied Multi-Model的Large Model，让我们看到了智能体从语言到图片，到采取行动在物理的世界中，在一个具有的物理身体的机器人的身体当中，能够跟世界智能的交互。

这是从模型层面的，我们看到从Google出来的创业公司，Everyday Robots，他们的这样的一个移动机器人，搭载了大模型，可以在Google的Kitchen里头，去拿你想拿的东西。

通过自然语言跟人类沟通，并且在他们的大楼里进行垃圾回收，特斯拉的人形机器人，也再次引爆了人们对巨深智能和未来通用机器人的畅想，所以在今天我们欢聚一堂，在这里头来探讨。

从今天的大模型到未来的通用人工智能体，那么我们的巨深智能与强化学习，在这里头将扮演一个什么样的角色，今天我们非常荣幸地请到了，海内外顶尖的学者共聚一堂，有来自美国UCSD的助理教授苏浩老师。

有来自北京大学的助理教授卢宗庆老师，有来自清华大学的副教授孙亚男老师，还有来自中科院计算所的研究员蒋树强老师，那么我们就快速进入我们下面的第一个报告，欢迎来自UCSD的助理教授苏浩老师。

给我们带来第一个报告，Modeling the 3D Physical World for Embodied AI，苏浩老师是美国加州圣迪哥分校的，计算机科学与工程系的助理教授。

现任UCSD巨深智能实验室主任，他致力于建模 理解，和与物理世界进行交互的算法研究，他在计算机 视觉 图形学，机器学习和机器人领域的顶会和顶刊，均担任过领域主席或副主编以上职务。

苏浩在斯坦福大学和北京航空航天大学，分别获得计算机与应用数学博士学位，曾获得美国计算机图形学会最佳博士论文提名，截止到2023年，他的论文被引用将近8万次，他也参与了一系列知名工作。

ImageNet并主导了ShapeNet，PointNet等重要的三维深度学习的关键性工作，近三年他专注于巨深智能为核心的，下一代人工智能体系的研发，让我们以热烈的掌声欢迎苏老师给我们带来报告。

首先非常感谢王赫主席的介绍，什么意思，把他这个主屏幕和副屏幕交换回来，这个，同学熟悉了，能帮着我去调整一下，这是吧，好的，对了吧，首先非常感谢王赫主席的介绍，以及非常荣幸能够来到这个讲台上。

跟大家集集一堂，亲身地去讨论这个问题，我这个报告会用中文进行，但是我主要的教学工作都是用英文进行的，所以当我用中文讲的时候，有时候可能不太准确或者不太流利，首先希望大家能够原谅，好。

我的题目是Modernist 3D Physical World for Embodied Intelligence，这里的一个关键词就是所谓的，Embodied Intelligence或者巨身智能。

巨身智能到底是什么呢，这个词近年以来开始变得很流行，但是也许不是每一个老师和同学都清楚它的内涵，事实上在整个的研究界中，这个词的内涵也没有完全地被对齐，但是今天我想跟大家分享一下。

我对所谓巨身智能的定义的理解，以及分享一下我们组的，在这个问题上的一些前沿性的工作，好，为了更好的来讲我自己对这个事理解，我会首先说一点我自身的研究经历，来帮助大家更容易地理解这个认知发展的进程。

所以巨身智能最近被引进来呢，主要是为了跟传统的互联网智能进行一次区分，我也是在互联网智能时代进入了人工智能的研究，那么09年的时候呢我有幸参与了这个，作为主要的贡献人参与了ImageNet的创建。

在12年呢见证了，AliceNet在ImageNet上，点爆了深度学习的这么一个时代，那么在图片理解的过程中呢，我开始认识到物体关系的重要性，物体的关系实际上是在三维的物理世界中的。

所以我就对三维的视觉产生了很大的兴趣，大约在14年左右开始考虑如何去铺垫三维视觉的工作，在15年左右呢我们当时做了ShapeNet，后来又基于ShapeNet做了算法PointNet。

但是时间轴来到2017年左右的时候，也差不多是我的博士完成的时候呢，有一个点就非常值得思考了，这个点就是以当时的技术发展来看，那么对于人类定义的概念，靠足够的数据，足够多的算力，足够大的网络，看起来呢。

这个它的核心技术问题已经基本上清晰了，那么技术方案也清晰了，是不是这样人工智能或者计算机视觉，这样的问题就要被解决了呢，在我开始当教授之后呢就非常多地去思考这个问题，那么这事呢应该说答案可能不是这样的。

我们可以说在互联网智能时代，最大的问题就是对于人类已经定义好的概念，如何去识别 如何去理解，但是我们想想这个例子，大家可能很多的同学尤其是男生都有踢足球的这样一种体会，当你踢足球的时候你知道。

你可以让这个球在空中走出一个弧线来，比如香蕉球 对不对，怎么踢香蕉球呢，你要用脚的一个部分打球的一个位置，具体怎么操作你能够通过看视频得到吗，你能通过读书得到吗，他们都会帮助你。

但是你知道你必须要去球场上练习，所以这个例子就说明什么呢，像踢香蕉球这样的东西，手工标注训练数据，会是非常非常地困难，甚至有可能是不可行的，对于相当多的所谓的智能认知，它必须在做中学。

那么所谓感知 认知和行动，它们是密切地相关的，而且构成一个壁华，像这样一种认知在最近几年，在如何识别这个问题得到了突破之后，就会变得越来越受大家的重视，其实这是一个很本质的问题。

这就回到了人类认识的理性的极限在哪里，这样一个哲学级的层面上，如果要往前追溯的话，可能可以追溯到笛卡尔，那么在这个，这个认知科学界呢，六十年代也有很多人去回顾它，那么我这里回顾一个，在认知科学界。

曾经被提出来的所谓的巨身假设，它说智能是智能体，在智能体与环境的交互中涌现，是感觉运动行为的结果，所以在这种观点之下，没有交互 没有巨身，我们的智能就没有办法，跟这个物理世界真正地打交道。

当然也可能可以稍微引申一点，像这个大模型里边的相当一部分，Hallucination的问题，对吧 大家都知道这是重要问题，有一部分的这种错误，它可能必须要回到物理世界，通过验证 通过假设检验完成。

巨身智能一定是人工智能中不可或缺的一环，所以在巨身智能时代，核心的科学问题是什么呢，我认为是概念的涌现，表征的学习，但是它的基础框架是在，偶合 感知 认知和行动，这样一件的大框架下。

因此我们可以说巨身智能的最终目标，是构造像人一样聪明的，能够自主学习的这种机器人智能体，但是它跟传统的机器人科学，它在方法论上可能是有一些区别的，这个区别就在于，它是数据中心的，它关心的是如何从数据中。

得到概念的涌现和表征的学习，那么从数据科学的角度来看呢，数据在巨身智能中有非常多有意义的，或者说值得我们思考的事情，第一，巨身智能它一定是一种多模态学习，机器人通过看这个世界，来了解这个世界，就有图像。

第二，如果它打算从互联网的视频上学习，如果它打算从人类的示范中学习，那么这里就有视频和音频，第三，如果它接受人的指导，如果它需要描述任务，如果它需要去对计划产生一种规划，那么需要有语言，第四。

交互是有力反馈的，那么这里它需要触觉反馈数据，模态，最后，这个交互最终会变成某一种控制信号，因此它的输出，它必然是一种控制信号序列，这样一种模态，所以巨身智能必是一个多模态的设定。

同时也就涉及到本质上来说，各种各样的这种神经网络的架构，来处理矩阵集合图序列等等，第二个大问题是，在巨身智能中数据的获得，那么可以说从互联网智能到巨身智能，这里也是个巨大的变化，互联网智能时代呢。

总体的这个模式就是，人类制作数据集，人类做标注，那么算法建立映射，而到巨身智能时代，那么一个机器人它应该能够自主地去学习，应该能够主动地跟环境交互中来收集数据，数据收集人不只是人，更是机器人自身。

它必须能够通过历史来学习，好 这就涉及到了，这个决策论中的一个很本质的一个，一对矛盾 就是，探索和利用，exploration versus exploitation，第三点，当数据被收集到之后。

应该怎样被处理，那么我们说，数据从感知端流动到决策端，中间会经过一次对世界的建模，所以这里就产生了这样的比如说，任务驱动的表征学习，比如说除了我们要知道它叫什么以外，对物体的功能的一种理解。

比如说对于我们从来没有见过的物体，通过交互需要新的概念，包括物体的概念，包括材质的概念，或者部分的概念等等等等功能的概念，这些涌现现象怎么解决，这都是新的科学问题，最后。

对于巨身智能体的performance evaluation，也是一个困难，那么它也面临很多的，如果您是从计算机视觉来的话，这里边有些问题你过去可能并不太关心。

比如说如果要机器人能整理这么一个混乱的屋子，它要能够去处理任何一个物体，它还要能够干嘛呢，把很多的基础技能串联起来，因此我们考察的角度，比如说任务的完成率。

还有比如说有一个叫sample complexity的概念，也就是说为了达到一定的成功率，你需要做多少次交互才是必要的，最后，决策这件事情，它是一个很长的sequence。

你可能需要某一种所谓的组合泛化能力，所以所谓巨身智能，它其实是一个相对遥远的目标，它能够涵盖人工智能将来的，也许是一半的东西，另外一半当然就是不巨身的智能，它基于40年代的控制论 信息论 博弈论。

60年代的认知科学，以及近年来视觉 图形学，自然语言 机器人 还有机器学习等等的进展，它是一个综合性的任务，是一个人工智能的下一个里程碑式的目标，下面我再说一点我个人或者我们组。

对所谓的巨身智能的核心挑战的一个理解，当然这样一个理解，我的感受是它在逐渐地成为一个学界的共识，但是并不是每个人都完全同意的，好，那么在这里我来展示去年的两个工作，去年是巨身智能有很大的进展的一年。

右边这个工作是Google的工作，它是在真实世界中的机器人，它跟大模型结合起来，跟工程师提前预定义的一些操作技能结合起来，左边这个工作是我们组，今年在iClear发表的一个所谓。

Mobile Manipulation，也就是移动物体操作的这么一个研究，通过强化学习，学会了这么一个机器人去做这些事情，那么虽然这些demo看起来都很漂亮，但是它背后是有一些小秘密的，什么秘密呢。

它们基本实现的方法都是所谓的技能链接，Skill Chaining，这里我对技能稍微做一个定义，这里的技能或者叫基本技能，它是一些个短距任务的这种solver，短距基本上你可以从时间上认为是。

两三秒或者最多是四五秒这么一个尺度，那么对于复杂的事情，它总是由这些基本的东西来串联起来的，比如说我们这个work，它训练了七个基础的操作，物体操作技能，那么C-Comp，我没记错的话。

当时是40多个基础的物体操作技能，它是工程师手工设定的，但是，事实上，如果你看这些demo，它们到底能不能在真实世界中部署，那么你会认识到，Basic skill，这些基础操作技能。

它很大程度上是一个瓶颈，为什么呢，因为这个时候机器人要对付什么呢，对付复杂的物理，这里的物理既包含光学的部分，也包含运动的部分，这个视觉的挑战，也包含摩擦力，物体的转动惯例量的变化。

甚至是软的物体还是硬的物体，这类的东西，那么还有物体的形状的这种变化，还有就是当你机器人去操作的时候，它的所谓的动作空间，可能是高危的，例如你用五指，它有几十个关节，那么这些关节的控制。

这都是很困难的问题，可以说对于巨神智能来说，尤其是像机器人式的这样的巨神智能，那么我会认为，所谓的物体操作技能的学习，是其中的一个这种cornerstone task，它的基石性的任务。

它的地位就好像在计算机视觉里边的，这种物体识别一样，如果识别能完成，那么剩下的很多的事情，它都没有能力去做，剩下的很多的事情它都没有那么难，好，所以下边呢我就会讲讲我们组，有关这个。

基本的操作技能学习的，一些近期的代表性工作，这个是一个采样式的这么一个介绍，如果对更多的事情感兴趣，可以看我的主页，我会分成数据和算法两部分来介绍，第一部分数据，好。

如果我们的巨神智能也打算走大模型的路线，那么我们就需要大数据，大数据哪里来，两个基础的来源，比如真实世界或生成合成数据，当然就是指的模拟器，那么当然在真实世界中采数据是有很多手段的。

比如通过这个摇操作对吧，比如在真实世界中去做强化学习等等，但这里呢我主要想讲的是模拟器呢，有一些真实世界数据收集所不可比拟的优点，第一点是所谓的可扩展性，那么真实世界中呢。

收集数据需要很多的真实的机器人，机器人的造价是高的，而且呢很多时候是危险性问题的，而且呢也很容易坏，我们的深度学习之所以这么的成功，一大原因就是因为显卡便宜，一块显卡当年可以做很多事。

当然现在也变得受到了很多的制约对吧，如果巨神智能想大的发展，它的所谓的可扩展性低成本，它必是一个重要的事情，第二点是可负限性，那么传统机器人呢，它很多时候都是基于视频来验证成功与否的。

对于当年通过物理建模，通过控制理论的方法，这当然是可以的，但是如果我们的巨神智能，它现在是以数据为中心的，这就有问题了，我们知道对于这种黑箱方法，可重复性，那么基于大量的测试来验证它的性能，这是必要的。

但是用真实机器人这很难，因为机器人的出厂设置不一样，或者型号不一样，等等都会带来问题，因此再通过一两个video来看，是不是做了一个好的巨神智能算法，这显然是不太合适的。

那么真实世界你很难做到这么大规模的严谨的测试，这时候模拟器也是有必要的，第三点是fast prototyping，这个快速原型，那么如果一组硬件用来收集数据，但是硬件又升级了。

这个时候你的demo可能会作废的，但是在模拟器里这一点要好很多，因为模拟器的数据收集的成本要相对低一些，总之我认为模拟器是一个一次投资，但是持续开发成本会较低的这么一种解决思路，那么基于这样一种思想。

我们组长时间都在推动，机器人模拟器这件事情的发展，那么今年我们做了一个工作叫做MiningSkill 2。0，它是有关物体操作的一个统一的这么一种测试平台，现在有20类的操作技能或者是任务的家族。

超过2000个物体，以及包含了超过400万个物体操作的这种实例，那么这有一个视频来看看，这是一个简单的推椅子的任务，这里我们建模了摩擦力 建模了碰撞等等，都是有很多精细的建模的。

好 我们在计算机视觉图形学，机器人等等会议上发了很多的文章，都是去思考如何提升它的真实性，从而使得它尽可能地能够在模拟器里，我们尽可能地避免创造在真实中不必要存在的一些困难，我这儿给大家一个我们最近的。

有关触觉仿真的这么一个work，我们通过有线源方法，可以对基于形变的触觉传感器进行了仿真，并且可以证明的是通过强化学习，你可以学到一个不需要视觉，只靠触觉反馈的这样一个。

对于任意一个物体的精细插孔操作的这么一种策略，在模拟器中进行训练之后，是可以直接地被迁移到真实世界中的，当然这个工作我们也是刚刚完成，但是代码的开源还没有进行，我们会逐渐地去做这件事情。

好 下面我讲一讲算法的事情，我们不管是通过真实世界还是模拟器，假设我们已经能得到一些数据了，那么下边一个问题是我们用什么样的算法，来得到这种鲁邦的可泛化的物体操作策略，这里通过模拟器。

我们比较容易去测试它的所谓的泛化性的，比如说这么多的椅子在这个房间里，你都希望它能够被推到一个指定的位置，再一个就是所谓的组合泛化问题，作为决策你应该尽量地做到，在简单的环境中进行训练之后。

这个策略能够在复杂的环境中被使用，所谓的组合泛化，那么要点就是考虑，如何让我们的策略是更加的结构化的，那么我们考虑一种策略是，比如说用简单的神经网络，这是强化学习一直在做的事情。

比如用MLP或者CNN来表达操作策略，这个问题就在于它的泛化性是比较成问题的，尤其是组合泛化性，当然如果用所谓的Rule Based，这种基于规则的系统，来的Rule能model到的地方。

它的组合泛化性和泛化性相对都是好的，但是它不具备灵活性，也就是说它很难能够通过视力来进行学习，所以这样来看的话，我们能不能走一个中间路线呢，也就是说我们能不能考虑某一种结构化的，基于神经网络的策略呢。

这是这样一个思考的重点，那么从理论上来说呢，以后的思维应该是叫做算法对齐，Or algorithmic alignment这么一种事情，也就是说你的神经网络的结构设计。

应该能够对应你的决策所需要的一种算法的推理过程，给大家一点点感觉，比如说你在理论上可以证明，那么比如2020年我们曾经展示过，实际上图学习方法呢，可以去近似任意的动态规划可计算函数组。

同样呢近年来呢还有更强的结果，它告诉我们呢，为什么GPT这样的transformer based model这么强大，因为实际上它的表达能力的上限是，它可以近似任意的图灵可计算函数。

那么我们的决策这件事情呢，背后有很多的reason，我们当然希望追求一种图灵可计算的函数逼近能力，能够实现它，因为这个transformer这一类的大模型呢。

或者sequence modeling的模型呢，在自然语言上取得了很大的成功，所以我们呢也受到这件事情的启发，想看一看，毕竟control signal对吧，控制信号它也是sequence。

我们是不是有好的思路，能够用像语言模型一样的，建模一样的方法去弄它呢，那么我们今年呢有一个最近的工作叫做，基于思维链的预测控制，那么这里呢我们考虑的是，把这个终端控制器的速度控制信号。

也当成是一种像语言一样的token去建模，因为我们有了minuscale collect，很多的这个势力的trajectory，这使得我们有可能探索这个方向，所以这也是模拟器的一个好处。

也许它做的东西还没有一步到位，但至少呢它降低了你的实验成本，那么至少从结果上来看，我们跟之前的一些其他的这种序列建模，控制信号序列建模的方法，比如decision transformer啊。

diffuser啊等等等等相比呢，在一些很困难的精细控制任务上，是取得了很大的提高的，这儿的精细控制是，比如说我现在打算，把这个棍子插到这个洞里去，当然这里呢有很多的随机性，棍子的粗细位置都会变化。

这个洞的位置大小也会变化，但是我们有个很高的精度要求，就是只允许有3毫米的误差，在这么困难的一个task之下呢，你发现强大的大模型是有好处的，好 我下边具体说一下，我讲了，那么我们这个方法的核心思想呢。

实际上是仿照了所谓的思维链技术，因为大家如果对语言模型，有一定了解的话，大家知道语言模型之所以那么强大，能解很多的数学题，它用了一个叫，Less things step by step的技巧。

也就是思维链的技巧，它把复杂的事情呢，变成一步一步去完成的，那么一步一步去完成这件事呢，就开始逼近我前面讲的，所谓的图灵可计算的，这样一种程序的对齐的，这样一种思维模式，所以我们这儿呢。

把整个物体操作中的，这个关键状态，用它来构成这个思维链，例如说对于这个pack insertion的task，这儿的关键状态就包括，手抓住这个棍子，棍子已经跟孔洞对齐。

这个棍子已经足够深地插入到这个孔洞中，这些关键针就可以成为一种，这个所谓的操作序列的思维链，那么为什么是这些状态呢，很有意思的是，像Chad GPT这样的大语言模型，它很强的，你问问他。

所谓的把一个棍子插到洞里分几步，它是真的可以告诉你的，他认为就是这样的，但这后边有些更本质的原因，这个更本质的原因是什么呢，那就是，虽然操作序列是一个长序列，有非常多的不确定性，但是在这个完成的过程中。

总会有一些个所谓的关键状态呢，它是某一种意义下的不变状态，它是一些个方差非常小的状态，也就是说，例如说我抓一个东西，我不管手是从这边绕过去，还是那边绕过去，我总会要抓住它，抓住它的状态是本质的。

如何绕过去就没有那么本质，同时这些关键状态呢，也是具备更好的所谓的可范化的能力的，因此我们所谓COTPC这个工作的基础思想就是，在每一步我们会动态地，首先去预测这些关键针，形成这个高层的思维链。

然后呢对于每一个关键针，结合过去的一段时间的经验，再去预测底层的控制信号，这样一种方法呢，可以形成很高的一种，很好的一种效果，那么我不具体地去讲它的架构了，但总体来说呢，是我们在GPT的基础上。

把它架构上改造重新训练，然后呢变成了这样一个，控制信号的这样一个解模工具，我们在里边用到了这种，Causal的和Auto-out的这种Attention Module，我们这里边呢。

作为一个Control Signal的Sequence Modeling Tool，也有Learnable的Prompt等等等等，大家感兴趣可以看细节，最后我展示一下这个事情呢，它在模拟器的训练。

也是可以Transfer到Real World的，好，最后一点点时间，我说一，我展示两个有关这个3D的AIGC，和所谓这个巨身智能的关系，这两件事情呢都很火，但是其实呢在我的观点里。

它们的关系呢也是很密切的，为什么，如果你会认为，巨身智能将来也要用大数据，那么它的数据哪里来，对吧，如果你打算用模拟器的话，那么模拟器里边，首先要有足量的几何数据，而3D的AIGC。

它可以帮你去生成大量的几何数据，基于这样一种理解呢，我们组长时间的都在关心这件事情，那么基于尤其是最近流行的，这个神经辐射场NERF这样的东西呢，我们做了一系列的工作，想办法提高它的这个重建速度。

想办法提高它对大场景的，这种重建能力，想办法不光让它能够去，这个Capture Appearance，而且能够让它把几何材质 光照，动态 性质 结构，就是物体的结构等等等等，一系列的工作。

那么形象一点呢，我给大家看一个最新的一个东西，假定我们用相机，在多个视角拍摄一个物体，那么在不需要人干预的情况下，我们现在已经能够非常自动的，通过一个我们组最近开发的。

叫New Manifold的一个算法，在差不多一二十分钟的尺度上呢，得到一个高质量的Mesh，它具有逼真的这种Appearance，而这样一个Mesh，是可以直接拿进模拟器仿真的，当然我这里稍微说一下。

它的物理属性呢，这是一个预假设的，它不是真的从真实世界中估计的，总归这是一种手段，能够让我们帮助模拟器里的数据，同时呢我们也比如考虑，把这个Diffusion Model。

就是扩散模型和Nerve结合起来，使得我们能够从比较少的数据出发，通过这个Diffusion Model呢，放大3D数据，那么我们希望呢，是3D数据的AIGC，在接下来的几年呢，会有突飞猛进的进展。

使得我们虚拟世界的内容更加丰富，好，所以基本上我的技术部分呢，就介绍完了，那么这是我自己对所谓的，巨神智能的一个全局性的理解，巨神智能呢有非常多的应用，有很大的这种工业价值，那么它的核心呢。

我认为是要完成大数据的收集，和所谓的Foundation Model的训练，而大数据呢是很多层面的，从几何到物理，到语言和交互过程等等，那么所谓的Foundation Model呢，我的观点。

机器人的Foundation Model也不是一个，它需要感知的Foundation Model，需要对这个物理世界的动态过程的理解，需要对任务理解，这都是Foundation Model。

以及决策的Foundation Model，好在现在的每一个Foundation Model，其实研究界呢都已经开始思考了，同时在这个过程中呢，这个有监督学习 强化学习，以及呢，这个如何去对。

去实现这种算法的alignment等等呢，这也是Machine Learning里边，很活跃的一个任务，所以像这样一件事情，能够把视觉 图形学 机器人，这个机器学习统一起来，就是，把机器人统一起来。

这个我认为是，接下来的若干年，非常让人激动人心的一件事情，好，非常感谢大家的临近，非常感谢苏老师的这个演讲，那么我们由于这个时间的关系，我们把这个提问和交流的环节。

留到最后的这个panel discussion，那么我们有请这个，我们今天的这个第二位speaker，来自北京大学的助理教授，志愿学者卢宗庆老师，给我们带来从视频 文本，到智能体的策略学习。

卢宗庆老师是，北京大学计算机学院的助理教授，博雅青年学者，国家海外高层次青年人才，北京智源人工智能研究院，多摩钛交互研究中心的负责人，他的研究主要围绕着强化学习，以及开放世界的通用智能体研究，好。

那么卢老师，谢谢王赫的介绍，这个没开吗，开了，OK，刚才苏浩从CV的角度出发吧，因为他background是CV，那么去谈到这个巨声智能，那么我的background是强化学习。

所以的话我从强化学习的角度，来看一下如何去做到巨声智能，那么强化学习的成功我就不说了，但是它的问题也很多，比如说Sample Inefficient，比如说对于Breakout来讲的话。

一个非常简单的Terror Game，可能需要一千万步才能完成这个，学会完成这个游戏吧，以及对于一些Long Horizon的，Sparsely World Task来讲的话。

基本上是Impossible的，就是如果我们从Learning from Scratch，去通过强化学习算法来去学的话，我们后面会看到一些，简单的Minecraft游戏来讲的话，基本上是学不会的。

那么最重要的就是，最被诟病的一点，强化学习就是Training Set，和Test Set是一样的，它在Training的任务上去测试这个结果，那么比如说就像玩一个，Authority的Game。

然后学完这个游戏，我们的任务是比如说建一个房子，那么显然是做不到的，那么或者我们，对于今年的话我们的一些思考是说，对于强化学习来讲的话，我能不能去leverage这个Video，或者数据吧。

Video和Test，来帮助我们策略的学习，比如说现在你要去建一个房子，那可能我想在座的大多数的各位，应该不会去，或者是从来就没有干过这件事情，那如果让你去干的话，你怎么去做呢。

可能问一下Chess GPT，比如说怎么去建一个房子，Chess GPT告诉你，blah blah 一堆对吧，然后你也可能比如说，在Minecraft里面建一个房子的话，那你可能是在。

比如说YouTube上面去看一下视频，看一下别人是怎么造的，比如说先去，lay the foundation，给这个房子，然后再去造墙，等等等等操作吧，那么我们是不是也可以，让智能体通过。

文本或者是视频，来帮助智能体更好的学习策略，那么这个的话，是这次讲座里面，想讨论的一个问题，当然我们刚才也提到了，对于Minecraft来讲的话，我们有很多视频，有YouTube的视频，然后我们也有。

比如说玩家在玩视频的时候，一些对话，一些字幕，那么这些呢，都是一些数据的来源，另外一个对于Minecraft来讲的话，它是一个开放的环境，那么是对比于，这个真实的人类的世界，当然可能一些操作。

没有像刚才苏尔讲的那些，simulator里面那么的真实，当然这边的话也是，对真实世界的一个analog，OK，我想和大家分享的就是，我们这半年吧，在智原，在北大联合去做了一些事情，那么去有一些尝试。

去如何通过视频文本，比如说语言模型，然后去更好的解决这些事情，然后在Minecraft的这个环境中呢，去得到一个更通用的智能体，OK，那么第一个问题就是，比如说我们有64万个视频，对吧 玩家玩视频。

那么我们能从视频中学到什么呢，从数据中去学习的话，从数据中得到一个策略的话，最传统的方法就是offline RL，对吧 offline RL，我就是有这样一个状态，action 下一个状态。

reward 这样一个，突破的data set，然后从，通过一些offline RL的算法，来学习一个策略，那么对于视频来讲的话，它最多也就是state的一个序列，比如说一个视频的方式。

我看到一个视频的话，从SC开始到ST，那么当然了，其实本质上来讲是observation，对吧 它不是state，那么我最多看成是state，那么如何去学呢，其实这边的话就像我们，我们想做的是说。

OK 对于我们要去建房子的话，我们去看了一些视频，我大概知道怎么去做，我大概知道了解一下，比如说就说刚才踢球吧，踢球的情况下，你可能看别人踢球，你大概知道，要怎么去玩这个足球，然后你去尝试一会儿。

你可能就学会了，对吧 那么这样的话，其实一个比较standard的问题，就是learning from observation，当然我们这边加的是visual observation。

就是对于一些视觉输入来讲的话，它其实本质的问题就是，我要学一个策略pi，pi的话，pi所导致的这个状态和下一个状态的，联合的概率分布呢，和专家的概率分布是一致的，相当于我们要最小化这个。

比如说f distance，其实这个是我们能从视频中，最好的能学到的一个东西，当然如果我们只是一个offline的学习的话，我们只是利用数据去学的，没有跟环境交互的话，显然这个pi是学不到的。

对吧 因为我都，the action space是什么，我都不知道，那么我们如何去做呢，这边的话，其实我们是做了一个，像一个形式吧，这工作叫，Perturbing Static Transformer。

相当于是我们在这个，embedding层面呢，去通过一个transformer，然后去预测下一个state是什么，当然是在embedding空间，e t + 1，然后通过一个辨别器，来判别预测的。

这个embedding和真实的embedding，这样的话，对于下游任务，或者是对于online的learning的过程呢，其实这样一个判别器呢，就可以提供一个reward，来让帮助智能体学习。

当然不同于，以前的learning from observation的方法，它都是一个online的学习的过程，包括这个判别器，那么这边的话，我们是用一个，transformer的结构来。

offline的去学习，相当于，ok我现在所有的视频上，去过一下这个数据，然后去预测下一个state是什么，然后通过这个判别器的输出，来构造一个reward，让智能体来学习完成这个任务，需要注意的是。

我们在学习的过程中，在跟环境交互的过程中，我们其实是不需要，环境提供任何的reward function的，所以我们在这个过程中，我们是需要，环境提供任何的reward function的。

或者是reward，我们仅通过这个intrinsic reward，就可以完成这个任务，这是，怎么说，这是一些公式吧，我就不一一介绍了，大体的就是，刚才说的预测下一个state。

然后一个MSE loss，还有一个判别器，当然这边的话，最下面那个公式，其实是一个，在temporal层面的一个regression，相当于，我给定两个state的embedding，然后我去预测。

它们这两个之间的，time step的difference，就是从它到它，去过了几个time step，这个的话是为了，提升representation的能力的，那么有了这样一个transformer。

相当于是我通过看视频，学到了一些，学了一个reward function，然后再去online的交互的时候，通过这个reward function，来学一个策略，那么这样一个策略的话。

这是Minecraft的一些，简单的环境，那么在这个简单的环境中，我们其实可以，有一定的成功率，比如说对于前三个的话，它其实成功率还蛮高的，因为在Minecraft的环境中的话，大部分的成功率。

都是以百分比计算的，因为它有概率是，你在环境中，比如说你找不到一头牛的，OK，细心的听众的话，可以看到这些caption的话，其实就是这个任务的描述，比如说我要去挤牛奶，它其实就是让agent。

在环境中去找到一头牛挤牛奶，那么我们是不是也可以，利用这个task prompt，然后去帮助智能体更好的学习呢，当然如果我们能去，最简单的去correlate这个，这个牛啊，这个就是一头牛。

如果大家不熟悉这个Minecraft的话，能够把文本和图像联系起来的话，其实就可以帮我们去做到这一点，相当于OK，现在智能体在环境中走来走去的，然后现在的任务是去挤牛奶，那么它看到一头牛。

然后它能够correlate，看到的东西和要完成的任务的，语言的描述的话，其实可以给自己一个奖励函数，然后让它去找到，首先得找到这头牛对吧，那么为了做到这件事情，我们同样的还是从这个video里面。

去找到一些这个video和text的pair，当然是通过关键字的搜索，然后去主要是匹配字幕，我们先用这个whisper，把这个视频的这个语音呢，转成了文字，然后在文字中搜索，然后再去匹配。

对应的timestamp上面的video，然后来组成这个数据集，然后就可以通过to_tower的clip，然后去funtune这个clip，让它去关联这个文本和这个图像，那么对于，在执行任务的时候。

就随机sample一些native的pump，这样的话就可以通过，这个consent and similarity，给智能体一个奖励函数来辨别，当前这个画面下，有没有我要找的东西。

或者是这个跟这个任务相关的一些，这个object，为了更适应潜望学习的话，我们在网络层面做了一些操作，相当于去additional align这个motion，除了这个entity之外。

去additional align motion，这样的话其实，对于这样一个微生硬盘model的话，它其实在一些任务上，还是有进一步的提升吧，但是对于这样一个方法的话，我们可以看到。

这个数字就是刚才说的这个reward，我们可以看到，当智能体离这个牛越来越远，或者是距离不一样的时候，它其实，这个给的奖励函数是一样的，但是呢，我们大部分的任务，都需要智能体去接近这个牛。

比如说我要取牛奶的话，我可能走到牛羹前，然后用我的手指头，走到牛羹前，然后用桶打开一下，然后就挤到牛奶，但是如果我们只是这么一个奖励，在任意的distance上面，reward都是一样的话。

它显然没法鼓励智能体，去做到这件事情，我们想要的可能是，一个bounding box，当然我对CV不是，background不是CV，所以对CV不是很懂，但是我们想要得到的就是，这么一个类似的结果。

相当于是，我离牛越近的话，reward应该越高，一个简单的方法，相当于是，我们可以通过一个，self-supervised segmentation的方法，去做到这件事情，然后。

就是通过我们target的这个entity，在pixel中所占的比例，它其实就能刻画，我刚才要做的这件事情，就是越近的话，奖励越大，所以我们可以通过，这个segmentation的方法，去做到这个。

我们的这个，我们的这个，这个处理的方法，然后就能够，让这个pixel的比例，越近，越大，那么我们可以看到，通过这样一个，简单的方法的话，我们可以看到，对于这个，比如说这个不是牛，这个是。

minecraft里边的羊，那么对于这个羊的话，随着它在这个画面中的大小的话，我们可以看到，这个分割出来的这个，羊的这个pixel的占比的话，可以被这个，完全的刻画出来，第二列的，第二列的话。

我们可以看到，虽然那个羊特别小，但是它还是，可能被分割出来，当然有了这样一个，奖励函数的话，其实我们会比，比如说我们，仅仅用clip来做，来驱动这个智能体，去完成任务的话，比如说。

mulecow或者combat pig的话，要做得更好，但是我们刚才说的这个，我们在做这个，segmentation方面的工作的时候，其实那会儿还没有，这个segmented nse model。

在做的这个过程中呢，他们release了这个，这个SAM，然后我们就用SAM去做了一下，比如说对于这样一个，这个minecraft的场景的话，这个segmentation，其实还是不错的，就是对于。

比如说这个点打得比较密的话，它其实分割的还是可以的，但是问题是，我们需要去判断，这个羊所占的pixel是什么，那么用，需要去分辨这个的话，我们还得再接一个模型，比如说我们用光电Dino。

然后先去做一个detection，然后找到一个bounding box，把这个bounding box呢，再给SAM，然后SAM呢，再根据那个bounding box，然后再去做分割。

那这样的一个情况的话，相当于我们就可以去，链接这两个模型，让它直接在这个minecraft的，这个场景中去做到一个，实体化的分割吧，当然能够识别出羊来，但是问题是，因为这两个模型。

都是在这个real image上面，训练出来的，对于这个minecraft的，这个游戏场景的话，它其实做的并不好，比如说，尤其是，从右边数的第二列的话，我们看到分割的话，它把羊分割了整个的区域。

这样的话，显然会误导智能体，去学习这个策略，我们从这个结果也可以看到，如果我们直接，Zero-Shot把它搬过来的话，它其实并不能做得更好，刚才是讲了这个，一些简单的任务，我们都是。

比如说在环境中能找到的东西，对这个东西进行一些操作，对牛 对羊，反正这些物体吧，然后进行一些操作，那么比如现在的任务，是比较复杂的一个任务，是说我们要去，造一个熔炉，Craft a furnace。

这样的任务的话，因为这个熔炉的话，它其实在这个，minecraft的世界中，是不存在的，是需要造出来的，那么造这个熔炉的话，如果大家玩过这个游戏的话，应该知道，这是一个tactual任务。

那么它需要很多的步骤，比如说，它需要先去砍树，然后造crafting table，然后再去造一个木鎬，然后再去挖石头，挖了石头之后，你可能才造熔炉，但如果是更复杂的话，比如说你要挖钻石的话。

你可能先要造完熔炉之后，要造这个石鎬，石鎬完挖铁矿等等，一系列的操作，这边只是举了一个简单的例子，那么对于这样一个任务的话，我们如何去完成呢，那么这边的话，其实我们，对于刚才这个tactual任务的话。

我们其实看到，它其实大部分都是有一些，skill的组合，其实就可以通过，它们这个skill的组合，就可以完成，那么这边的话，我们是定义了这些skill，比如说我们，找东西的skill。

manipulation的skill，以及craft的skill，那么通过，分为这么三类skill，就是可以根据gold base的方法，去简单的比如说，用成三个策略就可以了，那么如何去。

有了这些skill之后呢，我们就可以在这个skill层面呢，去做一个planning，比如说我们现在要，造一个熔炉，我可能先去，调用这个找木头，找树的这个skill，找到树之后，把那个木头砍下来。

这些一顿操作吧，最后把这个熔炉造出来，那么进入这样的一个框架的话，其实就是这样一个显示，就是我们要完成的是复杂的任务，刚才讲的那几种方法，就是不管是vision。

language model based的方法，还是state skill的方法，还是seqrl的方法，它其实都是用来，学习这个skill的，这边的话我非常同意，刚才苏浩讲的就是，现在很多的研究的话。

它其实把这个skill这一步，给跳过了，尤其是在Minecraft里面，同样的，比如说像NVIDIA他们做的，他们直接把skill，写成一个rubix的方法，然后直接去调这个东西，但是问题是说。

这个skill本身就是，非常难学的一件事情，如果你只是写了，rubix的方法的话，相当于把人类的知识，全加进去了，那么如何去学这个skill，其实是强化学习，一直关心的问题吧，就是从解决单个问题的。

角度来出发的话，那么我们这边，也是同样的，就是我们如何通过这个视频，来学这个skill，以及通过一些vision，language model，或者是分割的方法，从视觉的层面出发，去学这个skill。

这个skill本身就是，比较难学的一件事，即使是在Minecraft，这样一个游戏场景中，另外就是我们专门分离出来，这个找东西这个策略，为什么要去，把这个策略分开，分离出来呢，其实我们可以看到。

这是两个2L的方法，一个方法是，它的距离不一样，这个最下面这一行的话，是离你的target物体，初始化的距离很近，我们可以看到，当你当agent，离这个物体很近的时候呢，它的成功率就会显著的提高。

也就是说，如果我们要去砍树，我可能要两个策略，就是先找到树，然后把树砍下来，这样的话更容易去学习，如果你的一个策略的话，你可能成功率很低，那么对于找东西，这个策略的话，其实也是一个比较重要的。

对于Minecraft的环境，或者对于一些其他场景，也是一样的，你要找一个东西，其实你就是在这个环境中，去随机的探索去找，那么这样的一个方法，你只能去像state coverage一样，在RL里面。

你可能去遍历这个state，然后去找到这个东西，那么这边的话，其实也是一样的，我是一个hierarchical policy，然后hierarchical去propose一个target的state。

或者location，然后用让这个low-level策略，然后去reach这个target，那么有了这三类策略，这craft策略就比较简单，它就是一个合成，然后呢因为这个type tree的话。

它其实比较特殊，我们可以通过Chai GPT，就把这个他们之间的dependency，把它给它抽出来，比如说我们去做些prompting，然后让Chai GPT。

去输出这么一个dependency的graph，有了这个graph之后呢，我们其实就可以在这个graph层面，去做一个interactive planning，相当于先去砍树对吧。

砍完树你可能调用砍树这个策略，然后没成功，没成功的话再去做一次planning，就像就是MPC，在scale层面去做一个MPC，这个的话是在四类的这个type tree的任务上面，做了一些测试。

这个的话就是刚才提到的方法，当然我们这边的话，也用Chai GPT做了一些测试，就告诉Chai GPT，我现在有这些策略，让它去直接去做一个planning。

而不是基于这个scale graph去做planning，可以看到因为Chai GPT的话，它在数量上面，尤其在Minecraft这个数量，我不知道在其他的上面。

在Minecraft这个合成东西的数量上面的话，它通常会搞错，所以的话它的成功率并没有那么高，就比如说我需要假设，七个木头去合成一个工作台，它经常会认为是五个木头去合成一个工作台。

另外一个就是这个MineAgent，就是这个NVIDIA那篇论文，假设我们给它一个VisionLine Models，让它去学这个跟铁矿相关的一些任务的话，它的成功率就是0%，就是没有成功。

我们后面会看到为什么它没有成功，这边的话是两个Operation Study，那么第一个的话，相当于是我们如果没有这个Finding Scale的话，它的成功率可能会降下来。

其实验证了刚才我所说的Finding Scale，其实是要单独拎出来，或者是有了它之后，我更好地对这个任务做一个更好的Decomposition，下面这个是一个Interactive Planning。

当然Interactive Planning会更好的结果，这边的话就是我刚才说的这个长城的任务，这个任务的话，大家看到右边数的第三列的话，它其实每一个任务都大概需要一万步才能完成。

而且这个任务只有在一万步之后，才有一个奖励，那么对于这样的强化学习的任务来讲的话，Learning from Scratch，即使用了VisionLine Models的reward，都不可能学会。

这个是一个节点，这个是徒手造这个铁稿的一个，完成各种任务的节点，对，刚才忘了说了一点，其实就是另外在这个Planning的step上面，我们看到对于很多任务，比如说这个任务的话。

大概需要这些scale执行120多次，才能去完成这个scale，完成这个任务，所以的话它是一个非常难的任务，对于从这个Planning的角度出发的话，OK，那这样的话我们就有了可以去完成复杂任务的。

这么一套Hierarchical的结构，当然High Level的话，我相信如果我们Challenge GPT或者Language Model做得更好的话。

它其实可以直接用这个Language Model去做Planning，但是下面要接的scale，是需要精心的或者通过Challenge学习，或者通过从数据以及视频的方法去得到，OK。

那么从这些研究中有什么启发呢，首先对于策略学习来讲的话，我们可以通过比如说Offline RL去做游戏训练，我们先通过数据的学习，通过数据来学习一个策略，或者是通过看视频。

通过刚才的方法来去学一个Reward Function，但是我刚才没有提到，其实那个State-to-Go Transformer的话，如果再加上环境的Reward的话，它的学习的效率会非常的高。

这边没有展示，另外就是对于长程的没有，或者是Sparse Reward这样的Setting的话，我们是需要一个Hierarchical的结构，对于这个Planer来讲的话，目前认为。

我认为应该用语言模型，因为它的Resonance能力非常强，所以用语言模型会是一个比较好的选择，最后提到的这个泛化性的话，同样的还是，因为策略的话它不一样的task，它可能就是需要不一样的策略。

但是对于我们来讲的话，我们的视觉我们的语言，都是具有泛化性的，因为它是统一的表示，所以的话策略的泛化性，要依赖于视觉和语言的泛化性，来实现策略层面的泛化性，另外就是我们现在在做的一些事情。

应该是现在在做的事情，一是这个Large Language Model，它都是从Task文本里面去学到的，它是没有见过环境的，比如说我们现在真的要部署一个Large Language Model。

去做Planning的话，它其实没有跟环境交互的过程或者流程，没有这个过程存在，所以的话我们要做的是，比如说在Minecraft中，我们是希望在一边跟环境交互。

得到比如说Suggestion的这样一个Sequence，那么我们如何通过这样的Sequence，来Funtune这个Large Language Model，让它具有跟环境交互的这样一个经验。

或者让它得到这样一个知识，另外一个是我们同样是在Minecraft里面，在做一件事情，我们是希望做一个Visual的World Model，希望能从视觉的层面。

把它跟这个Large Language Model结合起来，让它更好的从，怎么说，通过Large Language Model，对这个视觉的感知有更好的一个，对物理世界或者游戏的引擎来讲的话。

有更好的理解，另外是Creative Agents，这个有点Crazy，就是我们也在做尝试，就是我们如何通过告诉智能体，比如说做一件Creative的事情，比如说让它去造一个房子。

那么它造出来的房子会不会不一样，会不会有什么Diversity，那么这个也是我们目前在做的一些事情，好 这是感谢我的团队，以及提到的四篇论文吧，也是刚刚投出去的接班年的工作，另外的话做个小广告。

因为我负责多贸态交互研究中心，所以大家有兴趣的话，我们持续在招研究员和实习生，如果大家有兴趣的话，可以扫码联系我，OK 谢谢大家，好 我们非常感谢，这个卢宗庆老师的这个Talk。

我们看到第一位Speaker和苏老师，那么关注的是物理，我们不管是Simulatory的物理，还是真实世界的物理和它里面的几何，怎么能帮助到我们的巨身学习，那么我们的这个卢老师。

我们在这个Minecraft这样的一个抽象，但是又非常复杂，具有非常长程的任务，这样的一个环境中去学习智能体，怎么把一个复杂的任务拆解成一系列步骤，怎么在这之中去有这些High Level的智能。

那么一个很重要的问题，就是我们的巨身智能怎么样跟我们人类打交道，所以说我们今天请到了第三位Speaker，来自清华大学的副教授，孙亚男老师，将给我们带来交互式建模与学习，重建人类运动能力的Talk。

允许我介绍一下孙老师，孙亚男老师是清华大学的副教授，致力于机器学习，神经交互和机器人技术研究，他分别于清华大学获得学士学位，美国加州理工学院获得博士学位，并在加州理工学院和斯坦福大学从事博士后工作。

研究成果，作为独立专题写入斯坦福大学等高校教科书的，Algorithm for Optimization，曾获2020年机器人与自动化国际会议ECRA最佳论文奖。

并在中国和美国应用于神经损伤疾病的临床治疗，那么他也多次担任人工智能重要会议的领域主席，并且由于在人工智能与神经科学的交叉领域的贡献，入选麻省理工科技评论35岁以下，科技创新35人中国区榜单。

让我们欢迎孙亚男老师，谢谢谢谢王老师介绍啊，谢谢大家这个今天今天下午啊，这个现在应该同时正在进行的，我看那个时间表上还有那个郑南宁老师在同步的在讲这个机身学习，感谢大家这个来我们这个三审。

我们更年轻一点，可能这个这个讲的东西更加的这个边边角角一些，可能会对大家的胃口啊，我今天的这个报告的题目叫做这个教务式建模与学习来重建人类的运动功能啊。

所以大家看这个title里面有modeling and learning，啊我们会讲一部分modeling讲一部分learning。

那么我们的目标是restore human motor functions，所以大家会在里面会预期的会看到一点啊，我们如何来这个重建人的这个运动功能啊，那么先简单的来过一下我们这个接下来的半个小时里面。

我们都要讲哪些东西啊，首先inbody intelligence啊，那么这是一个很大的概念啊，前面呢，这个苏浩老师给了一个很好的一个这个inbody learning的一个，一个概念啊，这个卢老师呢。

把他和强化学习之间进行了一个一个一个关联啊，后面那个蒋老师的报告会把他和这个视觉还有这个对世界的构建会有一个很大的一个关联啊，那么我的报告呢。

在这个里面其实是关注其中的一小块learning to move啊，我们关心的呢，就是说我们的智能体，这个智能体主要是指我们自己如何来学习运动，然后如何来控制运动啊。

那么我们的这个在现实世界中的这个应用场景，或者说我们想做到的目标是human motor function restoration，我们帮助运动功能损伤的不足的这些患者也好，老年人也好啊，这些这个人群。

我们希望能够让他的这个运动功能能够能够有一定的这个重建和恢复啊，那么我们是AI community啊，我们会从AI的这个角度。

我们从inbody learning从reinforce learning的角度来看，说我们如何来做这件事情啊，那么我们最早采用的技术路线其实是model free learning啊。

因为后面报告里面会给大家展开来看，说我们在很多人的身体控制上，我们很多东西都不了解啊，你没有办法形成一个很好的模型和这个基础知识构建的情况下。

我们没有办法做很好的model based learning，那我们就要从model free learning开始，那model free learning呢。

我们又要从online learning来开始啊，因为offline learning需要你提前有很多的数据，这事情在很多时候是没有办法做到的啊，那么这里的红字是我们技术上的一些主要的关注点。

第一个是safety，第二个是preference，首先learning with unknown safety constraints，大家知道如果我们在这个完全的虚拟世界里面啊。

来做这些这个交互的任务，比如说刚才这个老师这个minecraft里面，他去这个他要去这个喂牛或者是要去挤奶等等的，他这个不小心被牛踢了一脚也没什么关系啊，是吧这个但是在现实世界里就不一样了，对吧。

现实世界里如果我们的对象是人的这个课题的话，你在让他做online reinforcement learning的时候，这个安全性的保证是一个非常重要的一个前提啊。

那么第二个learning with human preference feedback，啊这其实是之前这个长期来讲，这个不是特别受关注的一个领域啊，但其实从去年今年啊。

随着这个Chad GPT里面的这个reinforcement learning，with human feedback，而他的这个feedback很多时候是来自于。

human preference ranking啊，那么又开始受到大家比较多的关注，我们会看到现实世界当中确实很多时候，这些preference feedback啊。

是可以来帮助我们去更加稳健的来构建reward的这个形式，那么我们会讲一下前面的这些方法，如何在现实世界的这个人的运动功能的控制，或者重建当中来得到一些应用啊，那在此基础之上呢，我们会发现还是有问题啊。

问题在于如果我们没有model，如果我们不做这些机器人的这些建模，这个虚拟世界的这些建模，我们在现实世界里面，我们的采样效率sample efficiency啊，所以大家看到这个第三个标红的关键字。

sample efficiency怎么样来提高，怎么样来显著的提高，数个数量级的层面上的提高，这个确实需要model啊，所以我们不可避免的要从model free，走到model based啊。

那么后面会介绍一下我们在neuromuscular skeletal model，我们的神经肌肉骨骼这样一个联动的系统上，如何来构建我们自身啊，并且基于这些自身的这个构建来学习啊。

所以呢我们这个talk和这个，这个前面的几个talk的一个很好的一个衔接在于说，前面两个talk都提到了一个关键词，world model对吧，大家看嘴还叫world model啊。

那么这个talk里面不会讲太多world model啊，我们其实更多的会讲self-control，我们从self-control来入手啊，那么看到self-control model free。

能解决一些问题，但是还有很大的局限，我们回过来来看如何来self-model啊，最终我们希望我们的工作和整个embodyed AI的这个领域的工作，合在一起。

能从world model self-model self-control啊，形成一个很好的一个这个这个闭环啊，好啊那么首先那个我们从learning to move开始啊。

大家看到这个这是这是一些数据啊，这些数据有可能再过几年，大家再回来看这些数据可能会觉得它是有问题的啊，这个因为我们的biological的这些数据，本身其实很多实验得到的这个过程，本身就不是很精确啊。

比如说第一条我们说human motor function，那我们的这些motor function呢，是由最终端的这些motor neurons，直接来控制我们肌肉来实现的。

motor neurons在我们人的身体里有多少啊，对吧大家如果我我写在这儿了，因为我们时间有限就不做提这个考大家了啊，这个写在这儿大概15万啊。

15万是现在的一个这个前面的一个教科书上的一个统计的数据，那么这是大概15万个左右的motor neurons啊，那么控制了这个600多条肌肉，我不知道在座的各位大家谁知道就是人精确的来讲。

我们人身上有多少块肌肉啊，多少块骨头知道吧，应该应该是多数人知道多少块骨头，206我们绝大多数成年人注意，绝大多数成年人是206块骨骼啊，但是这个数字随着这个不同的人也会有轻微的变化啊。

那么肌肉人和人的数目也不一样啊，那么通常来讲我们看到数字说，有人说640块左右，有人说600到700块啊，这是这个对于我们肌肉数量的描述啊，那么我们会看到哎，大概15万个motor neurons啊。

那么600多块肌肉，这个好像是我们今天强化学习，尤其是我们在simulation world里面往上探一探，差不多能摸到的一个数据了啊，所以这也是为什么我们在这个时间节点上觉得。

哎这件事情可能可以做了啊，因为如果我们看第二行的这个数啊，第二个这个100 billion neurons in the brain，这也是一个很虚的数啊，大家在很多这个神经科学的讲座里面会听到说。

哎人有这个86 billion neurons，有时候说是100 billion，有时候还会说再大再小啊，因为这个实验其实没有办法做得很精确，所以到今天虽然神经科学非常火。

神经科学和我们的AI的这些连接非常火，大家频繁的会在各种的talk里面看到说，我们人有多少个神经元in the brain，但是其实这个数到现在为止还不是一个确数。

那么我们的human motor functions，关注的或者说这个影响的人群其实很多的，它可以是由于疾病，比如说这个帕金森病，一些运动功能障碍的这些疾病，可以是损伤对吧。

比如说大家这个打篮球撞了一下呀，或者是这个怎么样的这些损伤，也有可能是就是正常的这个自然的衰老aging，aging就会使我们的这个motor function，我们的运动功能出现一个显著的下降。

好那么如何来控制我们的运动神经系统，我们从embody AI的层面上来讲，我如果是一个机器人，我来看人，我如何来控制一个人的运动神经系统，这既是一个生物学的问题，同时它也是一个计算学的问题。

我们今天特别感兴趣这个计算学的问题，就在于刚才大家看到前面的，大概15万和大概600，我们觉得这个数字可能差不多可以做了，好那么我们tackle的这个方法，reinforced learning。

我们会比较多的来采用强化学习的方式，来解决这些问题，那么一个关注点是说，那么我们是在线的强化学习，还是离线的强化学习，如果对强化学习熟悉的同学会看到说，online versus offline。

model free versus model based，到底什么样的方法，是可能对于这个问题更好的，更有效的，我们后面会进行介绍，好human function restoration。

那么具体的我们在现实世界中，我们的这个实验室，我们的合作者是怎么样来做这件事情的，我们通过两个方式来去，learn to move，或者说do the motion control。

第一种方式是from the inside out，这件事情其实大家了解的相对来讲少一点，所以我稍微花点时间介绍，大家看到中间的这个，有点像个虫子，然后上面还在亮的，这其实是人的一个通用的一个脊髓模型。

这是我们做的一个人的通用的一个脊髓模型，它上面在闪的这个东西，这不是我们人的神经信号，大家看到的是一个一个通道，这些通道是我们可以来植入，人的脊髓里面的神经刺激器，那么植入的这个神经刺激器。

在可以植入到这个地方，就可以来帮助一些，严重的运动失能的这些人，来恢复他的运动功能的一部分，甚至是全部，所以我们管这条路径，叫做neurostimulation by implanted device。

这些患者在体内植入的设备，我们是看不出来的，它在外观上来讲，和这个健康人是一样的，那么它是一个from inside out，我们通过直接来code，他的神经系统的活动，使他实现一个运动功能的。

这样的一个重建，那么对应的另外的一条技术路线，from the outside in，因为我们对于人的这样的一个客体的，这样的一个操作，就是要么我们是自内而外的，要么是自外而内的。

自外而内的我们可以通过这个外骨骼的，外骨骼机器人，或者是说交互式的机器人，我们来实现这样的过程，那么这个里面其实在控制的过程当中，或者说我们在学习当中，有很多的这些挑战，我以自内而外的这个形式。

我们通过直接控制他神经活动，来使得他的运动功能得以重建，来作为一个例子来看里面的一些问题，What are we exactly stimulating？，当我们在里面植入这样的一个控制器以后。

如何来刺激，如何来控制，那么这件事情其实是比较未知的，我们植入以后，他面对了大量的附近的神经元，到底哪些神经元被激活了，哪些没有被激活，哪些有连带这些响应，这个东西不知道，医生也不知道。

神经科学家也不知道，我们做这个东西我们也不知道，第二类问题，What is the mapping between，electrical stimulation and motor function？。

我们的刺激到底和最终的运动功能的，构建和输出之间是一个什么样的关系，我们的这样的一个信号，和本身大脑对于神经运动功能的，coding，还有脊髓自己对于运动功能的，coding到底是怎么一个关系。

那么再进一步，How to achieve motor function restoration？，我们如何来通过这样的方式，来实现一个好的刺激，我们也不知道，所以所有的这些问题基本上都现在。

我们不能说我们一点也不了解，但是我们了解的程度，没有办法使我们充分的实现一个，model based learning，或者model based optimization，这样的方式。

所以当我们面对这样的，现实世界的这个问题，Restore motor functions，without clear understanding of mechanism，我们不知道背后的机制是什么。

我们不知道前面的那些问题的，精确的答案是什么，那我们就要采用model free的方式，我没有办法给他一个很好的model，那这个时候我要用model free RL，同时呢。

历史上没有很多数据来告诉我，像玩游戏一样，前面的人是怎么玩的，我去观察一下，我们很多的这样的患者，很多这样的疾病，很多可能现在我们进入老龄化的，这些老年人群，新出现的这些问题。

它是一个online出现的，所以也就需要我们online来进行解决，那么我们就需要，第一个入手的方式，model free online reinforcement learning。

那么在这个里面有几个，也是有几个critical challenges，就是刚才我们几个哄字提到的，第一个safety，你在online reinforcement learning。

尤其是在和人打交道的这些，online reinforcement learning过程中，如何来保证安全，你如何在这个过程中来获取reward，我们的这些reward，大家知道你让人来给你。

填一个量表，打一个评分的话，很多人是对于这个事情，评分质量不是很高的，而且很多的东西是没有办法，很好被量化的，我们后面会看到，这个外五格控制的这个例子，有些时候你没办法，给一个非常精确的量化的评估。

这个时候human preference feedback，可能是我们仅有的，可能能用的这些评估的方式，efficiency，我们如何来尽可能的来提高，我们优化的这个效率，那么这本身。

我们会在model free的情况下，用算法来想办法把它推到极限，但是在后面一小部分的talk里面，我们看到最终解决它的方式，很可能还是在构建模型，以及基于model based的方法来实现，好。

我们model free online reinforcement learning，其实它的这个最核心的一个本质，我们reducing，把它约束到最核心的一个问题上。

还是constraint optimization problem，这个构型大家非常熟悉，maximize function f(x)，然后我们面临着下面的这些constraints。

这是一个非常经典的，constraint optimization problem，那么在我们online reinforcement learning的时候，会有一些情况下，它会要求你的每一步采样。

大家看，我在online来做这个constraint optimization problem的时候，你是每一次t=0，1，2，3，你每一次这个x(t)，你都取一个值，你来看一下这个f(x)是多大。

g(h)本身是多大，是不是符合条件，对不对，这是我们做constraint optimization problem，但如果你这个东西是真实世界里面，在现实的人或者机器人上来做的话，那你要确保的是。

整个在优化的过程当中，这些constraints一次都不被破坏掉，这是安全约束的，强化学习方法的这些要求，那么另一类就是说，在这个过程中，如果我没办法得到很好的函数值。

我只能得到human preference feedback，告诉我哪个好哪个不好，那么这可能也是一种方式，那么learning with unknown safety constraints。

这件事情为什么会很难，我们回到教科书来看一下，因为从经典的reinforced learning，这个方法的构建来讲，reinforced learning。

它是一个evaluation improvement，一个试错和改进相结合的，这样一个迭代优化的过程，而如果我们的环境当中，存在未知的这些安全约束，那未知的安全约束，其实破坏的是什么。

破坏的是evaluation，你没有办法非常有效的，非常充分的去试错，因为你可能试错一次，你的机器人就摔断了，或者是你试错一次，你的这个人的这个用户就觉得说，我不能够再继续了，所以没有办法充分的。

evaluation试错的情况下，整个这个loop就被破坏掉了，所以我们说，unknown safety constraints，break the reinforced learning loop。

它其实把RL的这个基本的架构，破坏掉了，那么我们怎么样来解决这个问题，其实也是过去的将近十年的时间里面，一直在这个方向上来努力，说我们如何来构建一个，在线的，安全的，强而学习的这样的一系列的方法。

那么大家看到说，由于前面的这个结构被破坏掉了，那你就不能再采用传统的，exploration and exploitation，这样的一个这样的一个巧良关系，大家前面在苏浩老师的那个幻灯片上。

看到过exploration and exploitation很好，没问题，在simulation world里面，我们不需要考虑安全性的问题，在现实世界中，我们需要考虑那怎么办，那就要再加上一个东西。

我们叫做safe expansion，所有的你的exploration and exploitation，一定要在一个安全的区域，一个安全边界内来进行，而你的算法一定要怎么样呢。

你的算法一定要在exploration and exploitation，的这个过程当中，最好能去扩大你的安全边界，你一边扩大自己已知的安全边界。

一边在里面来做这个exploration and exploitation optimization，那么这就能够实现一个在线的，安全的这样的一个优化的方法，那么如果大家对于这个方法感兴趣的话。

就这个方法的第一个工作，是写在这本书的16。6的这一节里面，那么后续我们还把它进行了一系列的拓展，好，前面是我们说这个如何来解决，safety constraints。

尤其是unknown safety constraints，这个问题如何来解决，那么另一个问题我们也说了几遍，preference怎么样来解决，这个preference或者说人的偏好。

也是在我们的实际的应用过程当中，我们会发现这是一个很实际的问题，比如大家看到，大家看这个图可能比较陌生，但是这是可以植入人体的电极长成什么样子，上面的这个红蓝点代表说，我把哪个设成阳极哪个设成阴极。

所以大家看到这是两个不同的neural stimulation构像，如果有同学对于脑筋接口感兴趣的话，会知道说脑筋接口我们分为，如何从大脑里面把信息读出来，和如何往我们的神经系统里面去写信息。

这就是如何往我们的神经系统里面去写信息，那我怎么知道哪个，比如说这个刺激这个写入的方式是98分，对吧这个是什么89分等等的，这样的分数是很难给的，在现实的我们的应用过程当中。

什么样的判断或者什么样的反馈，什么样的reward是比较好给的preference，所以我们的问题很多时候就转化成。

online reward maximization by preference feedback，我的用户我的患者可以告诉我说，当你面临两个选项的时候，你是选择A还是选择B哪个更好。

那么这其实是一个系统的理论化的来，构建和解决这类问题的一个初始方法，叫during bandit problem，Caltech的岳一松教授，09年他和Cornell的导师们一起。

一起做的这样的一个工作，那么我们其实在这个上面往前又进一步，考虑了一些其他的问题，如果我仍然是面临optimizing user preference，那么我把人的这些反馈在during bandit。

这个setting下面来进行构建，那么我面临一些问题，numerical feedback unreliable，这个时候我们用pairwise feedback。

这就是during bandit setting，那么还有一个新的问题，each preference yields a single bit of information，大家刚才看到说我两个不同的。

这样的这种选项，那我如何来这个，我比较完了之后，我如何来推断说，其他的另外的一个刺激的这个选择，对于这个人好不好呢，那么这是我们的利用Bayesian preference model。

我们能够把空间的连续性和这个输入的，input space和action space之间的，这些关联性能够进行一个构建，那么这是我们当时提出来的，这个可用的一个算法，以及说这个可以被证明的。

这样的一条技术路线，那么这是我们2015-2016年进行的工作，大家看到说我们其实是convergence with self-play，这是一个two-agent problem。

那么你一个算法可以跟自己，如果它是一个randomized algorithm，它可以dealing with yourself，那么而实现一个这种通过self-play。

实现optimization的这样一个方式，大家现在回过头来来看，说2017-2018年我们充分地接受了，这个alpha go zero，当时提出来说我通过。

这个self-play的方式可以来学习下围棋，那么我们也可以通过self-play这样的方式，其实以一个这个有理论保障的这样的一个方式，去解决online optimization的这样的问题。

那么来自于人的这些preference feedback，好，那我们来看说，整个的这个前面的这个方法性的工作，我们如何是在这个现实世界中，能够得到一些应用，那么好，左边state space。

那我们你可能是患者也好，可能是你目标的这个人的这个对象也好，那么我们希望他能够恢复相应的运动功能，右边是我们自己的action space，那么这就是一个典型的一个前网学习。

或者说在线决策优化的这样的一个流程，我们通过optimization algorithm，我们来看这个结果能够实现怎么样，因为整个how to stimulate。

其实我们最终把它划归成一个searching，and optimization over large action space，你在一个巨大的一个动作空间里面，那么如何来有效的来优化。

来得到一个好的站立，行走或者说抓握的一个结果呢，那么这是我们的一位患者，他由于脊髓损伤，导致完全没有办法控制自己下身的，任何的一块肌肉，但是他在神经刺激的帮助之下，大家看到说他穿蓝衣服的这一天。

其实我们找到了对他还可以的参数，但是效果不是特别好，那随着optimization的过程前来持续，大家会看到说我们后面就能够找到，对于他非常好的这样的刺激参数，也就是说人的控制的参数。

那么这个学习到的参数，它基本上可以靠这个东西来实现一个，完全独立自主的体重支撑的这样的站立，他仍然需要他前面的这个架子，他仍然需要这个架子来保持一定程度的平衡，因为平衡功能直到今天都是一个。

非常非常难解决的问题，但是他已经可以靠自己的力量，通过我们的神经控制这样的一套系统，能够让他去站起来，那么还有一些相应的实验，就是说来恢复高位结痰的人的，手部抓握的能力，比如说他坐在轮椅上。

他至少开了刺激以后，他可以自己抓起话筒，他可以自己来拿起来这个控制器等等，那么在行走方面，在这个自外而内的这个控制方面，我们其实可以来通过这样的一个，其实是同样的底层的方法论。

我们可以对它来进行这个gate training和gate control，这是我们在Caltech的这个合作者一起，我们如何来学习一个外骨骼的优化的一个步态，因为这件事情也是一个。

你的用户不同的人穿上这个外骨骼机器人，大家会看到今天外骨骼机器人，也是一个比较这个，比较关注的机器人里面的一个领域，就是你不同的人穿上以后，你喜欢的这些步态是完全是不一样的，那么如何来使得他能够。

自适应人的自身的偏好，这个东西很难通过，我人的这个量化的反馈给他，那么我们需要这个preference feedback，那么前面是我们通过外骨骼来自外而内的，来帮助人做步态的training。

那么我们也进行了说，我通过一个这个机械臂，自外而内的来进行人的这个手臂运动功能的，这样的这种恢复的训练，大家知道这个机械臂，机械臂在今天尤其是在中国，这个我们能够买到机械臂的成本，是在快速的在降低。

那么很可能再过不长的时间，大家获取一个机械臂的成本，跟获取一台手机的这个成本，可能都差不多了，那么在这种情况下，比如说家里面有需要复健，这个运动功能受损需要复健的这些老人，或者其他这些情况下。

那么是不是我们能够有一个，这个新一代的这些人机交互的方式，那么前面这些其实都是model-free online learning，能够带给我们的一些可能的方式。

但是我们会看到说model-free online reinforcement learning，仍然下面我们说safety的问题，一定程度上有解决，reward我们通过preference。

efficiency这个问题，model-free online learning，这个局限就是会非常的大，所以我们会从model-free，走向model-based learning。

of human motion control，我们如何来对于人的运动功能，来进行一个更加有效率的这样的一个学习，控制这个运动功能，一个更加有效率的学习，这是我们在构建这些模型时候的这个目标。

所以我的这个研究组也是过去的几年，花了相当多的功夫在这个事情上面，develop high precision personalized spinal cord model，大家看到左边的这个。

左边这个是一个个性化的人的脊髓的，其中的一部分阶段的一个模型，它的这个capability，它的这个能力怎么样，我们会在后面看到，那么同时我们还要构建一个更加精确的，尤其是更加完整的人的骨骼肌肉系统的。

这样的模型，刚才问大家我们其实都不知道，人到底有多少块肌肉，我们做到今天，我们也没有办法说清楚，人到底精确的多少块肌肉，因为人和人就是不一样，而且不同的建模方法也会不太一样。

好我们先从这个神经的这个建模来看，我们是通过神经建模，我如何来说自己的模型建的准不准，这其实是一个非常非常tricky的一个事情，因为运动功能的这样的一个建模，很多时候我们这个测量比较直接，那么神经呢。

也是因为我们有比较好的条件，和我们的合作者一起，可以在这些患者的实验的过程中，我们做一些相当的数据的采集，那么我们一个人的神经系统的这样的一个建模，我就可以通过，比如说大家看到。

这儿我一个电极的触点点亮了，点亮了以后呢，我的模型可以告诉我说，不同的种类的神经元，在这个周边的发放的这个发放率是怎么样，更进一步的相应的这些肌肉本身的，这个活动的这个活性是怎么样。

好 那既然能做这件事情，我就可以根据它，其实来做一个closed loop，大家注意到这是一个新的closed loop，它是用来做什么呢，它是用来来优化我们植入的这个电极，到底应该长成什么样。

那么今天我在这里不展开说，整个的这个优化的过程，本质上它是一个Bayesian optimization，for the design of the electrode rate。

因为本身我们的模型的计算的复杂度，是相当的高的，我没有办法来保证说，我把所有可能的这些空间里，所有可能的电极的设计，我都放进去，让模型把所有可能的这个刺激的，这个结果都跑出来，所以还是要采用一个。

这个在线学习的方式，我们去主步的去调整，这个电极的这个参数是怎么样，好 那么我们可以对电极来做优化，我们把优化后的电极，可以来植入患者的这个体内，那么对于它来进行个性化的，这些模型的构建。

所以大家在右边看到的这两个，这就是一个特定的患者，一个特定真实的人的，他的我们叫数字孪生也好，我们管他叫做这个，他的embody这样的一个，这个模型的构建也好，那么它可以干什么，它可以来帮助我们预测。

一个使您刺激，到底产生了什么样的肌肉活动，大家看到左边，这是我们关注的这个，和站立行走相关的几个主要的肌群，那么好，大家看到这是一个我可能的刺激模式，我通过这个刺激模式，后面整个的过程全部是数字化的。

我通过这个数字化的这个模式，我就能够学习出来说，我这些不同的肌肉，两条腿各六条肌肉，它们的激活程度是怎么样的，好前面是一个例子，这里是三个例子，不同的这些刺激模式，大家会看到说，我这个刺激模式。

这些肌肉激活了，这个刺激模式看着，也点亮了不少这个电极，但为什么没有肌肉激活，这个时候我们的真实的实验，和我们模型预测出来的之间，有相当高的这样的这个吻合的程度，那么这也是一些相应的，这个统计的数据。

好那么前面来验证，我们神经建模的这个准确性，那么我们在肌肉建模上，我们也是在过去的几年里面，我们的组里面来做了，Full Body Human Musculoskeletal Modeling。

And the Control Based on this Model，我们进行了一个比较全面的，人的这个整个肌肉，还有这个运动系统的这样的一个构建，大家看到我们有超过150个。

这个Rigid Body Segments，超过250个这个Joints，超过800个这个整个肌肉的这些单元，大家注意这个单元不是我们的肌肉数，这个单元是我们在数字模型里面，可以把它拆开的这些小的单元。

那么基于它呢，我们就可以来进行一个，比较高质量的这样的一个，这个人的运动功能的这样的一个描述理解，以及基于它的控制，所以这是一个快速的展示一个例子，我们对手的建模，对脚的建模。

在这里因为我们是AI Community，我就不放那个解剖的那些结果了，就都是要和人的真实的，这些解剖的结果去进行一定的对应，好那么这样的一个高自由度的，一个高复杂度的这样的一个模型，我们也是可以通过。

Hierarchical Reinforcement Learning，大家看到Hierarchical Reinforcement Learning。

这个keyword本身也在前面两个talk里面都出现了，它确实是一个我们控制一个，高维的空间的一个有效的这样一个方法，那么它也是我们从world model到self model。

最终来实现self control的这样一个路径，好最终总结我们在这个路径下，我们核心关注的是learning to move。

然后我们从model free online reinforcement learning，到model based learning of human motion control，这样的整个的路径。

大家如果感兴趣的话，更多的内容可以在我们的网页上可以看到，好谢谢大家，这个非常感谢隋老师的精彩的这个talk，应该说我们今年人形机器人，是一个非常火热的话题，我们没想到今天我们的这个talk。

竟然能把人类的这样的运动和机器人，外骨骼还有强化学习结合在一起，应该也是非常大开眼界，那么下面我们有请这个蒋树强老师，给我们带来巨身智能中的视觉导航，那么我们讲今天的巨身智能，里头涵盖了很多重要的任务。

应当说在视觉驱动下的导航，就是其中大家研究很广泛，而且非常重要的一个任务，那么让我来介绍一下蒋树强老师，蒋老师是中科院计算所的研究员，博士生导师 国家洁青，CCF多媒体专委会秘书长。

中国人工智能学会智能服务专委会副主任，主要研究方向是图像视频等多媒体内容分析，多模态智能技术和食品计算，主持承担了科技创新2030新一代人工智能重大项目，国家自然科学基金等项目20余项。

发表论文200余篇或授权专利18项，先后获得中国计算机学会科学技术奖，中国科学院青年科学家国际合作奖，CSIG自然科学二等奖，吴文俊人工智能自然科学一等奖，北京市科技进步二等奖。

让我们热烈欢迎蒋老师的报告，(掌声)，好的 谢谢王老师的介绍，也很高兴能有机会到这里来，跟大家来交流一下，巨神智能中的视觉导航技术，听了前面三个报告，感觉压力很大，大家做的都很好，然后我们实际上是从。

巨神智能中的下肢的一些路径规划，和它的一些行为，然后开展了一些研究工作，首先是研究背景，这一页我想就不用讲了，大家也都知道，然后从巨神智能角度上来说，可能大家关注的很多的，实际上可能从人工智能。

实际上关注很多的是离身的智能，我们做一个机器学习的算法，然后做一个这样的问答等等的话，实际上是一个简单的输入和输出，但是巨神智能实际上是要有一个本体，然后它在环境中进行一个交互。

所以它应该是一个巨神化和情境化，巨神智能可以和真实的世界交互来完成任务，这里我拿一个语言为例，就是我们说的任何一句话，它实际上都是在环境中，或者是在情境的上下文的下面，对它的理解才有一定的意义。

所以在很多情况下，就是我们的人工智能的这些任务，这样的一些技术，实际上都是和我们的实际上下文，都是紧密地相关的，当然从巨神智能这个角度上来说，它的内涵可能是更加地丰富，它实际上是要有一个巨神的体验。

有一个巨神的反馈，有一个巨神的学习，有一个巨神的增强，然后来完成一些跟巨神有关的任务，就像我们小孩子，我从来没有见过这个东西，我把它不断地来学习来提升我们的能力一样。

当然跟它相反的就是我们这样的一个简单的输入和输出，这张片子实际上是从最近的一个Survey的文章中拿过来的，我来做一下示例，我们现在很多做CV的，可能都是给一张图像，然后我们可以做分割，做检测。

做分类都可以，包括语音分割，但是巨神智能实际上它更强调一个动态性，就是我们在一个环境中，然后来不断地观测，不断地决策，不断地来得到反馈，然后再完成我们的一些相关的任务，当然从这个角度上来说。

它有一个摩拉维克的这样一个悖论了，它的一个基本的意思，就是说我们可能很多这种简单的输入输出的这样一些问题，可能我们可以回答得很好，但是一旦涉及到行为，就是感知认知和行为一旦结合在一起。

可能现在的人工智能的能力，可能连个一两岁两三岁的小孩子可能都还达不到，它实际上需要这种巨大的计算资源，需要我们对很多这种任务的复杂的结合等等，刚才苏老师也讲Internet AI。

这一块实际上是非常火热的，它实际上可以说是离身智能的一个典型代表，实际上它也非常伟大，我觉得也非常有用，但是现在大家也逐渐地在关注巨身智能，可以说是和Internet AI。

我认为是和一个并驾齐驱的一件事情，当然它可能未来的空间可能会更大，然后我们想象的这样的一个可能性可能会更多，当然给我们带来的挑战也更大，至少我现在认为，从巨身智能这个角度上来说，可能才刚刚开始。

可能很多任务才刚刚被设定出来，或者可能才刚刚被初步地设定出来，因为在我们这种复杂的真实的物理世界中，怎么样让这种智能能力真的能够满足我们人类的需求，或者说能够达到人类的这样一个能力。

就基本的这样一些行为能力，我觉得还有很多的工作实际上需要做的，从这块来说，巨身智能它实际上还是一个多种任务相结合的这样一个事情，我们需要做听觉，需要有视觉，同时也需要有语言的理解，有记忆，也有导航。

然后有动作，包括有反馈，当然现在我们实际上还是在一些具体的任务上来做，包括我今天汇报的视觉的导航，包括很多我们三维物体的理解，包括可能视觉和语言的，就是叫interactive的QA等等的话。

实际上都还是很多具体的任务，但是真的像人一样，这样一种相当于能够全面的智能能力，实际上还有很长的路要走，这一块实际上是，我们肯定是需要有一个智能体来做支撑的，这一块实际上包括人形机器人。

包括很多机械臂等等的话，实际上现在都得到大家越来越多的关注，这也是我们可以开展这个方面的一个重要的基础，当然机器人可能只是它其中一个重要的方面，但是也不仅限于此，包括天上飞的，包括水里游的，对吧。

包括可能我们周边的，可能其他的一些东西，有可能都会对我们，它都可能会有一定的具身性吧，当然从现在的研究来说的话，实际上我们就是说的low一点，从发paper的角度，可能现在大家还是在这个虚拟环境中。

可能做的比较多，这个可能也是由于我们现在，得到大规模的训练数据，不容易，然后得到的这种多样性的反馈，和交互，也不容易，同时构建这样一个可以同台竞技的，这样一个评测的标准和benchmark，也不容易。

所以的话，现在就是很多的工作，实际上都是在这种虚拟环境下来做的，但是我们肯定还是需要从虚拟环境走向，真正的这种我们的实际环境，把它怎么样迁移到真实环境中，也就是seem to real。

这一块实际上也是目前学界关注的一个重点，当然这一块实际上是特别的火了，我们实际上关注这方面的研究工作，差不多是，当然对这个问题了解是比较早了，但真正的就是着手开始做，差不多是在19年左右吧，19年左右。

但是呢，当时也没想到后来会这么火，但是今年的话实际上大家是关注度特别高，当然同时呢，这里有一个就是相当于其他一个关于，居身智能的一个调研的文章了，说这个方面是发的论文是成一个指数级的，指数级的增长。

当然这里只是一个数据了，但在我看来呢，我觉得这个事情实际上确实需要得到大家的关注，因为我们真正讲的这个智能，它肯定不是一个点上的智能，而是一个各种能力相结合的一个智能，或者是两三个能力。

或者一个综合的能力的相结合的一个智能，这个方面的话呢，它肯定是离不开我们的感知，我们的认知，包括我们的行为，特别是我们的行为，实际上它是反馈我们对环境的理解，和对我们一些推理能力的一个重要的方面。

就举个例子，就是我们可能跟一些人在交流，我们这个，有的人可能说的话可能让我们高兴，我们就会笑，让我们不高兴呢，我们可能就会沉默或者稍微的皱皱眉，这种的话实际上就是我们对这个话的一个理解。

然后呢我们学习这个话之后呢，我们肯定还有我们的反馈，所以的话呢，我个人认为这个人工智能的话，它的未来的发展方向，这个巨神智能是必不可少的，当然现在的话呢，实际上在国际上也有很多这方面的研究。

特别是在这种模拟器上，包括一些相关的任务上，不管是它的上肢，它的下肢，它的这种和语言相关的一些交互等等的话，实际上都有很多相关的工作，然后呢，这里是有一些Benchmark，就是像什么AITOL。

就是做导航，包括现在最新的右下角那个Proctol，它有一万个房间，实际上也是做导航，还有一些其他的，这里呢实际上也就像我们做CV的，那个Coco，ImageNet，实际上呢在这上面玩得比较赚了之后。

也可以得到一些结果，然后的话也可以发发论文，实际上就是现在也很卷，我觉得就是如果要达到这个SOTA的话，也不容易，但是呢毕竟还是现在，可能才做的，可能现在人没有那么那么多吧，已经开始有一些了，当然了。

这个事情未来肯定是要满足我们真正的需求，真正的需求包括它的下肢能力，它的上肢能力啊，这里呢实际上是举了一些例子啊，时间关系呢我不展开的说了，就举一个例子，就是像这个归纳还原，就是将来的话。

就是在一个房间里，如果有一个东西，它不应该放在这，就举个例子，这个东西它就不应该放在这，它应该放在这个地方，那么你就可以把它找到，然后把它放到它该去的这样一个地方，在这个里面呢。

实际上就涉及到一些综合的能力，包括它的一个导航的能力，它的视觉导航，它的记忆能力，以及它的这样的一个，相当于移动和它的这样的一个上肢的一个抓取的能力等等，都是有的，另外呢还有一个任务是这个。

视觉语言导航，这个呢就是给你一句话，举个例子我来到这个地方，我应该怎么样进入这个房间，可能有人跟我说，王鹤老师跟我说应该怎么怎么走，那么我就会按照这个指示，然后看到一些关键的一些节点。

然后就会跟它的语言结合起来，这样就涉及到一个视觉语言的匹配的问题，然后我再去，就是决策我的规划等等，这里也有一些相关的工作了，另外呢这个关于巨神智能呢，现在肯定离不开我们现在大家关注的。

这样一个计算的资源，我们在学术界肯定卡不是那么多，但是呢这个事情还勉强可以做一做，就是当然跟那些做大模型的那些肯定没法比，但是呢现在至少在目前那些任务上，还是可以有一些结果的，当然另外一个方面呢。

从虚拟到现实实际上还是非常困难的，因为你在虚拟环境中可能能得到不少的结果，但是呢你在真实环境中那完全是另外一回事，因为我们实际上也在真实环境中，也试图搭建一个平台，我后面也会跟大家来介绍。

但是呢它里面就是在边缘设备的这样的一个，适配的问题，包括这种里面的噪音的处理的问题，甚至包括那个机器人，它可能走的不稳，可能会颠簸的问题，等等的话呢这些在，在那个模拟器的环境中实际上都是没有的。

所以的话呢，就是在真实环境中真的能让它work，实际上还有很多的工作需要做，当然毋庸置疑这一块肯定是有广泛的运用前景的，不管是在哪个行业，什么方面，肯定都是有广泛需求的，这个我就不花时间来讲了。

对这一页是最近新加的一页PPT，就是多么太大模型嘛，你既然讲人工智能是离不开这个事情了，但是实际上也没啥讲的，简单一句话呢，就是现在这个GPT，TED GPT和GDPFall肯定对我们的冲击力特别大。

但是呢这个东西实际上，可能对直接的这个巨神智能，可能它的这个作用可能还相对有限，当然另外一个方面呢，从这个大家也非常关注这个，面向巨神智能的大模型或者中模型，或者简单一句话来说。

就是这种pre-tuned的模型，是不是能够对各种巨神智能的任务来产生帮助，不管是在巨神智能中的一些，跟视觉表示有关的预训链模型，还是跟视觉语言行为结合在一起的，包括在模拟环境中的数据。

包括在真实环境中的数据，包括它们综合在一起的数据的联合的训练，这些是不是会对巨神智能的各种任务，是不是会产生帮助，这些呢实际上还是有很多值得探索的空间，包括我们也试图想，前两年我们试图也想训一个。

把行为视觉语言结合在一起的这样一个预训链模型，但发现没有那么多机器，后来想想就算了，当然这个是谷歌做的一个工作，实际上它还是非常好的，然后下面就报告一下，我们在视觉导航上的一些相关工作了。

这个导航我们肯定都知道，包括刚才我来是用高德地图导航，然后天上的卫星实际上它也需要有导航，包括冠导啊无线电啊等等大家都知道，但是呢现在在巨神智能里面呢，有一个任务就是和导航有关。

不管是叫这个Point Based，就是点导航还是物体导航，还是视觉语言导航等等都是的，它实际上就是在一个开放的环境中，然后给你一个目标，然后让一个智能体走到它该去的那样的一个位置。

就是我们人一个简单的说，就是一个找路的这样一个能力，当然这一块肯定是我们人类赖以生存的一个非常重要的方面，要不的话这个敌人来了你跑不掉，对吧你不知道该往哪跑，这个肯定是不行的。

当然对我们智能系统来说肯定也是非常重要的，这个也不用花时间去介绍，传统的导航呢就是从机器人的角度上来说，它实际上都是要建好图的，就是像SLAM这种方法，包括我们在酒店里在餐厅里这种自动的送东西。

送菜的这一块，但是我们在做视觉导航，实际上更多的是关注这样一个位置的环境，没有建图，纯粹的通过这种视觉或者通过机器学习，包括强化学习的办法，然后来实现一个他自己自动找路的这样的一个能力。

就像我从来没有来到过咱们这样的一个会议中心，我怎么样去找到这样一个房间，我需要有哪些能力，基本上是这么一个事情，它肯定前期也是需要训练的，我不可能没有任何鲜艳知识，我虽然没有来过这个地方。

但是我之前去过很多类似的会议大厅，我怎么样去找，我肯定是要学一些东西的，然后我还要根据我当前的观察，然后来判断我应该怎么走，所以它应该有一个前期的学习，另外它还有一个当前的观测，它一个基本的架构。

实际上它也是一个end-to-end的过程，这个实际上也没什么特别新奇的东西，包括我们的视觉编码，包括它实际上还有一个相当于，它的输出实际上不是像图像分类似的，是一个标签，而是它的一个动作。

不管是左转还是前进等等，另外它实际上还有一个强化，一个讲诚的这样的一个机制，有一个reward，不管是正向的还是负向的，它的一个基本的架构实际上是这样的，当然基于这种强化学习的这种视觉导航。

实际上要考虑各种方面的事情，包括就是要需要前期足够多的数据，包括我们对视觉表示，然后要有这样的一个比较强的能力，包括用这种预训练的模型，另外我们的训练方式也要考虑多任务。

比如说用这种meta learning的办法等等，实际上都是可以对这个事情起到一定的支持作用的，对视觉导航来说，它实际上就像刚才讲的，给定一个目标，然后在一个环境中，没见过的环境中。

然后根据你输入的这种视觉的数据，然后怎么样来找到我们的这样一个目标，所以它的输入，它的输入基本上就是像这种视觉信息，当然也可以有其他传感器，包括有深度信息等等，然后还有这种我们的目标到底要去哪里。

然后的话这种语意，然后来支持我们怎么样去找到它想去的位置，所以在这个方面，它就需要考虑怎么样在做位置环境下的，这样的一个视觉感知，这里的话，实际上我们的很多视觉能力，不管是物体检测，还是分割等等的话。

肯定还是有帮助的，包括这种开放环境下的标签等等的话，另外还有这种位置环境下的路径的规划，另外还有这种多智能单元的这样的一个协同决策等等，当然这一块的话，肯定还是有很大的这种应用需求的。

特别是在这种开放的没有见过的环境中，举个例子是一个野外环境中，怎么样去完成一定的任务等等，对，当然从国家需求上来说也是需要的，我们前期实际上是做过一些，做过一些工作，实际上从机器人的交互上。

实际上从17年就有一篇文章，然后的话，实际上是从19年开始做，然后后面开始陆续的有文章，就是做一些视觉导航，视觉语言导航等等一些相关的工作，从视觉语言导航角度上来说，从视觉导航角度上来说。

它现在的技术就像刚才讲的，也是一个encoder decoder的这样一个过程，大家看基本的一个感觉，实际上你跟做个图像分类，跟做个image captioning也没啥区别。

实际上就是给它做个视觉编码，然后给它输出，实际上它就是一个行为，它无非就是加了一些，跟强化有关的一些东西，但是它存在的问题，现在主要还是一些黑箱的操作的事情，当然前期也有一些用鲜艳知识。

然后来做一些相关事情，但是在这种情况下，就是这个鲜艳知识怎么样来构建，怎么样它能自动的更新，怎么样来学习物体之间的这种关系，以及它大范围的这样一个尺度性的信息，实际上还有很多的问题需要考虑。

这是我们做的几个工作，包括构建场景图，然后包括多目标的导航，包括这种instance level的导航，就是说这是一个，举个例子电子设备，这有另外一个电子设备，它们两个虽然class是一样。

但是它们instance不一样，是怎么去做的，另外还有这种相当于新目标的导航，就是Zero-Shot的这种导航等等的话，下面的时间关系，我可能就很快的，然后来汇报一下，我们在最近的几个工作。

第一个就是怎么样来构建一个场景知识图，然后来进行这样的一个物体导航，当然这一块也有一些，所谓的生理的心理的一个依据了，就是人的场景的一个识别的能力，跟他场景记忆的能力。

实际上它是有一个互补的这样一个机制的，简单来说就是我们这个思路的话，实际上就是我们来构建了一个层次化的这样一个场景图，就是包括这种物体，还有它的一个sub-zone。

和它的这种整个场景这样的一个层次化的这样一个场景图，然后来作为我们一个鲜艳的学习的这样一个知识图，来指导我们在新的环境中，然后来进行导航，就举个例子是什么呢，就是说我们要找一个锅。

那么我们前期实际上可以学很多知识，就是这个锅它应该在一个什么样的一个房间里，它可能在厨房的可能性比较大，然后在厨房的什么位置上的可能性比较大，我们就可以学一个这样的一些知识图。

但是它只是一种可能性的这样一种图了，然后的话来指导我们后面来，根据当前的观测再给它结合在一起，然后来给它进行一个视觉表示，然后来帮助我们去做导航，对 这里就是我们的一个整体的一个流程图了。

就是把我们学的这样一个层次的图，然后怎么样来嵌入到一个当前的这样一个表示，以及它的一个视觉输入中去，然后来帮助我们去做导航这样的，然后这个就是我们的一个整体的一个思路，就是它的一个底层。

实际上就是物体和物体之间的关系，然后中层的话就是相当于一个子区域，举个例子就是像厨房的一个灶台，和它的一个举个例子洗手盆之间的，这样一些区域之间的这样的一些关系，最顶层的话实际上就是它的一个场景节点了。

这里是我们建图的一个过程，就是它实际上通过这样的一个，通过这样的一个剧类，然后来建立这种典型的物体分布，然后编的话就是这种区域之间相邻的这样一个可能性，然后这是其中一个环境，然后一旦有多个环境。

我们要多个学就涉及到这种在场景层面的，就是这个Scene wise的这样的一个图匹配的这样的一个工作，来得到一个对应的节点和边，然后来给它进行融合，然后有了这个我们实际上就可以来给它进行一个导航了。

然后我们就可以给它进行这样的一个，然后我们就可以给它进行路径的规划，然后搜索这种自由的路径，然后来不断地去利用当前的知识去查找，然后来做这样的一个行为的决策，然后我们这个图可以在这个学习过程中。

它还可以不断的去进行更新，这个就是我们评价的一些方法了，都是在模拟器上，实际上这都是一些Benchmark的一些东西，就像刚才讲的就是类似Amid Night。

Coco这样的在做图像分类的很多的这样的一些任务，大家如果想关注这个任务的话，也需要在这上面发发文章什么的，如果的话是需要有一些评测，基本上是这样的，这个是21年的一个工作，其实性能应该也还可以了。

这个时间关系这些就不讲了，由于我们加了一个这样的鲜艳图，所以它会避免一些Cast在失脚，或者它有一个这种原地打转等等这样一些情况，然后第二个工作是我们最近的一个工作，就是我们基于前面那个工作。

我们加入了一个因果分析，简单来说的话，就是加入这个场景图，它不一定对，加入这个东西，实际上它不一定是对我们有直接的作用的，是什么意思呢，我们前期学的这样一些经验，你用上来的。

它有时候可能不一定起到好作用，因为如果跟前面的，就是跟前面的这样的这些环境，如果是比较熟的话，就是已知环境和未知环境，如果比较类似，那么这个经验就会发挥正面的作用，而如果布局差异比较大。

它这个经验反而会起到一个负面的作用，所以的话我们就怎么样来考虑，能够自适应的，合理的利用这样的一个前期的经验，这个S的话就是我们的观察，然后这个G的话就是我们的目标。

A就是我们要做的这样的一个执行的动作，因此的话我们就利用了这样的一个，因果学习中的这样的一个思想，然后用这种反事实，然后来结合出这种经验的这样的一个影响，这个结合经验的影响，它有点像反事实的思想。

它是怎么做的呢，实际上就是考虑我们前期学的这样一个布局，跟我们当前的这样的一个观测的这样一个环境的布局，它们之间的这样的一个差异，时间关系呢，实际上就是我就快一点，就是这个地方有一个它们一个差异。

然后把这个差异呢，然后放到我们的这样的一个，反事实的这样一个学习框架中，然后来去除它负面的这样一个经验的影响，然后的话呢，让它如果是新环境的话，没见过的环境，那么我们就少用过去的经验。

如果是跟之前的环境比较像的话呢，我们就多用之前的经验，差不多是这么一个思想，这里呢是我们一个整体的一个流通图，它呢实际上是对这个，现在的话呢，它实际上是可插把的。

它实际上对现在就是各种的这样的一个导航的框架，实际上都是可以集成到里面去的，这是我们的一个一些相关的评测吧，一些数据集，然后时间关系呢，我就简单说一句话，就是呢，它对这种布局差异比较大。

就是跟前期如果没有见过的情况下，然后就是我们的性能可能会提升更大一些，所以这里有一些实验结果，时间关系就不展开说了，这里还有一些过程的可视化，然后最后呢，我们再汇报一个工作呢。

就是我们做这种多目标的导航，就是前面介绍的实际上都是，它只找到一个物体，但是呢让它找到多个物体的话呢，我们就要考虑就是我们前面走的路，是不是会对后面要找第二个物体，第三个物体的时候，它是不是会有帮助。

所以它要考虑多任务之间的，这种长期的一个规划的能力，来提升它导航的这样的一个效率，因此的话呢，我们的一个主要的想法呢，就是来探索这种存储探索库的空间，然后的话来构建这样的一个羽翼图，然后的话呢。

再利用知识和数据双启动，这样来形成一个长短期的，这样一个自适应的策略，来提高它这种导航的效率，所以这里是我们的一个，基本的一个框架了，基本的一个框架，它实际上包括就是这种。

相当于一个空间的这样的一个记忆机制，然后来建立空间羽翼图，另外呢我们还要根据当前的这个观测，然后的话呢来做一个预测，最后的话呢，再给它有一个门控机制，来决策它后面是一个长期的。

还是一个短期的这样一个策略，这里面就包括这种空间的，这样的一个羽翼的，这样一个记忆的机制，它这个呢实际上就是现在这种，用CV的办法，然后来构建这种空间的一些羽翼图，然后来给它构建一个。

这样的一个前期的知识表示，另外呢我们可能还要考虑，就是用它这种数据的这种驱动的策略，然后的话通过强化学习的模型，来学习这种空间的这样的一个表示，然后来预测目标的这样一个潜在点。

后面呢我们再用这种门控的机制，来筛选这样的一个导航策略，它有一个长期策略，一个短期的策略，来给它进行一个下一步的一个，动作的预测，然后呢这个就是我们的一些评测，这个是在这个Gibson和。

和这个什么Metaphor的3D上，然后来做的一些相关的实验，这是一些实验的结果，然后和现有的方法相比呢，它实际上就是，我们考虑了这样的一个前期的经验，所以的话它的路径的规划实际上会更短。

然后最后还有两分钟的时间呢，我给大家汇报一下我们，从模拟器到现实环境中去迁移的，一些相关的工作，当然这个发论文是发论文是一回事，然后真的要搞一个这样的环境，真的是要得有一个这样的一个。

就是我们构建了一个，140多平米的这样一个环境，然后呢这个环境呢，它实际上是可以动态变化的，就是一会儿我会讲，然后我们也有一个，这个Local Goat的一个机器人，然后来做这个事情。

然后当然要做的话呢，有一个事情就是它seem to real，就是它这个表示实际上是需要，有一点的迁移性的，就是你在虚拟环境下的表示，它肯定不能直接用在真实环境中，所以怎么样能够让他们这种进行。

一定适应实际上也是需要考虑的，当然你也可以真实环境和虚拟环境，联合去训练pre-training，也是可以的，这里面的话呢，就是我们相当于构建了这样的一个，就是相当于一个建图的。

这个相当于是构建一个场景的，一个碰撞图的这样一个机制，然后的话呢，我们在真实环境中呢，实际上就是来建了一个demo，就是说是能够在任何一种环境下，任何一个位置，它都能找到我们想要的那个目标。

来规划它的这样的一个行为路径，就是我看看，它这个呢，就是我们这个固形图啊，它实际上是可以随时改的，任何的物体，它实际上也都是可以随时改的，对这里也是一些相关的，就是去找杯子的一个demo，这个时间关系。

我可能就不给大家来展示了，然后这里呢，实际上我们是希望将来能做这种，更复杂的一个交互，加上人脸识别啊，这样一些能力，然后它能够就是实现，把东西送给谁，这样的一些相关的任务。

然后包括在一些有障碍物的情况下，我们也能实现它的这样一个目的，包括就是它可以在这样一个环境中，在新的环境中，它可以不断的就是来迭代，就是一轮一轮的来提升，然后让它的这样的一个学习能力，会逐渐的越来越好。

就是说一边导航，边增强，是这样一个过程，然后呢，我们前期一些工作，实际上也是用在了一些地方，包括外部机器人啊，包括一些服务机器人等等，最后是个总结展望，就是呢，现在呢，我们认为就是这一块呢。

实际上确实有很多工作需要做，但是难度实际上和挑战还是挺大的，做导航这件事情呢，实际上也是一个非常非常具体的工作，可能呢，现在可能还处在研究阶段吧，然后真的在开放环境中，能够找到一个物体。

它包括它的视觉能力，包括它的规划能力，包括它的学习能力，实际上还有很多工作需要做，另外一件事情呢，就是这个SIM2RAIL，也有很多需要考虑的事情，时间关系就不展开说了，另外呢，我们这种大模型。

肯定是我们非常重要的一个，非常重要的一个工具，但是怎么样用在这种，这个巨神智能里面，肯定还有很多需要考虑的地方，未来的话肯定是值得期待的吧，对吧，这个时间关系我就不敞开说了。

这是一些合作者和我们发的论文，好 谢谢大家，这个非常感谢蒋老师，给我们带来了这个，巨神智能中重要的一些任务的前沿进展，那么我们下面呢，就开始我们的panel discussion环节，[音乐]。

[音乐]，是啊，谈一下这次的(疫情)，主要是关于(疫情)，(这里听不清)，有吗，(这里听不清)，(这里听不清)，(这里听不清)，那么再次感谢四位嘉宾给我们带来了，一彩分成的四个talk。

涵盖了我们巨身里头很多方面的精彩的问题，和一些前沿的进展，那么下面在圆桌讨论的环节，我们将根据巨身智能的一些新的特点，和我们人们关心的通用的巨身智能体，巨身大模型这一系列重要的问题展开讨论。

也欢迎台下的听众积极地参与到我们的圆桌讨论当中，那么我也是抛砖引玉，从今天这么多嘉宾介绍了这么多学术和研究成果，那么我们想先讨论的第一个问题比较泛一点，就是相比于之前的一些我们讲离身智能也好。

这个Internet AI互联网智能也好，那么巨身智能到底引入了哪些新的研究问题和挑战，那么我们要不然就从苏老师来可以先讲一讲，好 这是一个挺难回答的问题我感觉，但是从我的观点来说呢。

我会认为数据的引入让大家必须要思考，怎么把感知 认知和行动给它耦合起来，这个耦合就是我们面临的，我心目中啊 这就是我们面临的一个最大的挑战，这个耦合的核心呢，其实在问对世界怎样的一种建模是最有效的建模。

那么尤其是如果这背后呢，有所谓的这个叫新的概念的涌现，这件事怎么弄 对吧，你当然可以说用传统的方法，叫做gradient descent，你说这个就不叫概念涌现了吗，那么问题在于这样的分布式的一种表示。

在多大程度上还可以支持推理，可以帮助你实现好的组合范化，也就是说这样涌现的概念，在多大程度上要变成symbolic的，那么如何能够把这个涌现的概念变成symbolic的呢，那么这个连续的梯度下降。

怎么能跟symbolic结合起来，我觉得这是一个可能从理论上很本质的，这么一个不太确定的地方，可能有些其他的，我可以把一些别的挑战留给别的老师来讲，确实比较难回答一个问题啊，从我的角度来看的话。

因为现在的话Foundation Model比较火，那么巨声智能的话，Foundation Model相当于是把数据变成了知识，尤其是Large Language Model的话。

它其实就是学过了我们所有，可能互联网上所有的数据吧，那么它其实对于一个具体的环境的话，因为它只是语言，语言是一个抽象的表示，那么抽象它的范化力强就表示，它对一个具体的东西的话，它不能够表述得很细节。

那么对于Large Language Model来说，把它融入到巨声智能的话，它需要适应这个环境 对吧，它需要在这个环境上面，再去积累关于这个环境的一些具体的，巨声的一些表象 对吧，或者是巨声的知识吧。

如何在这样的，这Online Interaction的过程中去让，不管是Large Language Model也好，还是Visual Language Model也好，让它融入到环境中。

不管是虚拟环境还是现实环境，我觉得是需要下一步解决的问题，挑战的话可能，另外挑战的话还有一个一点吧，因为我个人的观点，我是把这个Large Language Model，认为它是一个。

World Model的一个抽象的表示吧，因为我们语言的描述的话，其实很多很大层面上，就是描述的物理世界，当然除了一些其他的之外，大部分都是描述物理世界，那么它是一个，也是一个抽象的表示。

那么在巨声的时候，如何从一个抽象的物理世界，到一个具体的物理世界，那么如何学习一个，输入是Visual的这么一个，Input的世界模型吧，如何把它结合起来，去真实的从一个文本的。

或者Symbolic的表示，让它具体到每一个Pixel上面，我觉得这个World Model，就是至于Visual Information的World Model，可能是我们接下来要做的。

也是有挑战的事情，好 谢谢，对 我觉得这个，卢老师谈到的就是说，我们现在有了机器人，那么我们在这个，巨声机器人的学习当中，很重要的一点呢，是引入这样的一个World Model，那么这样的一个概念。

就是能不能让卢老师，可以再这个阐述一下，你觉得这样的World Model，为什么在过去的一些，比如说我们讲Internet AI时代，它并没有那么的重要，那么现在呢，包括这个Yan LeCun等等。

图灵讲，这个德主在一系列重要的，这个报告当中，反复谈这个世界模型，那么它对巨声智能带来了哪些，它的意义是什么，它的研究问题是什么，因为这个World Model的话，其实处处就是怎么说呢。

从强化学的角度来看的话，因为Model Based RL，它本身就是World Model，基于Model去做一些Planning等等，因为之前我想从，比如说Internet的，AI的任务来讲的话。

从CV的任务来看的话，它其实没有涉及到，比如说决策的这一部分，如果我们接下来，要做的是巨声智能，我们要去考虑的是，每一步我要做什么动作，那么这个时候，从强化学习的角度来讲，它就是可以用。

比如说基于World Model，Based的方法，或者Model Based RL的方法，去做Planning，我觉得这是我自己的一些，浅薄的理解吧，我这地方补一下，因为我们组做了很多的。

这个Model Based RL，那么有个什么问题呢，就是说Internet AI时代，你做前项预测对不对，那么预测完了之后，对错你是很难讲的，对吧，它就是让人看一看，到了巨声智能的话。

这个事它有很大的问题，在于所谓Model Based RL，它是要在一个World Model，它是要跑很多步，这个过程它会有误差积累的，那么而且呢，从一个确定的这个出水状态，世界它是随机的。

所以呢其实你这个World Model，它必须要做到，是一个Long Horizon的，一个Generative Model，具备Uncertainty，而且还要。

它的Distribution还要Correct，这样一个东西，那么在巨声智能之前，它几乎都是无法验证的，但是巨声智能的话，它是可以的，因为最后它好或不好，它将决定Task Success Rate，对。

对我觉得这一点上，我也是非常同意，两位老师的说法，因为我们人的学习呢，本质上是一个，Perception Action Loop，也就是说当你在，感知这个世界之后呢，你要根据你的感知呢，去执行一个。

你认为有效的行动，那么这个行动呢，将进一步地改变这个世界，那么刷新这个世界的状态，那么你再，重新去进行Perception，所以在这样的，Perception Action Loop当中呢，当你去想去。

Take一个Action，做一些行动的时候呢，如果你能对这个世界，进行建模，那么你就能预先知道，那么我做这样的一件事，我去碰这个杯子，到底是能把它拿起来，还是会把它打翻，那么这样的事情呢，对巨身智能体。

在复杂的长程的交互当中，去怎么样去做，正确的交互学习，和怎么样去选择，正确的交互方式，都是非常重要的，所以我们看，这个World Model，可能在巨身智能当中，会被作为一个聚焦的，一个问题去研究。

那么这里头其实也，引入了就是刚刚这个，隋老师这个讲座的时候，的一个问题，就是说我们巨身智能当中，其实经常有这个，Safety安全性的考虑，对吧，那么我想让隋老师，也谈一谈就是，这个巨身智能与安全。

或者是从您的角度上讲，有哪些引入的新的问题，在之前的这个智能时代，是没有被充分考虑的，好的谢谢，我就接着这个问题，和我谈这个两点，我自己的这个感想，不一定是对于，这个问题的回答，很有可能是给大家。

提出了一个新的问题，一个是关于巨身智能，相关的这个新的研究问题，一个是挑战，那么我不知道在座的各位，就是大家是最早听到，巨身智能或者是巨身性，这样的一个描述，是在什么样的一个场景下，是在计算机科学。

人工智能的这个领域里，还是在其他的什么，这个领域里，这个因为对于我来讲的话，我最开始认识这个词，是更早在哲学领域里面，就大家如果往回翻说，在哲学领域里面，这个来谈这个巨身性，还有甚至是这种。

就是巨身的这些智能的，这些表象，好像是比我们，至少是比我们，这一轮的这个巨身智能，在人工智能领域里，活起来要更早的，那么这里面，其实有一个很有意思的现象，大家会看到说，随着我们自然科学和技术的。

往前的进步，哲学是在退守，哲学在越来越退到一些，这个更小的一些领域里，比如自然哲学的数学原理，对吧，大家知道是知道，这本书是讲什么的，牛顿的嘛对吧，自然哲学的数学原理，讲力学，讲这个讲物理学的。

后来有了物理学，我们就不再管它叫化学等等，我们就不再管它叫做，这个自然哲学了，那么哲学领域里面，还有一个这个科技哲学里面，就是非常有名的，前面有一本书叫做，这个计算机不能做什么，可能我们计算机专业的。

有些同学，如果对哲学感兴趣的话，会看到那本书，这是大概五六十年前吧，可能是那个时间，可能没有那么早，几十年前，反正说这个计算机能力很强，但是它不能做什么，哲学家认为说，有些事情计算机是做不了的。

人是具有这种独特性，这些，然后过了些年，计算机发展很快，那么哲学家又写一本书，同样一个哲学家，叫做计算机仍然不能做什么，大家感兴趣可以看一下，这两本非常有名的书，计算机仍然不能做什么，这本书。

当时里面的那个，仍然不能做什么，今天又有大量的被break掉了，所以其实结合着，哲学家的这些思考，还有巨神智能的这个概念，在哲学里面的，更加的早的提出，其实在座的各位，如果大家想找新的研究的问题。

尤其是跟我们人工智能的研究的问题，可以去哲学家现在描述的这些，这些仍然健在的这些领域里面，去找一找，可能会找到一些，这个有意思的东西，所以本身巨神智能相关的，新的研究问题，一定和这个里面会有些关联。

那么挑战方面，其实刚才其实接着，王河老师问的这个问题，巨神智能本身其实是，机器人的系统，因为刚才蒋老师讲的一个，就是说这个，它的巨神智能的一个重要的载体，就是机器人，机器人作为一个重要的载体。

然后巨神智能很多时候，和环境交互，也有很多时候是在和人在交互，那么和人在交互的过程中的，这些安全性的问题，因为如果它是一个完全的这种，无人的环境里面，比如我们的这个自动的码头，这种自动的工厂等等的。

这些安全性的问题，相对来讲小一点，更多的其实就是一个，经济成本问题，但如果是一个和人在交互的，这样的一个环境当中，其实这个里面的这个算法问题，和这个里面的伦理问题，就都会是可能比较严重的，这个问题。

有些也许我们能够有技术性的解决，有些可能不一定有技术性的解决，像大家可能这些年会熟悉的，这个trolley problem，对吧，一个火车你是撞五个人，还是拐下去撞一个人，这样的这种伦理判断的问题。

那么我们其实在前面的，这个临床实践过程中，差不多十年前，我自己亲身的体会过，这样的一个冲击，就是刚才我在报告里面讲，我们在做安全的强化学习的算法，在线的强化学习算法，但是我们知道，如果我们可以放弃。

一定程度的安全性的话，算法的效率会显著的提高，对吧，我不要求现实世界里面，一定我们的每一个行为都那么安全，那我的这些采样的效率，会显著提高，但是它带来的负面是什么，就是一旦有这些坏的发生。

在我们2012到2013年的，这部分临床实验里面，就会发现说，当时没有考虑安全性的问题，一旦有坏的这些事情发生，人本身对于算法的信任程度，和对于一个智能系统的，信任程度是远低于。

对于另外的人和专业的专家的，这个信任程度，马上就不让我们再做这件事情了，所以当巨神智能，我们的这些能力，系统的能力在逐渐提高的时候，可能还是要特别的小心，去看它和人在交互的过程当中。

有哪些是我们要特别注意的问题，这我大概的一些想法，好，我觉得安全可能是一个，fundamental的对于智能巨神机器人的，一个挑战，但是我们从学术上，那么我觉得今天，蒋老师给我们深度的展示了。

巨神导航里头的一系列问题，我也想请蒋老师，从学术的角度，研究的角度上，除了导航以外，还有哪些值得研究的问题，特别是可能在座的很多同学，都有发表的需要，对吧，那么你们在做paper的时候，还有什么。

有很多空间的问题可以去研究，好谢谢，是这样，我觉得巨神智能实际上，给我们很大的想象空间，我们反正都知道，人工智能的图灵测试，是吧，现在的图灵测试，到什么状态和阶段了，我们先不去评价，但是我们可能。

从巨神智能这个视角来说，我们可能也希望一个巨身体，是不是有这种类人工智能，人就是这种智能性的，这样的一个感觉，这个我不展开说了，但是在这个过程中，实际上是有很多问题，值得我们思考的，特别特别多的问题。

这个说很多，多长时间可能都不一定能说完，但是我觉得这里面，至少是考虑到一个事情，就是我们传统的，很多人工智能的研究任务，因为现在我们有很多很多人，都说都在做AI，对吧，但是这个AI。

你一旦在巨身这样一个场景下，那么它会发生什么变化，会有什么结合，对吧，会有什么新的一些东西，我觉得这里面实际上，是值得我们思考的，对吧，就像CV的东西，在巨身智能下是什么。

NLP的东西在这个里面又是什么，包括Motion Learning的东西，在里面又是什么，我觉得这里面实际上有很多，值得我们思考的东西，这个呢，就像刚才几位老师讲的，这个里面实际上，这个问题很大。

我觉得一句话说不清楚，但是呢，我们实际上呢，你看到任何一个人工智能的关键词，我们都可以从你认为，你理解的巨身智能的这样一个视角下面，看是什么东西，后面又会怎么发展，我觉得就会有一些新的东西出来。

我们共同去思考这个问题，可能未来会带来很多变化，这个是我想讲的第一个意思，第二个意思呢，就是，就是这个，大家都在讲这个学习，学习呢实际上是两个方面，一个呢，我觉得现在大家思路上逐渐的是在。

在这个reframe，是什么意思呢，就之前反正就是图像识别啊什么的，就是train一个model，然后去test就完了，现在呢，大家开始这种大模型的思维了，什么东西都是说，你一个大模型训练的东西怎么样。

我还是想讲一些机器学习的东西，机器学习它反而是一个training data，一个test data，现在呢是一个big training data，然后呢在一个test data下去做，对吧。

那么呢在居身智能这样一个场景下，实际上呢它还是要有一个，环境的，有一个动态的环境，和一个上下文的，是吧在这种情况下呢，这种大模型不一定好用，就举个例子，如果说我们家里面有一个服务机器人。

他不需要认识那么多人物，他不需要知道那么多知识，他只要知道那一个，他真正关注的那两三个人，有现在几个知识他能搞定，能弄明白，就已经非常非常棒了，但是这里面是不是能弄明白，能搞定。

实际上也有很多东西值得探讨，这个我不展开说，我实际上就总结一句话意思是什么呢，这个大模型和小环境怎么适配，就是居身智能实际上是，这个我觉得是值得思考的，就是大家都也都在讲这个大模型。

什么将来可以用到这个，用到用进来用进来，但是是不是真的能用进来，怎么用进来，用进来效果是不是真的好，这里面实际上，至少到目前为止还没有一个，至少还没有一个特别清晰的答案，但是这个大模型怎么样来用。

或者怎么样来训，我觉得肯定是有很多值得琢磨的东西，这个我也不展开说了，这个话题也很大，这是我想说的第二点，第三点呢实际上现在，就像刚才那个隋老师讲的，实际上我挺认可的，实际上很多哲学啊，包括很多安全啊。

人的交互啊意图，这里面是有很多值得思考的问题，我前段时间我闲着没事，我看了一些文章，我发现那个东西，我本来想搞搞那个东西，我搞不了，那个叫啥呢，那个叫Theory of Mind。

可能也有一些相关的论文，就是讲这个，讲这个什么人的意图啊，人的目的啊，那个什么False Belief啊，就类似那个东西，实际上它是真的你能够知道，你该干啥了，你相当于，要做一个懂事的人。

要知道你该干啥，就差不多是那种感觉，好像有个形容词叫什么来着，反正就是说呢，在一个场景下，你要知道你应该，你知道每个人的想法，是吧，知道每个人就类似那种，我看看我能不能说清楚啊，我用一分钟。

就类似那个叫什么，就是咱们经常玩的一个游戏，就是那个叫什么，是那个杀人还是叫什么，就类似那个东西，你能不能分析出来，谁是一个骗子，或者谁是一个什么这样的东西，就是一个意图性的东西嘛，实际上我们很多时候。

就是更深层的，实际上是要知道，一些人的这种意图的，当然那个更多的伦理啊，或者那个东西，我觉得肯定还特别特别遥远，咱们先不说那件事情，但是一个意图这个事情，我觉得还是有很多值得思考的，因为你最终实际上。

是要为人服务嘛，对吧，但是这个东西反正我觉得，也不好搞，我也说不太清楚，但是我，我觉得这个东西蛮有意思的，至少，行我就说这些吧，我觉得就是这是认知层面啊，人对其他，就是智能机器人。

对我们人的mental state的一个建模，确实很重要，我们最后其实会讨论这个，人机共融的这个问题，那么我围绕着咱们这个巨身智能，引入的新的研究问题，我个人感觉啊，在导航，就是我们在移动能力之上。

其实呢，巨身智能里头很关注的，就是manipulation，就是操纵的技能，跟场景交互，物理交互，比如说你用去手抓取，然后你使用工具的，这样的技能的这个研究，是非常重要的这个研究问题。

那么围绕着这样的一个技能研究，其实我们发现呢，其实巨身的这个很多模型，它里面都有很多的技能模型，这样的技能模型呢，也需要很多的巨身大数据，来进行学习，我们知道今天的这个。

就是Chad GPT GPT-4，它之所以成功，就是依赖于互联网上，大量的图文对和文字材料，那么其实我们未来，展望未来，我们巨身如果要能发展出，这样通用的能力的话，那么这样的这个巨身大数据。

到底如何获得，那么可能有很多不同的路径，比如说是从人类，通过摇操作采集，一些这个demonstration，也可能是通过在模拟器里头，进行讲话学习等等，那么我觉得这个问题呢，也是很多。

就是同学和研究者关注的，我也想听听这个，大家各位老师的观点，对，我先说，好好好，这个显然我个人感觉，就是这个巨身学习实际上，这个巨身大数据，它是一个很重要的一个bottleneck，没有巨身大数据。

那么谈这个所谓巨身foundation model，就是很难谈的，但是巨身大数据的获得呢，这儿有两个问题，人类摇操作采集或模拟器，两种可能选择，其实背后吧，这个还是缺很多的infrastructure。

我觉得这个很大的一个问题，是缺infrastructure，就是到了这个巨身智能时代，我个人感觉，我自己培养学生也是有这个感受，你进入到这个领域之后啊，这个工程能力啊，它会变得很重要。

不管你是打算做摇操作，还是模拟器，背后其实都有很多的，它其实可能还不是那种理论问题，不是那种原则性问题，比如刚才这个蒋老师，实际上提到了一些个有关博弈论的，或者是等等相关，它还不是那个问题。

它背后有很多很困难的工程问题，当你采用人类摇操作的话，那么一个困难是什么呢，立反馈怎么办，那么我人类摇操作，如果只是做基础的抓取，这个应该是没什么问题的，二指手 柔性的二指手做抓取。

人类摇操作我相信是一个手段，当然在这个setup下，你又未必用人类摇操作，你可能手工设计算法也可以，不过更复杂的五指手，精细操作，例如说啊，当然机器人可能也没必要做这件事，比如转笔，人会转笔。

转得啪啪啪的，什么王者水平 钻石水平的，那个东西你怎么摇操作，这就是一个事儿了，所以说其实呢，可能要弄清楚，相当于是把这些scale呢，和这些技能呢，定义一个层级，如果有可能摇操作采集的。

用摇操作也可以，但肯定我认为有相当一部分，它的摇操作难度是非常大的，那么回到模拟器，模拟器呢表面上看起来，这个，有一些好处，但模拟器里边呢，也有一些问题，难度比如说这个，首先是3D的内容，那真实世界。

所有的东西都在这呢，当然你要花钱买，是不是 你要雇人去标，这个你要有成本，但模拟器的话，首先要不让模拟器里有内容，这就跟你弄个电视台，做节目很难的，然后呢，这个内容你不光要几何，你的。

刚才老师也提到了这个，你的reward，你的激励怎么标啊，你吸收激励的时候，它就不好做了，那么你不吸收激励的话，那么是不是能有些个，reward的pattern transfer，这也是一些个困难。

但好在呢我们觉得，就是说虽然这些事都很难，但是呢我感觉啊，就是进展也是在不停地发生的，比如说Google也给你展示了一下，砸很多的钱是不是，人类要操作能拿些什么事情，那我们组呢，还有NVIDIA。

就是我们都是属于很关心这种，底层的这种模拟器怎么构造，其他的比如Igibson，AI2Solar这些个，他们关心这个上层的模拟器如何构造，总归呢就是有些个effort，不过巨神智能弄到今天。

它有一个时候，它缺很多infrastructure，你需要很多的技能，你需要学习很多的知识，其他领域的知识把它结合起来，我觉得这个其实是核心困难，抱歉我说的比较多，对这个其实我们补充这个背景啊。

就是我们其实看到Google的C-con，他们的RT1这些，这个背后呢，其实是非常非常，大量劳动力的一个，这个摇操作数据的采集，那么RT1呢，大约花了17个月的时间，采集了13万条。

人类用遥控器操纵机器人，执行任务的这样的一个数据，那么他们的算法呢，就完全是一个模仿学习，imitation，这个behavior cloning的算法，那么模拟器呢，其实今天我们的talk里头。

包括这个卢老师啊，包括隋老师啊，包括这个，就是大家都谈到了，这个模拟器的一个重要性，那么除了这两种数据以外，其实还有这种人类的视频数据，对吧，那么卢老师也可以再谈一谈，好，其实我，当然做具体的操作。

尤其是机器人控制的话，需要这个，真实的操作的数据和模拟器，但是对于这个，从world model角度来讲的话，其实我们可以利用，我们拥有的大量的视频，因为视频的话，大概率就是我们的，第一人称的视角。

当然除了电影之外的一些，比如说ego 4D的这些数据集，它其实就是人在操作一些东西，做一些task，来完成一些任务，那么我们要做的是说，如何基于这些视频来学，这world model。

就像上午杨乐坤讲的那样，对吧，如果给你一个视频的数据，你能从这个数据里边，学到一个world model吗，或者是回头能得到一个，对于具体的任务的操作吗，那么这个问题的话，其实我想说的就是。

我们有大量的视频的数据，available on the internet，对吧，我们如何用好这个数据，来学习能够帮助我们，做巨声智能操作，或者是起码作为一个，pre-trained model。

然后去进一步去做，后续的这些工作，那么这个是我们可能需要，第一步去做的一件事情，当然这个也可能是，我们从学术的角度来说，比较方便去做的，因为刚才王赫已经说了，对于后面的操作的话，其实工程量以及经费方面。

都需要大量的投入，那么我们从学术的角度来看，如何从视频中去学一个world model，是我们需要去做，或者是有挑战的事情，好，对 所以我觉得就是，这种被动的passive的，你光看人类展示的数据。

其实可能对于我们学world model，学visual feature，甚至一些最近的工作，学visual based reward，用到真实世界的强化学习当中，都有很多重要的应用，所以我这里看。

我们的数据其实不止这两种，第一种是人类的一个，就是视频的数据，这些数据虽然说，跟机器人的巨身不一样，它是另外一个身体，但是仍然对我们机器人，怎么做好这个任务，具有重要的起底作用，那么人类摇操作的数据。

是最直接的，你直接回放这个数据，就能让机器人干这个事，那么simulator里头，它是最便宜的 对吧，你可以无限在里头高效地做，其实还有第四类数据，就是强化学习，机器人直接在真实世界中。

进行强化学习的数据，所以这个第四类数据，其实就引发了我们，下一个我觉得很重要，要讨论的一个问题，就是强化学习，在发展通用的巨身机器人里头，它可以发挥什么作用，我们既可以在simulator里做强化学习。

我们又可以在真实世界，虽然这个很危险，做强化学习，所以这一点呢，我觉得今天这个崔老师，讲到他们的人的，就是机体运动能力，重建的这个东西，竟然是在真实世界里头，通过强化学习采集的。

所以我想崔老师也可以谈谈，这块您的一些看法，好的，我连着上面那个第二个问题，然后到这个问题，就是我们的这个巨身智能的数据，有些是从解剖里面来的，人体解剖来告诉我们这些，我们的world model。

因为我们的world model，刚才说我们其实是从一个，广泛的world model，到一个self model，我们说的这个self model，其实是人的物理的课题，所以需要从解剖里面来。

有些从人的解剖里面来，这个可能不一定合适，或者不一定成本合适的话，我们会从动物的这个，解剖的这些数据里面来，这都是我们认识世界的方式，因为像这个Minecraft，有可能下一代，等这个算力起来了以后。

这个游戏的真实性，和这个物理交互性会很强，现在这大规模的3C游戏，是吧，这个可能在座，有些这个大家喜欢玩的，它那个交互性和那个，simulator本身做得非常好，那么这个里面的这个数据在哪来。

这个数据其实，可能本质上还是需要我们从，你从牛身上来取样，你从人身上真的来取这个样，来看你的肌肉的这个，弹性系数是怎么样，对吧来看你皮肤啊组织啊，然后骨的这些强度，然后神经的本身的这些。

这个脑脊液的这些流动的，流动的这些参数粘性等等，就这是我们来构建world model，或者说我们来构建，巨神智能的这个一个底层的，还是要从这个物理世界里面来，所以这也是为什么我们说。

哎在这个真实世界里面啊，真的来用这些强化学习的时候，我们希望我们希望一定，首先先有一个model based，的一个版本啊，那么model based的版本之后，从这个sim to real本身。

这还是一个很困难的，很困难的事情啊，所以所以就是其实永远，没有在真实世界里面的，pure model based learning，在真实世界里面一定是一个。

model based加上model free，model告诉我们所有尽可能，它能够告诉我们的，我们再根据它再来进行，这个online的这些这些调整和适配啊，所以早期我们的这个，一些研究工作在人上的。

这个神经刺激的也好，这种外物格或者机器人交互的也好，可能我没有这些数据，我没有这些模型啊，我需要code start啊，这种方式来通过这个model free。

online reinforcement learning，这个样子能够能够来做起来，我们能够看到一些很好的效果啊，但是到了今天，我们就可以来一步一步的来构建，真实的这些世界的人的模型啊。

机器人的这些模型啊，那么这些模型，seem to real可能最终确实是，这个这个这个强化学习，在我们的现实通用机器人中，发挥作用的这样的主要的途径啊，这是我的认识，对我我非常感谢这个隋老师啊。

我觉得就是这个seem to real，其实它在很多巨神任务的学习当中，都起到了重要作用，其实在蒋老师的报告里，对吧，我们的这个巨神智能体的导航，它其实呢，我的理解啊。

就是您的团队应该是在simulator里头，用强化学习，做了很多这个导航策略的学习，然后呢部署到了真实世界，您觉得在这个过程中，这个seem to real的gap，是一个多大的困难。

然后呢就是强化学习，能否就是如果我们依赖强化学习，加seem to real，它有什么局限性吗，我觉得局限性还是挺大的，因为客观说就是在模拟器上，然后用强化学习，给它的training data。

然后来训一训，一个model还不错对吧，然后呢你一旦换了一个环境，实际上强化的学的东西，都不一定很好使了，在真实环境中，实际上还是主要是建图，然后的话通过学习的这种办法，可能会更好一些，所以的话呢。

我们一个基本的体会，就是强化学习，肯定还是需要足够多的数据的，或者它的泛化能力要足够的强的，要不的话它这个见的少，它可能就不行，见多才能识广嘛，所以这一块还是，我认为还是需要，有足够多的数据的支持。

包括呢还是需要有，可能还是需要有更多的这种，环境的一些真实的反馈，可能才能让它的泛化能力，可能会更好一些，就是我是觉得，就是在这个巨神智能中，未来这种强化学习，是一个非常重要的工具。

它应该还是要跟其他的要相辅相成，一个是数据，一个呢可能还是要跟一些，就是其他的一些结合吧，你举个例子，就是跟这个相当于，跟一些知识学习吧，就是我不知道这个词应该怎么样来说，因为现在反正国内。

有一个说法嘛，就是什么叫什么数据驱动，和知识引导的什么学习，但是它怎么数据驱动，怎么知识引导咱们不展开说了，但是呢我觉得未来这个巨神智能啊，它要是要发展的话，它不能是纯数据驱动的。

它还是要有一定的知识引导，并且呢这个知识引导呢，可能有一些呢是人的反馈，就是人的反馈，然后来让它更好的来提升，它的这样的一个，巨神智能的学习能力，和它的行为能力，但是这一块我觉得。

实际上还有很多工作需要做，我简单总结一下呢，我觉得现在的实际上很多，就是至少是像什么视觉导航，视觉语言导航啊，就是虽然我也在这上面发文章啊，我觉得呢你如果是这个，在虚拟环境下。

反正玩一玩发几篇文章是可以，在真实环境中挑战还蛮大的，离真正的work还挺远的，反而是那些操控啊，那些什么东西的，我觉得可能有一些东西可能，可能会更近一点，我就简单说这些，其实就是这个操控啊。

其实我觉得苏老师的团队，做了应当说我们，co-author了这个Sapien的，这样的一个仿真平台，那么苏老师也发起了，ManySkill这样可范化的，就是机器人操纵技能，通过强化学习的。

这样一个挑战赛，对吧，那么苏老师您对这个问题，有什么样的看法，谢谢啊，我是觉得强化学习可能在，至少三个层面是有用的，第一个层面是，强化学习本来是来自于控制领域的，就是说底层控制，底层的操作技能。

这个东西是可以通过强化学习，学到一个可靠的控制器，这是第一个层面，这是底层层面，第二个层面实际上是，一个上层层面，那么如果广义来说，强化学习就是在反馈中学习，那么我们现在不把它当成一个控制工具。

我们把它当成一个，exploration的工具，当成一个在错误中，调整一个上层的planning，规划策略的这么一个工具，这也是一种强化学习的用法，我不知道大家能不能感受到，这个区别。

就好像我们小时候做作业一样，做错了，我们改了重做，NLP里，对吧，你不能说NLP里，他们也在说他们什么，Human in a loop 是不是，也说强化学习，那显然不是控制型学习。

那是一个规划层面的学习，第三个就是刚才讲，seem to real这件事，至于在操作技能这件事呢，我个人觉得强化学习的空间更大一点，因为某种程度上，像刚才蒋老师讲的，navigation这个问题啊。

你不用强化学习，直接去建模，好像也可以，这就不能给它一个，它的必要性似乎未必那么大，对，那么manipulation里边，好些个情况下，你去看看经典机器人，在软体，在摩擦比较复杂，等等一些个。

或者是叫under-action system，就是欠驱动系统，这些setup下，传统方法知识，还真就给你弄不出一个可靠控制器来，这时候强化学习，这个必要性会大一点点，我觉得就是这个，苏老师刚刚讲的呢。

让我就是也进一步的感受到，就是说我们在技能学习里头，它其实非常的复杂，这个操纵对吧，那么这里头的可能试错，是一种重要的学习方法，但是可能像Google，他们的这套摇操作系统，它通过模仿学习。

也是一个重要的方法，那么其实未来呢，我觉得就是这种巨身机器人的，技能学习会长期成为，我们一个通用机器人的一个bottleneck，你的机器人到底能学会，多少种不同的技能，叠衣服是一个技能。

倒水是一个技能，挂衣服是一个技能，那这样的技能学习，在未来呢可能呢，只有充分的让我们的机器人，可范化的低成本的，学到这些技能呢，我们的机器人才能有更多的，在真实世界中的用途，那么其实说到这里呢。

其实我们已经，马上就到这个问题了，就是说畅想未来，我们通用巨身智能机器人，还有我们讲的多模态，巨身大模型，那么怎么从我们今天，已经有的这些，数据采集方式，数据的这个就是生成方式，到我们现在的。

这个大模型和这个，各种学习方法监督性学习，强化学习和，模仿学习等等来，共同推动这样的一个，伟大的这样的一条，这个发展道路，那么我觉得这个卢老师的，这个Minecraft，可以说已经是一个。

挺复杂的一个环境里，当然了它的物理是比较简单的环境里头，发展出来的一个，借用了很多大模型的一个工作，卢老师也可以分享一下您的看法，OK 好，就是我个人理解，就是目前来看的话，基于比如说了。

这个Large Language Model，或者是像GPT-4一样的，带有这个可以输入视觉信息的，这些模型的话，其实是可以跟，比如说我们有一个，Scale Library。

然后在这个Library上面去做Planning，是可以去完成一些，比如说像Minecraft里边的，简单的任务的，当然这个Scale的学习的话，我同意刚才苏浩老师说的这个，这部分可能是需要用强化学习。

去尝试的，我想说的就几点吧，一是我们需要构建一个，Scale Library，Scale Library的话，它可以是很简单的一些动作的，比如说Sequence。

但是我们要有这么一个Scale Library，有了这个Scale Library之后呢，比如说我们通过这个Scale的组合，比如说通过用Large Language Model的组合。

通过这些Scale的组合呢，其实我们就可以完成一个，比这个Scale Library指数级别的一个Task，完成这样的任务，那么这样的话，其实我们相当于就连接起来。

Large Language Model和Scale，因为我们要构建巨声大模型的话，那么Scale Library肯定也是一个，必须要构建的，但是它的数量需要多少呢，我还真不知道。

因为对于Minecraft的话，它可能是Limited的数量的Scale，但是对于具体的机器人的操作的话，它可能需要很多很多的Scale，以及如何在环境中持续地，学习这个Scale。

也是另外一个非常重要的点，第二个是，我们需要一个，我们需要一个，另外一个非常重要的点，另外需要说的一点就是，刚才提到了World Model，我相信如果我们真的要，巨声智能，然后去跟环境交互的话。

至于视觉的这个World Model，是不可避免的，或者是如何把这个视觉上的World Model，和更抽象的Language Model，因为它更具备一些Resonance的能力，如何去结合起来。

也是我们需要考虑的一点，那么就是，Large Language Model，World Model以及Scale Library，我大概就这些Comment吧，我看苏老师是想，我只想说非常同意，对。

其实关于巨声大模型怎么发展，其实关于巨声大模型怎么发展，其实关于巨声大模型怎么发展，也有很多学者同学们有问题，也有很多学者同学们有问题，那么我见了很多感兴趣这个问题的，那么我见了很多感兴趣这个问题的。

这个学者，他们也会问，是不是未来的巨声大模型，是不是未来的巨声大模型，它就是我们现在的GPT-4一样，你给一个图，给一个语言的Command，我要渴了我要喝水，那么这个大模型，直接输出这个机器人。

底层的控制信号，比如说我迈你条腿，我的手怎么动，那么这是不是巨声大模型，你会发现到其实现在像Palme这样的，它所谓的巨声大模型，它所谓的巨声大模型，它其实输出的并不是底层的，这个机器人的控制信号。

而是机器人的Skill，那么这样的不同的发展道路，那么这样的不同的发展道路，就是上层的调度，接着底层的Skill Library，接着底层的Skill Library，或者是End-to-end。

一个从图文直接到，一个从图文直接到，肌肉控制或者是电机控制的，这样的发展道路，大家觉得就是哪一条，可能是未来真正的道路，或者我们现在应该走哪一条道路，或者我们现在应该走哪一条道路，大家有什么这个看法吗。

大家有什么这个看法吗，我先说那我就，我坚持刚才的观点，我自己的观点可能这个Skill的话，其实因为人的话本身要学很多Skill，比如说你小的时候学走路啊等等，其实都是要学的，所以的话我认为啊。

还是需要一个Skill Library，在Skill层面去做一些Planning，另外还要需要强调的就是，强化学习的重要性就是，我认为强化学习主要可能用来，就是做Skill层面的学习。

包括比如说你要练习打网球，比如说你要练习打乒乓球，你要拿世界冠军，这个不管是Model Free也好，Model Based的方法，这个Trial and Error的尝试，你需要苦练才能得到这个技能。

好 这就是我的Comment，苏老师和蒋老师有什么看法吗，那我说一句吧，这个巨神大模型我觉得，这个路可能还挺远的，我就是反正相对比较保守吧，因为这个大模型它是从哪来的呢，它肯定是从训练数据来的。

对吧 你这个训练数据是啥，它实际上能训出来的基本上就是啥了，对吧 然后呢现在的话呢，说句实在话咱们在这个巨神智能上，它的场景 它的任务，涉及面特别广，然后呢你如果想真的做一个。

特别通用的东西可能也比较难，即使是做一个专用的这样一个大模型呢，可能也比较难，因为这个数据采集实际上是个特别，特别复杂的一件事情，另外呢我觉得这个大模型，当然你可以讲是巨神大模型。

但我觉得可能一开始还是从点上来的，还是点上来的，然后在一些特定任务下可能是好的，或者你反正你反正做表示嘛，或者视觉语言表示你再加一些指令，你也可以训 对吧，然后你说我可以在什么情况下好。

也是可以 但是这个大模型，是不是真的能够满足我们实际的需求，实际的任务我觉得可能，还是有，有一段时间要做吧，包括回应到刚才那个关于数据的问题，我实际上挺担心的，因为这个数据将来肯定会有。

但是学术界可能不一定能搞得出来，我感觉那个东西太花钱了，是吧 然后呢，你企业的话他们出来以后，可能他这个大模型，他们那个逻辑上的大模型可能就会有了，但是是不是真的能满足实际的应用需求。

我觉得可能还是有一定的距离的，但是当然这个事情，肯定是值得做的，并且肯定是不断的会有人会提这件事，但是它是不是真的能，那么好的，能满足我们的这个，想象的那个事情我觉得还挺远的，我现在是这种感觉。

不一定对，我说一点点，就是说我感觉，这个巨身大模型这个事，我也是我坚持我自己的观点，我那个报告最后是放了一张图的，我是觉得呢，就好像我们说要求，如果你是完成一个long horizon task，你要。

你是不能直接训练这种东西的，你必须引入一个所谓compositional，genres的思想，来组合，巨身大模型也一样，我觉得它不是一个模型，它是好多个模型，perception模型。

world model模型，那么这个decision模型等等，我觉得它是好多个模型的集合，当然呢，实际的发展路线可能是这个，你要结伴了之后呢，你才有可能对每一个模型所需要的数据少一点，少一点。

而且你引入skill之后呢，你才有那么多的low level sequence，不需要那么多的control sequence，所以其实巨身大模型里边，一个问题其实只有，怎么把这个巨身大模型变成若干个。

小一点的大模型，然后还能把它组织起来，人也差不多，比如我举个例子，当我们做一个什么新的事情的时候，我们第一次去做的时候，我们是会想的，我不知道什么是合适的游戏，打游戏吧，或者是什么的，那你一上来的时候。

你看看你是要很多的基础知识去想的，但是你玩了很多遍之后，你就下意识反应了吧，这就是说，这就是一个，你既有必要有skill，当你反复练习之后，skill又会融合，就这么一件事，所以巨身大模型。

我的观点就类似于蒋老师的，巨身智能它是个很大的事情，你不能说我一把就做到了，没有这种事存在，所以聪明的做法应该是结藕，还要找到中间的藕和点，虽然说，看来我们这里头的观点，相对来说，都是比较偏向于结藕的。

那么，我也不能为了反对而反对，对吧，我个人的理解吧，这种结藕，我也非常认同刚刚四位老师说的，那么这个可能也有一种数据的考虑，就是上层的规划，或者是图文的，你理解你high level要去干什么。

这个部分呢互联网的图文大数据，已经越来越多的能帮我们做这件事了，但是low level的skill呢，具体你怎么做，动哪根手指，对吧 这样的数据没有，所以说可能呢从数据的角度呢，我们是底层的技能。

获得了什么技能的数据，就能学会这一个技能，那学会这一个技能呢，就是一个小的垂直模型，那么可能今天我们有抓取大数据，那我们就学会了物体抓取模型，明天呢我们有什么移动大数据。

我们就能解决机器人在场景中的导航，那么我们有什么搅拌的大数据，我们有什么什么对吧，一个个的技能，那么这样子的话，底层的垂直模型，跟上层的平行的图文调度大模型，对接可能是短期内来看，比较可行的一点。

那么展望未来的话呢，这个可能这个答案呢，就还需要留给各位在座的学者和同学们一起去研究啊，那么在巨身大模型之后呢，其实我们最终想讨论的一个问题，就是可能很多同学也很关心，那么这样的通用机器人。

离我们还远吗，对吧，特斯拉的这种人形机器人，会不会跟人类之间发生一些冲突，发生一些这个威胁，怎么能让人与这样的智能的这个机器人，这个共荣共生，我觉得这个隋老师可以这个谈一谈啊。

好的这个人和机器人如何共荣共生，我们今天已经和这个机器系统共荣共生了，对吧，在座每位都都有手机，而且很难把手机放下24小时48小时，这个这个离开它对吧，就我们已经，我们已经像这个这个需要空气和水一样。

来需要这些信息化的这些辅助工具啊，所以但是但是这里面是其实其实是两层，人机交互要看它是物理层面上的硬交互，还是这种软交互，或者说是现实交互还是虚拟交互，我们虚拟交互的这些这种设备。

已经已经我们这个使用的非常非常习惯，非常非常常见啊，但是物理世界的这些硬交互的，尤其是和人产生直接的这种物理接触的，这些这些机器人，这还是接下来的一个比较比较大的一个难点啊。

那么人形机器人本身从这个现实的应用来讲，其实有一个有一个需要解决问题，其实还是平衡啊，前面我的报告里面给大家看到了，那个人可以靠自己的力量能够站起来，但是平衡不行，到今天为止。

包括我们在内世界上所有想尝试通过这条路径，帮助瘫痪的人完全靠自己力量站起来的，这样的尝试已经在全世界已经有不少地方，在平行的在做这个事了，包括我们国内也会也会后面有更多的地方在做。

我们发现说站起来靠自己力量站起来没有什么问题啊，但是走起来也可以啊，走起来比站起来要更难，但如果想要保持平衡啊，我连个拐棍都不住，我就靠自己的双足这个直立行走，到今天为止还不太能不太能做得到啊。

所以这件事情在人身上是这样，在机器人身上本身机器人系统，尤其是双足机器人系统，它的这个sensor and actuators，它的对于这个力学相关的传感器也好，它的这些控制器也好。

其实跟我们健康人相比上来讲还是差的比较远的，那在这种情况下，其实可能我们更希望的是，至少我们第一代和人来交互共融的这些，共生的这些人形机器人，尽量不会摔倒砸到你，对吧，大家做过机器人的可能知道。

你机器人随便一个东西，哪怕它倒了砸在你的脚上是很疼的，对吧，你这样的一个大的这种人形的机器人，特斯拉要做一个1米75到1米8之间的，小米那个机器人也差不多，它就是要仿人的这个样子，平衡的问题。

在这种日常生活场景下的平衡问题，其实是这个第一步要解决的，所以可能从我个人的观点来讲，可能这种足式机器人里面，不一定双足的会是最早和我们实现这个共融共生的，然后很多的轮式的机器人。

今天大家在酒店在这些地方，很多这种轮式的机器人，已经开始和我们有比较好的交互，可能会有这么一个过程，对所以这个也是一个很好的问题，为了在短期内我们人与机器人共融共生。

那么我们机器人应该采取一个什么样的形态，是二足的人形，还是四足的这种狗形，还是这个当然了也可以是马形，那么还是这个轮式机器人，我觉得这个好像今年是一个挺热的话题。

就是说很多人形机器人的公司都雨后春笋一样的出来了，我不知道各位老师们有什么看法，你们个人对比较支持哪条路线，这个机器人首先得可能巨声智能之后，有了的这个机器人才能可能才能谈到共融共生吧。

我个人因为我做很多多质能体强化学习的方面的工作，刚才那个蒋老师提到了理念，就是有了这个真的是巨声智能的一个机器人的话，他真的能够做什么或者是他如何去预测你的行动，或者你的精神状态是什么。

这个想一想有点可怕这个事情，但是当然我们还没有到了那一步，等我们到了那一步再说，这个机器人的话我觉得只要能，目前来讲的话只要能服务人的帮助人类更好的生活的话，我觉得不管是什么形状都可以。

对可能现在第一步就是防摔对吧，不要把家里的小朋友砸坏了，那么所以说可能人形机器人在这一步，还是有比较大的一个就是挑战吧，那么今天其实我们的报告，就是这四个报告，我也想让在场的各位学者和老师和同学们。

把握这个机会跟我们四位嘉宾进行一个交互，有没有在场的观众想提一提问题，关于我们巨声智能今天的论坛的，好请把话筒给到这位观众，各位老师好，首先我自己介绍一下，我是一个本科生，但是我在特斯拉待过半年。

然后我现在入职的也是协作机器人的一家头部企业，然后我最近也是在思考一个问题，就是关于这个多模态的，就是传统的多模态和我们现在大模型下的多模态，它到底革命性的点在哪里，因为我听人讲就说。

我们能把传统的这种多模态，它是从不同的维度过来的，然后我们从利用大模型把所有的维度融到一起，就像有一位老师讲的，就是说建立一个整体的模型，然后这个整体的模型再去输出它对于这个环境的。

再去输出它这样一个最后动作的结果，我还是没有太明确，大模型它能够给多模态带来的意义，就想听一下各位老师对于这个的理解，我可以理解你的问题是，一般的多模态大模型和巨身多模态大模型的区别吗。

可以这么理解你的问题吗，也可以理解为传统和现在的大模型给多模态带来的什么，是不是说传统那个多媒体多模态的那个领域的研究，和现在的多模态的区别是说这个问题，对对类似，之前的，那我就简单说两句。

就是之前实际上它就是相当于，不管是图像文本视频相当于把它联合学习，不管是把它embedded到一个空间中，还是怎么样子的，在语义概念上给它进行学习多模态这一块。

然后现在的话就是用transformer这种架构，然后所谓的这种多模态的大模型，它实际上还是希望能够建立这种视觉和语言，这样的一个对齐的这样一件事情，但是我觉得实际上还是挺难的。

因为语言那个词和词的对齐可能还行，但是你真的要是跟这个图像中或者视频中的对齐，我觉得还挺难的，不管是数据还是这种训练这里面，当然现在也有一些效果，简单一个感觉就是，虽然现在多模态大模型很火也有一些效果。

但是它是不是真的达到我们想要的效果了，可能我觉得还有待观察，我现在是这种感觉，仅供参考，来 罗老师，对 我猜你说的多模态应该是指的，包括声音什么的 是吧，现在的大模型上面的多模态主要指的是文本和图像。

如果声音的话它其实可以比如说，人说的话的话可以转成文本的形式，这样来输入进来统一成Transformer的输入，包括文和图，现在的多模态基本上指的就是图和文本，没有说声音层面的大模型，我补一句吧。

就是我觉得在这个AIGC的这个Setup下边，这个多模态有些个很神奇的事情，比如说像Dali，这是一个或者Stable Diffusion吧，可能Stable Diffusion说起来更有意思点 是吧。

它既是一个图像的闪烁模型，但是呢它也借助了文本的Embedding Space，帮着它去Initialize一些事情，这样它做出了一些很有趣的玩意儿，有些个这个Embedding Space的差值啊。

它的差值其实很大程度上还是被，如果要是离开了图像，如果离开了语言 只有图像，那么就更像传统的GAN之类的那种，那么你是不大容易弄出来一些个非常有趣的效果的，它的这个文本空间，文本空间很适合。

文本它非常适合组合，非常适合组合繁化，所以其实呢，那么文本和图像和视频和这个3D的结合，文本这边对于它的这个组合性质的学习，提到了很大的帮助作用，但另外呢那些个具体的，跟物理世界有关的那些模态呢。

又补充了文本不能Cover的一个Embedding Space，我觉得这个视角也算是一个有趣的视角，对那我最后呢，就是还是因为围绕巨身吧，就巨身多模态大模型跟多模态大模型，到底有什么本质的区别。

就巨身的话呢，它是根植在一个机器人的形态里的，所以从Morphology上讲，这个机器人形态它能执行什么任务，它有几条胳膊，它有几条腿，对吧，它到底以什么形态去进行运动，进行跟场景的交互。

那么所以巨身多模态大模型，它一旦谈到巨身，那么它的能力就会受制于，这个它自身的形态，同时它的形态呢又能够进一步地，这个去驱动这样的一个大模型，能做什么样的事情，所以我感觉如果我谈巨身大模型。

和普通的多模态大模型，我一定会从它自身的这个形态，和它能做的事情上去区分，这两者的区别，那么我们时间机会非常宝贵啊，有没有其他的观众愿意，好 跟我们嘉宾交流，多谢 我的问题稍微不太一样。

我是从宾夕法尼亚大学来的，抱歉我的中文说的可能有点不完美，所以如果您能允许我说一些，尤其是对于，如果是在Minecraft上面，或者像我们最新看NVIDIA出的，那个Voyager。

在这个Embodyed Agent的，这个Framework里面，可不可以把它用在，我知道我们今天这个Discussion，主要是在机器人和这个Research方面，但是可不可以用在。

如果说金融或者政治这方面，用这个Embodyed Framework，像假如说做一个小Agent，它可以做模拟Trading，这种感觉，我认为是可以的，但是首先需要你的。

Language Model具备，比如说金融的Quant也好，就是量化交易也好，这样的一些，它这个有说像Bloomberg训练的，那个GPT吧，它有这样的能力，可以作为一个Planner的话。

它是可以指导，比如说你的一些Skill，只是做一些高频的操作等等，我觉得是可以去尝试的，那你觉得这个Approach和，直接让一个LLM像Bloomberg GPT，就是给它一大堆Data。

然后让它Train，然后就是一个Black Box下面，更像是一样，这个两个Approach的Difference会在哪里，首先就是对于，Large Language Model的文本的话。

我不一定拥有，就是一些操作，我不知道，有没有一些操作上面的记录，如果这个是有的话，比如说它就是从文本到文本，就是操作也，比如说交易也被记录下来了，那是有可能的，如果没有的话，那可能就不太行。

这部分的话可能依赖于数据本身，所以我们这个Embodied Agent的，这个Approach的主要的Advantage是说，在这个Data有可能LLM没有，这种Transaction Record的。

这种Direct Availability，对，我觉得这里头你用到金融里，你谈巨深的严格意义上，我个人觉得不太合适，因为你做的这些操作，它都是一些抽象的操作，但道理确实是相同的道理。

就是说它其实可以被强化学习，来帮助你的这样的一个金融的交易，因为它都是Action，它都是Decision Making，对吧，那么所以说你也可以，我觉得你完全可以想象，据我所知。

国内的有一些基金是用强化学习，在学习交易策略的，当然了这个也可能很危险，对吧，那你培大了，你强化学习，对吧，这些管不了，对吧，那你可以甚至建立一些，用我们的思想，对吧，我们用巨深起底一下。

能不能建立一个交易的Simulator，对吧，你先在你的Simulator里，学一学交易的策略，然后再把这样的Policy，拿到你的真的市场上，做一些Real World Adaptation。

会不会能止一些损，对吧，可能我只能从这个角度去讲，巨深智能对你们可能的一些起底，应该补充一点，就是有时候你是做交易，有时候你是做Portfolio Management，对吧，然后交易的话。

可能是Portfolio Management，下面的一步，那这样的话，其实就是如果你的任务，是比较宏观的任务的话，那你可能在上层的话，可以用老实谈过之Model，作为Planner，但是比较微观。

或者比较涉及到高频的交易的话，那部分的话，我猜可能用强化学习，这个会更好一点，因为包括国内一些量化公司，也是用强化学习，去做相关的一些操作，多谢，还有观众想提问题吗，好，大家好，刚刚各位老师讲到了。

巨深数据的获取，我想谈一个可能更大一点的问题，就是它的训练环境的构建，然后我关注的场景，可能稍微跟机器人相比来说，更加抽象一点，比如说我举个例子，就是我们虚实的实际的人，和虚拟的智能体。

要协同交互完成一些任务，比如说举个例子是，《星际争霸》这种，这种战略，即时战略游戏里面的任务，我可能真实的人，和虚拟的这个Bot之间，我们要通过语言的这种交互，然后虚拟的智能体，智能体可能是通过。

视觉的这个获取感知，然后他们之间，要通过这个视觉语言的交互，去协同的完成战术任务，当然我关注的可能不一定是，专门是《星际争霸》里面这个环境，如果是别的，比如说我要创建一个，更加真实的三维环境。

去做这种协同任务，那么各位老师就是，特别是卢老师和苏老师，我想请问一下就是，像这种训练环境的构建，包括还要去采集专家数据，或者是还有这个场景的数据，大家对这块的训练环境的构建，有什么框架性的思路和意见。

这个训练的话，因为你是agent和人交互，你最终的目标对吧，对这个的话是不是，可以参考像Alpha系列，然后他用self play的方法，只不过你现在加上了，一些语言的形式，然后去训练去达到这个过程。

因为你最终需要跟人交互嘛，可能self play是一个方法吧，我现在能想到的，我觉得你这个setup，它更像无人驾驶里边对不对，那么既有真人也有无人车，当然了无人驾驶跟你这儿，有个不太一样的地方。

就是说你刚才这个假设里边，有很强的对抗性，无人驾驶没有那么多对抗性，所以如果是这样的话呢，这就涉及到今天几乎，没有讨论过的一个问题，就是多智能体，那么如果是巨身多智能体，这里边的这个博弈的部分。

该怎么表达，这博弈部分呢，在我的看来它可能，不是传统意义下的强化学习，或者是language model去表达的，但博弈也有博弈自己的东西，对吧，比如说围棋是博弈，那么MCTS。

Multicultural Research，这就有用了，你说博弈很大程度上，还是用model based搞掉的，所以你这个地方呢，在这个setup下，你需要想办法去model。

每一个agent的intelligence，你怎么得到每个model intelligence呢，这就引入一个观点，就是我觉得这个，多智能体系统，当然今天都可以做，但是最有趣的多智能体系统。

我觉得它还没到来，因为单体智能还没在呢，对 如果单体都很弱，群体的现象也就没有那么有意思，所以我觉得随着时间的发展，单体越来越强，它们必然到一个多体，训练会很重要的时候，好的 谢谢苏老师。

我另外还想问一下，问杨老师一个问题，就是我们也关注，视觉语言导航这一个具体的任务，这块就是，它既对表征，多模特的表征有挑战，然后对基于表征去做，长程的任务规划和执行也有挑战，就在您看来这种。

混合性比较强的这种任务，它的大概的，本质的解决思路，大概是什么样子，谢谢你的问题，它现在就是做视觉语言导航，它实际上需要几个方面，一个你要对语言有一个表示，然后的话，你要对当前的观测有一个表示。

还要把它们关联起来，同时你还要对你过去的，这样的一个行为轨迹，也要有一个表示，就是所谓的历史信息，然后利用这些信息，然后的话，你实际上，你要做好一个这样的，视觉语言导航，你实际上还要有一个。

这样的一个全局的地图，然后你知道你当前的位置，和在全图地图中的这样的一个，一个位置，然后你才能做下一步的决策，所以它是一个挺复杂的一件事情，所以的话呢，现在从研究角度上来说。

反正有一些benchmark，可以在这上面去做，当前我觉得最，大家主要关注的还是，这种视觉和语言的结合，以及它怎么样和，这种下一步的行为结合，就是主要还是在这一块，反正在我看的一些材料上来说。

主要的还是，这种相当于，它跟之前的那种多模态的那个，本质上我觉得，差别没有那么大，客观说没有那么大，但是呢，就像您刚才提的这个问题，一旦到一个真实的环境中，在一个真实的环境中，如果是。

你还真的要把这个语言给它理解好，然后呢你还得真的要跟这个，你的视觉观念要给它，要给它关联起来，对吧 但是呢，现在实际上真正做的它没那么做，是吧 它没有那么做，它给它关联起来，然后呢才能做下一步的决策。

所以呢 它的环境的理解，语言的理解，这实际上是，都说是这个自然语言是人工智能的，这个明珠，又是CV又怎么重要，我觉得真的要做的话，可能要把这两方面都得给它，达到一定的状态之后，然后才能。

讲这个视觉语言导航，当然你如果纯粹从机器学习的办法，就是从这种inviting的这种角度上，然后做一个预测，当然也可以，再加上强化啊什么也可以，但是我觉得这个事情，反正我今天可能。

讲的事情都是比较保守哈，我都觉得这个事情，每一步都很难，能做出来都很难，但是呢我觉得，真的要做好的话，还真的需要，几个方面都得结合最后，谢谢江老师，那么我们还有观众，这位观众，老师好，其实就是刚刚听到。

那个苏浩老师说，他就是说，我们要做这个inviting的AI的话，它其实是会有一些，infrastructure的一个问题，然后的话infrastructure里面，然后您提到说就是simulator。

然后还有一个就是，我们的fundamental model的一个问题，然后fundamental的话，fundamental model的话，那个卢老师他是认为就是。

LM可以作为一个fundamental，可以作为一个fundamental model的一个近似，但是还，因为我自己我之前去调研文献的时候，我看到的那个，就是我们要去建立一个。

真正能够具备更多智能的这种模型的话，那就是今天早上，的那个professor Leuken他说的，order model，然后就是我不太理解，就是order model它和这个LM。

就是它这种具体的一种体现是什么，因为我自己之前看到的文献里面，就是order model的话，其实它是从那个神经科学的角度去出发，它就是，有一个神经科学家，他其实是，研究了这个，他是认为我们大脑。

它是在对这个东西，它进行一个研究，对这个世界进行感知的时候，就是我们是先对它进行一个预测，然后去先建立了，不断地更新我们对这个世界的模型，然后去建立一个预测。

所以其实这个order model其实是不变的，但是刚刚听你们说，就是我其实不确定这个order model，它是不是变的，因为order model我觉得它可能，对于每一个人来说。

它的order model其实是不一样，对，那么那个，那我来简单说一下吧，我觉得可能这个order model，它的一个概念呢，在学术上比较学术的定义是，对于当前的世界的某一个状态。

当你take一个action的时候，这个世界的状态将发生怎么样的一个改变，那么这个呢，是强化学系里头讲的model，那么这样的一个order model呢，你可以把它当做一个simulator。

让你的policy跟这个order model进行交互，得到大量的这种，你的order model，这个给出的下一步的状态，那么你可以基于它去算reward。

那么这是一个典型的这个model based，reinforcement learning的思想，那么我们谈order model，就是说如果我做这样一件事情会怎么样。

那么large language model，LLM在一定程度上，你可以跟它用语言的方式去交互，我现在在一个房间里头，如果我就是，这个比如说，这个我的脚下有一盆水，那么我跳进去会怎么样。

那么可能large language model就告诉你，水花四溅，对吧，那么虽然它的这个输出，是一个语言上的描述，但它仍然呢，也可以认为一定程度上是，一个世界状态变化的一个状态。

那么所以说我觉得这是我理解的，卢老师讲这个LLM，可以被当做order model，来使用的一个case，所以老师就是，因为我理解那个强化学里面，它的那个model，它其实是一个固定的一个模型。

就是我们其实是先对环境进行一个建模，然后的话，就是但对于真实的order model，我之前可能一个想象就是，真实的order model，其实应该是要去根据我们，对它交互的过程当中。

其实order model它其实是会改变，就是我们现实世界的一切，其实都是物理学支配的，你从这个角度上讲，物理学就是我们这个世界的world model，如果你能model所有的一切原子。

什么的运动全都能model的话，那这就是你的order model，只不过我同意你说的一点就是，我们做的model不可能是一个大统一模型，把什么东西都完美的model，所以它可能要被update。

但是它并不一定在用的过程中，它要实时被update，这是我的看法，好谢谢老师，好那么我们今天，今天由于时间所限，我们就非常感谢我们四位speaker的到来，那么我们北京智原巨身，北京智原今年也建立了。

智原巨身智能研究中心，我们从物体的抓取，到物体的功能性的操作，灵巧手到三维世界导航，也做了一系列工作，那么我们认为从物体抓取，灵巧操控 寻物导航，等这一系列的技能，将能帮助我们建立一个。

通用的移动操作平台，最后打的这个广告就是希望，如果大家感兴趣这个巨身智能，特别是为了通用智能体，build这样的一个移动操作平台，可以联系我们去研究科学家，或者是实习的岗位。

再次感谢所有到场的嘉宾和观众们，谢谢大家，今天我们的论坛到此结束，谢谢大家，谢谢大家，谢谢大家，谢谢大家，谢谢大家，谢谢大家，谢谢大家，谢谢大家，谢谢大家。

谢谢大家。