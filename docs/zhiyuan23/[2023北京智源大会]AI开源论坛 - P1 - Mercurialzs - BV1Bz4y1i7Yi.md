# [2023北京智源大会]AI开源论坛 - P1 - Mercurialzs - BV1Bz4y1i7Yi

尊敬的嘉宾女士们先生们大家下午好，非常欢迎大家来到2023北京志愿大会AI开源论坛，我是今天的主持人杨轩，我来自Linus Foundation亚太区，简单讲两句。

Linus Foundation是全球最大的开源组织，非盈利组织，而LFAI and Data是Linus Foundation下边，专职的是人工智能领域的基金会。

同时LFAI and Data也是全球最大的AI领域的开源社区，现今AI开源已经成为人类超大规模智力协同的最佳组织方式，可以说没有开源就不会有今天AI的成就，现在AI已经成为人类开源创新的主战场。

今天我们有幸请到了非常多的AI领域的专家，一起探讨从AI与数据的开源到大模型的机遇与挑战，我们希望这次的大会能够对大家未来的发展有所帮助，工作上或者是事业上有所帮助，另外我们也希望，我在这里再呼吁一下。

希望能够有更多的朋友能够加入到AI开源的社区的行列，我们在LFAI and Data这边，我们有大概40到50个开源项目，这些也都欢迎大家来去使用参与这些项目，今天我们有请第一位嘉宾。

这是我们有请LFAI and Data董事会主席，杜俊平老师为我们带来主题演讲，AI与数据开源挑战与机遇，有请杜老师，感谢主办方支援和林老师邀请，今天给大家介绍一些关于AI和数据领域开源的挑战和机会。

借着这个场合我首先介绍一下LFAI and Data基金会，LFAI and Data基金会是一个开源的软件基金会，它是一个非盈利组织，但是它托管了在AI和数据领域全球最重要的一些开源项目。

同时在我们当前的基金会里面，大概全球有50个左右的member，包括像咱们国内大家熟悉的一些大企业，甚至包括资源研究机构也在里面。

同时我们全球托管了大概有46个这样一个关键的technical project，也有超过接近2万名的开发者，为我们的AI和数据类的开源项目在持续的贡献。

这张图能看到LFAI and Data作为一个大的基金会的开源社区，它的开发者的规模，它的技术的这种飞速的发展，过去的5年成长了大概5倍，531%的成长，一方面也反映了当前在AI这个领域的开发的趋势。

就是更多的开发者，更多的开源的公司投入到AI这个领域，也涌现到开源这个领域，同时我们也希望借着这样一个平台能够更好地拓展我们的，让技术更好的发展，同时这里面的这些关键的project。

包括一些深度学习框架，包括这种ONEX这样的一个框架之间翻译的平台，包括Hardware在分布式的这种学习框架，甚至包括项目资源也有FLAG AI在这个里面，然后这些top的organization。

大家耳熟能详的像亚马逊 微软 Meta等等，像国内的华为 百度 阿里，都在我们的组织里面，或者是member，或者是包括投资的一些关键的项目，我们的这个基金会它的运作主要是，相当于是一个分层治理的架构。

有government board就负责董事，整个基金会的一个层面的治理，另外我们有technical coordination，就是tech meeting。

我们的TAC technical advisor，它是专注在技术层面的治理，包括我们整个开源社区这些项目，它的life circle从sandbox到incubator到graduate。

它是一整套的一个相当于毕业的演进流程，那么我们typical来认为，sandbox阶段是一个开源项目它的总结段，在这个阶段里面更多的，我们会关注它的开发者的生态，更多的这种开源的开发者。

能够加入到这个项目当中，那么到了incubator就是，更多的它是开始拓展自己的用户，就它已经有足够的开发者，有些完整的功能能够适应在一些场景之上，然后再往前走它就是一个graduate的状态。

就是它的功能比较完善，它的生态比较丰富，用户也积累的比较丰富，它可以进行一些规模化的落地，这就是它整个完整的开源项目的life circle，那么回到我们今天的正题，就是我们再看看当前我们所属于的时代。

实际上当前我们的时代在，不管是工业界还是学术界，大家的认知都是一致的，就是我们认为这是属于AI的技术爆炸的，一个起点时刻，就是我们在同样陆续在，在这个关键时刻走向通用人工智能，那么现在的一个。

很多这种buzzword就是说，从前几年的我们的Machine Learning，Deep Learning AI Framework，慢慢的现在更多的大家听到的都是。

Transformer 然后Attention，AIGC Large Language Model这些词，其实这些都看到这两年每隔，每隔一年或者两年它的焦点都不一样，尤其是当去年底。

ChaiGBT出来之后，实际上引爆了一个核弹，快速的走向这个，让这个AI的生态快速的能够蓬勃的发展，那实际上业界把这些模型，现在我们看，现在应该是一个模型，飞速发展的时代，那么业界把这些模型。

从这个小模型到中模型，中等的模型，大型的模型，超大的模型，每个模型的，它能干的事情也看得比较清楚，一些小模型可以做一些，简单的一些阅读理解，包括这种Debugging的工作，然后中型到大型的这种模型。

类似于GPT-3，GPT-3。5之类的，它可以做一些，包括GPT-4，可以做一些这种GRE层次的这种阅读理解，甚至包括一些对于这种类比，比喻，还有一些包括逻辑的推导，一些这个生成，代码的生成。

我们的在Github上的Copilot这些，其实很大程度上提高了我们，在这种开发生成这些文档，包括这种写代码开发的效率，其实未来我们看到随着这个，我们的这个模型会越来越，发展可能会有些进一步的一些。

功能会被开发出来，包括它有一些初步的一些，更强的一些自我的意识，或者更复杂的一些工作，甚至是一些argument，它都可以去做，这就是一个我们这个时代的继承，它是一个飞速发展的过程，那么实际上去年。

两周前我们看到Daybreaks，发了一个报告，就是在2023年的，他认为这个State of Data+AI，有些有趣有意思的数据，比如说，AIGC这个，就是TiGPT发布以来的几个月。

实际上大家用这个API的模式，或者是这种模型工具链的这种模式，会成为这个主流，在这之前大家都是，每个公司自己可能逊一些，或者是依赖于一些，开源的开放的一些模型，那么在这之后可能直接。

我们就开始用这个API去访问这些模型，那么短短的半年时间，从去年底到今年的5月份，这半年时间，这种直接对于API的层次的，这种访问量，升了1310%，也就是13倍，那么同时呢，现在在NLP的这个领域呢。

在整个这种科学计算，在整个Python的这种Data Science这个领域，它占的这个比例，已经得到大概占到了50%左右，就非常的popular，也占用大量的这种。

这种Machine Learning和科学计算的这种任务，那么同时呢，还有一个意思，还有一个有趣的现象就是，企业对于模型的重要性，也是越来越有更强的认知，那么在过去的一年当中，模型上线的。

在生产量上线的模型，实际上翻了400%，甚至包括在这个过程当中，大家的成熟度，使用模型的成熟度也提高了，以前呢一年之前，大概是每5个模型，就是在测试阶段，最后变成一个生产模型，那么现在呢。

这个比例是3比1，就是每3个处于这个，测试阶段的模型呢就，就有一个进入生产，实际上这种比例的变化也，也意味着我们在，AI这个info或者是整个，这个行业的应用当中慢慢地走向成熟。

一方面是我们认为这个大模型的生态是，现在发展的是欣欣向荣，但是从另外一个层面我们看到，冰山下的部分永远是数据，某种程度而言，模型只是数据的一个，一个转化品或者衍生品，它是一个，这个模型是数据的。

在某一个切面上的一个投影，或者是一种折叠或者是一种压缩，所以一个高质量的这个数据集呢，实际上是可以训练出，这个不同的维度的，这个多个高价值的模型的，那么作为这个冰山之下的部分，这个数据的重要性呢。

就是大家一直在强调，一直在提，但是始终呢大家都觉得，这块可能有很多各种各样的挑战，那么今天我们待会儿也会去，更多地去讨论一下有哪些挑战，其实第一个就是，从这个模型训练的角度来说，这个数据从来就没有够过。

这个够体现在一个是质量一个是数量，我们需要更高的质量，然后更多的这个数据等等，不管在整个模型的这种产生，诞生从训练阶段是吧，到后面的调优阶段，到后面的这个推理到。

生成之后的这种Prom Engineering，其实它都离不开这个，不断对它进行这个数据的一个投喂，所以就是如果我们说大模型就像一个，这个贪吃的，贪吃蛇或者一个贪吃的怪兽，那么就是它一直吃不饱。

始终是有更好的这个，需要更多的数据和更高质量的数据，那么同时呢很多的企业呢，初步已经感受到这个模型的Power之后呢，下一步就是说如何能够更好的，提升我的这个模型的这个能力和质量。

也是在数据这块层面做很多工作，就是所谓的这个Data Centric的工作，那么实际上当前的这些数据呢，来源无非就是三个来源，那么第一个来源呢就是自己，去这个去收集一些数据，去扒取一些数据。

那么第二个呢就是，从第三方去购买是吧，或者是获得一些数据，那么第三个呢就是通过，Public Available的方式，找一些公开的数据集，类似于我们的Hugging Face，在这个之上去获得一些。

但是这三个层面上呢，多多少少还都是有一些挑战的，第一个从主动收集领域，其实很多我们现在的数据呢，很多的公司还是在管这个公司，它的这个私欲，属于在这个私欲之中，很难从外部去获得，而且以一种合规合法的。

这种方式去获得是比较难的，那么另外一个层面上讲呢就是，对于这种第三方的购买，或者交易的这些数据呢，实际上也是有很多挑战的，因为这个数据的价格如何定义，然后这个数据如何定义它的质量。

或者是它的对我的这个外部的公司，外部的企业它能够真正产生益处，所以这个，所以有很多的这种数据交易市场，国内我们也有一些，海外的也有一些，甚至在一些云服务上，云服务厂商，包括这种数据的这种。

production，或者数据的SaaS service上，都有这样相应的market，但是目前来看它的交易量，交易的数量和规模其实还没有，没有达到这个理想的状态和要求，那么第三个呢实际上是在。

这种公有的这种数据，那实际上我们现在看到很大量的数据集，还是不是一种完全开源的状态，它可能还是在，商用或者是方面有很多的限制，就是你可能做研究可以，但是你不允许商用非法，或者是当你训练完了之后。

你的这个预训量的模型，或者将来的这个AI的模型的产品，不允许不能够限制，那么实际上这也使得我们的，可用的商业可用的这些数据集都是，很有的限制，我认为这个是一个很重要的挑战，在三种渠道上。

各有各自的这种挑战需要去做，那么另外呢还有，就是看到这个数据的这个，quality还是quantity，大家会有很多不同的，这个要求和纠结，现在基本上大家达成一个共识。

就是quality over quantity，就是数据的这个质量可能优于它的，它的这个quantity，因为其实质量呢，它决定了这个模型是吧，是否精确，是否是相关，是否会不会有一些bias。

会不会有一些就是不公平等等，但是quantity呢它实际上也是有用的，它包括你大量的数据训练出来的模型，它会有更强的这种泛化能力，包括它有更强的这种robust能力，它不会说这个过于依赖。

某一部分的这种数据，然后同时呢它看到一些这种，没有见过的情况的这种case，它能够很好的处理，所以quality和quality这种，经常是也是有企业会去纠结的，那么我们认为就未来的方式呢。

就是可能在数据，在保证数据质量的情况下，我们更高的，更多的获取数据，然后同时用一些，比如像现在当前是用人工的方式，去打一些标签，或者是做一些相应的这种工作，未来可能会更多的自动化的标签。

或者有相应的开发出相应的模型，自动的会给我们的这个数据，贴成共同栈的这种标签，那么可以更好的训练我们的，我们的这个模型，然后同时呢在这个，data set的治理层面，其实有很多的这种问题。

其实原数据的治理呢一直在，在业界就是一个问题，包括我们看到很多企业内部，大的公司内部，不同的team不同的团队，其实互相之间呢因为，一些沟通的问题或者是，部门墙那些问题，其实没有办法，很好的共享数据。

哪怕这些数据呢对，在公司内共享是很有价值的，那么第一个难点就卡在这个，数据原数据的发现，就是我们能不能，有一套这个原数据的标准，很好的定义不同的，不同的这个，这个数据，数据集它在做什么事。

我们其实看到比如，以这个hacking phase为例，其实同样是文本数据是吧，MDB的这个数据集和这个Wiki，就是还是有一些差别，那么对于，对于我们这个普通大众来说，他的认知还好，还是能够理解。

但是一旦，这个牵涉到企业不同的，不同的业务，它的业务逻辑在里面的时候，这个，互相之间就很难去，达成一个一致，所以在这个层面上，数据集的这种metadata是，需要一个标准，我在在这个领域其实LFAM。

data基金会可以做的更多，我们可以做更多的，这种相应的开源的标准的，这种工作，那么从另外一个层面，就是从治理的层面，实际上当前的很多时候，在强调这个data的governance。

这种data governance，是包括一些原数据，包括一些数据的血源，但实际上在当前来看，除了数据这块需要治理，其实在整个，Machine Learning的Pipeline上，有很多更复杂的东西。

包括我们从，数据的这个预处理之后呢，会有涉及到特殊人工程，对吧，在后面的model training，包括这个management的serving，在后面现在还有这个。

Prompt Engineering，所以这里面，有很多大量的东西，你有很多从数据，从data set到，这个feature的映射，从feature到model映射，model到后面的一些。

Prompt的一个映射，这个形成了很多，这种网站的关系，但当前现在是没有一个，很好的一个治理的产品，或者治理的一个框架来去，来去这个完善，那么我们认为这个是，市场上，这个急需的一些方向。

那么刚才说到的这些挑战呢，其实还有一个很关键的，就是在于，我们看到在，更多，更多的讯息，我们可以看到，在这个网站里面，有很多的一些网站的讯息，在那边呢，比如说有个网站的讯息，就是说有一个网站的讯息。

是说有一个网站的讯息，是说有一个网站的讯息，在这个网站的讯息里面呢，它是专门的讯息的，讯息的一个网站的讯息，它是专门的讯息的一个网站的讯息，那这个讯息的讯息呢，它是专门的讯息的一个网站的讯息。

那这个讯息的讯息呢，是专门的讯息的一个网站的讯息，在网站的讯息里面呢，它是专门的讯息的一个网站的讯息，在网站的讯息里面呢，它是专门的讯息的一个网站的讯息，多的挑战，其实还来自于，包括我们的数据的。

全球的分布，然后多区域多云，造成的新数据孤岛的问题，我们看到现在很多这种，致力于，拆展国际化业务的公司，其实面临着越来越，严苛的一些数据的监管，那么怎么样去兼顾，以合规的前提，就是兼顾监管的要求。

但同时呢从全局的角度，从业务的角度，它需要一个全局的视角，全局的视图能看到，这个不同的地域，它的这个业务策略，和数据驱动的这些策略，其实这里面，其实有一个很强的一个gap，那么这样的话如何统一。

去做这个数据的分析，治理，训练，推理，其实在，当前越来越成为各个，大公司或者是有国际化业务，有，这种并购业务的公司，它的一个很大的一个，难点，那么近年来其实像，如果我们关注像Berkeley。

他们也提出了这个，Sky Computing，一个云联邦的架构，某种程度上在IS这个层面，解决了缓解了这个多云带来的，这种复杂性，和这种性能问题，但是要解决，彻底解决这样一个，多云造成的这种数据割裂。

数据孤岛的问题，或者模型的孤岛问题，其实还需要，这个更颠覆性的，产品和技术出来，但是虽然，这里面讲了很多的问题，就是很多挑战，但我觉得这里面，其实意味着更多的机会，很多这些机会，很多这些问题。

就是之前是隐藏在水下，那么现在随着，这一波的，这个，大云模型的这种蓬勃发展，实际上我们有更多的可能性，我们的问题更多了，但是同时我们的这个手段，我们的能力，包括我们的这个聚焦的资源，也更多了。

那么现在有很多，聪明的工程师，包括很多的资源，都在快速涌向，这个AI这个领域，和数据这个领域，我觉得，我对未来我们快速，这个领域快速的这种发展，充满了信心，那么我们现在，正在做的这个。

这个我的一个初创的公司呢，也在以开源的方式，来解决，AI和这个开源，和数据开源，困难的各种问题，这个，用开源的方式解决，这个AI和领域，和数据领域的这些，最尖端的通电问题，一直是我的，这个，这个秉持。

这个我的这个，理想，也是我的，一个公司未来的一个愿景，那么也希望，更多的小伙伴可以跟，跟我们一起，就是在这个领域探索，创造一些伟大的技术，创造伟大的产品，好 谢谢大家，感谢独老师的精彩分享，接下来有请。

智原研究院副院长，兼总工程师，林永华老师，智原研究院，自然语言多模态组负责人，刘广老师，带来主题分享，智原Open，Flag Open，大模型技术开源体系，开启大模型时代新Linux，生态建设。

掌声有请，[音乐]，谢谢，等会儿，这个session，有我还有我同事刘广，等会儿会把他请上来，那首先我还是想，借今天的这个机会，感谢杜君平老师，还有感谢Linux基金会。

Linux Foundation，AI and Data基金会，对我们智原大会的支持，以及组织了这么好的一个，AI开源的论坛，我看到后面大家都已经，站都站了，包括坐在楼梯上的小伙伴们，前面其实还有座位。

大家也可以坐，那个开源其实是很重要，不好意思我拿一下翻页的，开源很重要，包括在这一次智原，我们发布大模型，其实我们很重要很重要的一个keyword，就是开源，所以为什么说，AI开源论坛也是这次。

其实历届智原大会，都是很重要的一个话题，这个就不说了，今天说了太多了这个，好，我发现那个用的这个图，跟杜老师的图一样，但的确就是说看到了实际上，虽然现在从去年下半年到现在，很火热的例如像AIGC文生图。

例如像TRACKGPT，大家其实看到了冰山上的部分，但水面之下大家看到的angle不一样，从智原来看这个数据当然是，很重要一部分，还有很重要的是它整个，冰山水面下的技术站，那这里头的技术站包括了。

各个重要的基础模型，包括语言视觉图文文生图等等，也包括了我们这些数据集，以及做数据集很多的重要的工具，还有大模型的评测的方法，另外还有就是支撑整个大模型，高效训练的AI的系统技术，那这里头也是很多。

包括框架的并行优化，平台的调度算子优化，甚至AI易口芯片技术，这也是为什么我们其实是说，AI系统是今天上午，我们特别也是有一个专门的论坛，去讨论，那对智原本身来说，正因为看到这个水面之下。

最重要的这个技术站，所以这是我们的定位，就是说我们需要帮助整个产业，科研这个产业把这个技术站，整一个打造出来，那这里头就包括我们的，一系列的基础大模型，从昨天早上，我们的整一个全体会议的时候。

announce的我们开源出来的几个大的，这种包括语言视觉跨模态的大模型，也包括我们自己的数据集工具，AI基础大模型评测，这也是等一下下一个session，介绍这个由我们杨希杨博士。

介绍那个FlyEvo，去介绍，然后还有我们的整一个，九顶的自算平台，那首先我也想趁今天这个机会，来说一下为什么我们智原，要走大模型开源开放的道路，其实很简单就是说两个原因。

第一个是说推动整个社会资源的合理使用，包括数据和算力，其实基础模型，如果我们这个基础模型，它不是一个行业性，而是一个通用性的话，其实它去构造这个基础模型，所使用的东西是很类似的。

都是需要我们有一定比例的，这个互联网数据加高质量的数据，也需要海量的这个算力，所以其实大家是想就是说，如果基础模型，尤其是通用性的基础模型，没有开源出来，也进一步没有能够是，以商用的版本开源出来。

那势必只能够逼得各家企业，自己去重复性的去造这个轮子，而这个轮子很高昂很昂贵，我在昨天的那个我们，新发布的那个天鹰大模型的时候，我第一页就说，为什么我们认为基础大模型，就像这个就类比AI中造一个CPU。

第一个原因就是它贵呀，那贵是贵成什么样子，那资源是什么样子，当时我们就至少是，小几千万到大几千万这样一个量级，而另外一个就是说，哪怕是金钱不是问题，但算力以及推动算力，运载算力背后的能源。

也是很重要一个问题，大家要知道是说，现在我们实际上，付给去租用这个GPU服务器，哪怕是英伟达这样子的，它在能耗比上面，已经算是做得很不错的，其实我们很重要的一部分钱，就是付给了这个电费。

所以真的没有必要重复大家去造这个轮子，这个轮子都是通用性的，何不就是有人能够把这个东西开源出来，并且它是商业可用，但当然重要一个是，可以保证这个版本可以持续地往前迭代，那第二个很重要的是。

它这个基础大模型，今天它已经不是只是一个理解，它是一个能力的生成，并且它是一个认知对外输出，价值观对外输出的东西，因此它对社会可能带来的影响是巨大的，因此我们在训练这些基础模型的时候。

所使用的预训练的数据，也是相当的考究，这个可能大家也能留意是说，前阵子国家网信办也有一个征求意见稿出来，其实有很大一部分也是在探讨这个数据的安全的问题，其实志源我们最近也对持续的关注。

check我们在今年1月份到5月份，全球开源的这些通用的语言大模型的统计，这里头是有一些数字，未必完全准确这个总数，国外开源的语言大模型一共有39个，其中可以商用。

并且并不是使用copy left的协议的大模型有16个，那为什么这两个东西很重要，因为我们来看科研是一个问题，但现在最重要的是怎么推动这个AI产业落地，产业落地必须我们要符合产业的游戏规则。

而产业的游戏规则是说，你要用的东西必须是带有可商用的版本，非商用版本其实是对企业未来的发展，它的使用是有risk的，那另外一个copy left的协议，这个因为在座都是对开源可能就已经从事很多年。

就知道例如像GPR这种类似的license的话，现在我们也看到有一些模型开源，也用了类似这种copy left的协议，那这种协议它定义的是说，只要在这个模型上面，further derived。

continuing training的模型以及它的微调的模型，都必须开源，那这个实际上对于很多企业的商业的利益的保护，是很有弊端的，这也是为什么其实咱们看整个开源界它使用的版本。

也是越来越多的开源的代码，使用像Apache MIT这种，BSD这种copy，就是不是copy left的license，所以这是一个，那另外咱们回归到看咱们国内开源的，发布的语言大品模型有28个。

开源的数量只有11个，那其中我们同样也去看它这个开源可商用的版本的模型，目前只有一个就是Belly的一个小的一个，基于Broomsea的一个指定微调的对话模型，所以这里头可见是说。

为什么这一次致源我们在发布我们的，Aquila这个天鹰大模型的时候，直接很干脆地使用这个可商用的license，以避免企业的一个顾虑，那致源我们这一次开源的整一个就是我们的大模型数。

但实际上最重要的是底层的这个基础的大模型，在这里头我就不多花时间，那开源开放使得我们可以站在前人的基础上去前行，那因此，这个就是为什么致源刚才说我们打造的是，冰山水面以下。

这些对于咱们构造大模型应用很重要的技术的部分，而我们实际上在今年的2月28号就全面开源，发布出来，这里头包括了最核心的Flat AI大模型算法开源项目，这里头会包含我们致源最新发布的。

天鹰大模型也是放在这个里头去进行发布，这个等会刘广博士会给大家介绍，哦 by the way，我要强调这个大模型算法开源项目，我们去年6月份正是第一个version开源出来的时候。

就第一时间donate给Linux基金会，donate到AI and data，这是因为我们希望是说，以这样一个决心来促进，咱们在中国主导，但是全球可以开放使用的这样一种合作的氛围。

那besides这个Flat AI，我们还有Flat data，这个主要是做数据工具的开源，另外还有Flat eval，这个大模型评测开源项目，这个等会第二个topic会讲。

还有最后是说下面一个我们对AI系统开源的评测，那算一下这个Flat perf，它是其实有点对标ML perf，但是ML perf的话它实际上。

主要还是比较traditional的workload作为benchmark，而进入我们大模型时代，实际上我们需要新的一批大模型的workload，来牵动我们eagle芯片的发展，同时我们希望是说。

在这个用来帮助做系统性评测，芯片性能评测的时候，不要只有Pytorch一种框架，我们也是积极的去跟Pytorch Pytorch，Minespot这些国内的框架合作，使得是说大家对芯片的评测。

可以也基于多种的框架结构，那下面我们很高兴也获得了，这个不同的包括天数，天数智星 昆仑星，包括华为生腾的等等芯片厂商的支持，陆续的把他们的一些相应的部分，也加到我们这个评测的开源系统里头。

那我们希望打造的是说，帮助我们eagle芯片的用户，可以很快拉起一个完整的测试，这个因为等会会讲我就skip掉，那这个是我们的flat data，flat data实际上它是一个工具的开源。

那包括了数据清洗 统计分析等等，其实这是智源一直在过去三到四年，做大模型域训练数据的时候积累的工具，然后，诶 等一下，我看这个是不是有点，哦好 不好意思啊，那个我要讲的部分已经讲完了。

然后在此我邀请刘广 刘博士，刘博士是我们智源负责，这个自然语言处理和多模态生成的研究组的负责人，哦 同时他也是咱们flat AI的负责人，谢谢 谢谢林老师的介绍。

首先就是接待林老师后面的演讲是非常有压力的，然后我觉得是给大家简单先介绍一下，我们flat AI做的一些事情，首先我认为坚定地去开源，肯定是会推动和降低整个大模型的门槛，我们也希望能够通过开源这种方式。

让更多的小伙伴们一起加入到，这个大模型这个行业发展中来，然后虽然看到这个曲线它的stop并不是特别多，因为还等待大家各位的一些帮助然后投入，但是我们想做的事情是说。

我们希望打造一站式的高效易用灵活的大模型的算法与工具，去解决刚才上一页PPT中讲到的一些问题，就是现在我们训练了Aquila，然后Auto Diffusion等等这种大模型。

在这个训练的过程中我们面临了很多的问题，首先第一个是说现在有很多开源的加速的，各种模型数据并行的一些开源框架，但是这些框架特别的，就是开源技术特别的复杂，然后标准不太统一。

在这种情况下去学习和使用某一种开源框架，进行大模型的开发是比较困难的，当然如果你坚定地只选择一种，然后坚定地去为这一种框架去排坑是可以的，像我们之前一样去一步一步把这个坑都掏完。

也是可以的 但是这个时间周期会比较长，第二个是说各种不同的加速技术，然后它们复杂地交接在一起的时候，组合在一起的时候，会影响到整个系统的稳定性，导致你训练不收敛，或者导致你训练的效果不达预期。

在这种过程中怎么样去解决这种兼容性差的问题，所以说这个是技术比较复杂的一个点，第二个点是说现在有很多开源模型，大家可以通过很方便的方式得到一些开源模型，但是开源模型它的质量怎么样，它能达到一个什么效果。

其实我们是在真实应用的场景中是很难去，去感知或者去体会到或者评价得到一个比较好的结果，我们需要投入大量的时间和工程的人员去，把这个算法复现然后下来，然后去做一些对齐。

在我们的真实的业务数据中去做一些验证，所以说这个成本非常高，花了大量的时间验证了很多我们不用的模型，第三个是说除此之外，大模型是一个新兴的领域，这个新兴的领域有新的技术，有新的研究点 有新的问题。

那么我们在这么一个复杂的工程中，怎么样去让更多的人去接触和加入到大模型的这个行业中，推动这个行业的发展，是我们希望能够做到的事情，所以说在这么一个时间成本 人力成本和经济成本非常高的一个大模型。

开发的这么一个问题和前景这个行业中，我们希望通过开源的方式来做一些贡献，对我们提出了一个叫FLAG-AI的大模型开源的框架架构，我们的KUILA模型和之前很多视觉和动模态的模型。

都是在这个框架下进行训练推理，然后开源出来，然后在这个过程中我们有很多坑已经掏过了，所以说大家可以很方便的基于我们踩到我们肩膀上继续前行。

我们之前的话就是用的BMTRAIN对BMTRAIN进行的技术进行了升级，然后提高了它的训练的稳定性和提高它的训练效果，同时我们把微调的LAURA推理加速，ONX大文件的处理等等。

甚至于国产芯片服务器的一些支持，我们都做了很多很多工作，近期我们也是准备把PETOS的FSTP这种技术做个集成做个加入，所以说整个如果需要去做一个大模型的新手的入门。

甚至到中级你要训练一个中等或者是大规模的一个大模型，都是已经完全可以满足需求，所以说大家可以去试用，然后我们也可以在开源社区中空着去讨论，去探索怎么样的一个更好的技术能够发展。

或者是去训练更好的一个大模型，因为我们现在还是在一个，因为我们的人力肯定是有限的，所以说希望能够利用开源的力量，让大家一起来加入到开源社区，通过扫码加入我们的群里面，然后我们一起去讨论一起去交流。

然后能够共同的去推动，不只是FLAG AI，甚至是整个我们中国或者是整个世界开源的一个项目的一个发展，我要讲的这么多，谢谢各位，谢谢，感谢林老师刘光老师的精彩分享，接下来有请北京志愿人工智能研究院。

技术平台智能测评测组负责人杨曦老师，杨曦老师将会为我们带来，其实林老师刚才已经讲过了，叫FLAG EVAL大模型评测开源项目，OK 掌声有请，杨曦老师，[音乐]，[音乐]，大家下午好。

我是志愿研究院的杨曦，然后我今天代表我们志愿研究院跟大家分享一下，我们最近发布出来的FLAG EVAL天秤大模型的开源评测体系，以及相关的平台，首先来说的话。

大家都知道大模型其实是一个快速发展的这样的一个领域，早在2021年的时候，其实IDC它就有了已经有了一个报告，当时它就预测说超大规模的大模型的发展将会引领一个新的潮流。

当时的人们就会觉得它是一个会在产业以及学术界会有非常大的一个发展，所以的话就是说在各大企业以及高水平的一些研究机构上面，都会在大模型上面去做一些发力和布局，我们也可以看到。

特别是从China TPT出来之后引发了一波更大的一个热潮，最近的一段时间，不管是模型的数量还是说大模型所处的领域，也都会越来越多，可以说是到了一个百花齐放的这样的一个境地，首先来说的话。

其实是说数模型大模型它在NLP领域的话是首先爆发的，但是它的爆发的话，对于我们评测来说，其实就带来一个非常新的一个挑战，难点在于是说，首先大模型特别是大语言模型，它的潜力其实是很难刻画，很难评价的。

也就是说我们的大语言模型，它是规模大，然后结构复杂，它具有非常大的待开发的潜力，但是我们很难捕捉或者是说刻画到它的潜力，它具体的一个形式以及上限是什么，我们如何能够捕捉到它的一个上限，以及它的能力。

其实对于我们评价来说，其实是一个比较大的挑战，直观上来说，例如说我们有一个赛车，有一个公交车，我们用传统的方法来说的话，它们其实是都可以达到一种，完成一个任务，就是说达到我们想要的一个指定的地点。

但是呢，其实这种评价或者是这种任务，并不能够体现出F1赛车它的这种速度，也不能体现出公交车，它的一个载客的这样的一个能力，所以是说我们要怎么去刻画大模型的这种潜力，或者是它的能力。

其实是我们要关注的一个点，第二个点是说，在我们传统的评测上的话，其实是以任务为先的，那这种的话呢，就是说传统小模型的时代的话呢，是说我有一个任务，然后我在任务上有一个好的表现，那我这个模型其实就是。

就会比较好，我就可以去商用，可以去落地，但是大模型的话呢，它其实具备了很多的通用的能力，那它已经突破了单一的这种任务层面的一个限制，在很多的下游任务的过程中的话呢，它可以完成很多的这种下游任务。

因此的话呢，就是说从任务的这个维度再去评价的话，其实还是远远不够的，另外的话就是传统的一些基于任务的这种评测方法，以及评测的基准，其实在对大模型的这个评价上面，其实已经有失效了，那这里边呢。

有举出来一个例子，这个是斯坦福大学，他们对大语言模型提出来的一个HELM框架，在它这个框架里面，它就提出来说，在传统的一些，例如summarization这样的一个任务上来说。

传统的ground truth，所谓的ground truth就是人写的这种参考答案，其实从质量上来说，已经不如大模型它输出的这种答案，那同样的话，基于这种ground truth来做出来的。

例如说round 2这样的一些指标，它其实也失效了，所以对于大模型来说的话，怎么去评价，以及它的评价的指标是什么，其实也是一个需要探讨的问题，那第三点的话，就是原有的一些，我们可能更关注的是。

例如说在准确性上面，的这样的一些指标去评价，但是呢大模型的出现，那我们可能要考虑到会更多，例如说鲁邦性，例如说它的一些不确定性，例如说它的一些效率，那这些的话都会影响大模型的，它本身的一个。

会对大模型的它的整体的一个发展，提供很好的一些帮助，那我们也希望能够更全面的，一些有指标化的体系，去评价大模型，那第四点的话是大家都知道啊，训练一个大模型非常的难，那它的成本，算力成本。

人力成本都非常的高，那就我们职员而言的话，为了训练大模型一天的，就是一个语言模型，一天的它的一个成本，大概是在十万元以上，那如果我们在训练的过程中，不能够及时的看到，看到模型存在的一些问题。

以及模型的发展的一个趋势，我们就放任着模型自动化，让它去跑，让它去run，这个的话会造成，极大的资源浪费，同时的话是说除了是说，算力人力以及经济上面的，这些浪费以外的话，它会带来的这种环境的影响。

其实也是巨大的，所以的话呢，在这个过程中的话，就需要评测与训练相结合，希望的是说通过，通过评测为训练过程，能够提供及时的反馈，提供及时的监测，那如何去把两者去结合起来，其实也是一个比较大的问题。

最后的话呢是说，其实大模型目前而言的话，缺少权威的中立的，一个评测的榜单，那HELM的话呢，就是斯坦福大学出来的，那个HELM榜单，其实给我们指了一个，比较好的方向，但是它仅仅的话是，支持英文。

它不支持中文，在我们中文世界上面的话，其实并不是很友好，那这个榜单的意义在于是说，不管是对于科研团队而言，还是对于企业界的，想要去做大模型，相关的一些落地的企业而言的话，他们在选行上面。

其实都存在一些问题，因为大模型它想要去评测，需要的算力和人力的成本，其实是巨大的，对于有一些团队的话，想要把所有的这些模型，都做一遍评测的话，其实没有这么多的成本，对，所以因此的话。

我们就是基于上面的几个考虑，我们致源的话，在科技部2023的一个，重大的项目的支持下，我们就要开发了一个，FlagEvo开源的，这样的一个评测的平台，那我们希望的是说，能够在这个平台上，覆盖全领域。

覆盖整体整个的模型，它的一个生产的生命周期，并且的话支持多芯片，特别是国产芯片的，一个芯片和框架，所以我们目前的话，这个开放平台的话，第一期推出来了，三种评测的一个方法，一个是说，我们对大语言模型的。

它从能力任务指标，这个三维的话，我们做了一个整体的拆解，另外的话，对于多模态以及文生图上，我们开放了一些相关的，一些评测工具，大家有兴趣的话，可以到我们FlagEvo的平台，或者是工具。

Github上面去看一下，顺便给我们点几颗心，那从刚刚我们提到了，说大语言模型的，它的整体的一个评测的，能力来说的话呢，我们其实是考虑到，在传统上，能力任务指标，其实是混在一起的，没有进行过好的拆解。

所以的话，我们把模型的能力任务指标，做成了一个三维的评价体系，所谓的能力的话，其实就是刻画这个语言模型，它的整体的一个能力的边界，它能为我们做什么，它能达到一个什么样的程度，那我们怎么来探测。

它的能力在哪呢，那就要用到我们的任务，我们用不同的任务，去探测不同的能力，或者是说在一个能力上，用不同的任务去探测，都是可以的，那我们这样的话，就把能力和任务，进行了一个拆分，具体的话。

我们如何量化地去刻画，它的大模型的能力，所以的话呢，我们又提出来了四大指标，所以这样下来的话，我们一共可以做600多维度的，全面的一个语言模型的评测，那到目前为止的话呢，我们现在是整个是包含了。

22个主客观的这样的一个评测数据，及8万多道的这个题目，当然这些的话可能还并不足够，我们还需要去做进一步的探索，而且我们在做能力评测的，这个框架的时候，我们还特意一定把安全，以及相关的价值观的评测。

作为一个非常重要的部分，放在了里边，但是呢这一部分的研究，还是属于一个起步的阶段，我们还需要在这上面，有很长的路要走，具体的话我们是说，要去怎么评测，肯定还是离不开数据，离不开任务。

那我们一期发布出来的这些，一共包含了五项任务，中英文分开算的话，有中英文的选择问答，中英文的文本分类，还有一个中文的开放问答，那这些数据的话，其实来源于，主要来源于两个方面，一个是现有的一些。

开源的一些数据集，一方面是我们志愿，与像北京大学等等，这些共建单位一起合作的，共建的数据集，那这些数据集呢，其实就是为了要弥补，一些我们能力上面，评测的一些空白，那对于一些开源的，那个数据集的话呢。

我们是做了一些，做了适应性的调整之后，把它做了统一化的处理之后，才放到我们平台上面，去进行的评测，然后呢，现在的整体的，所有的数据集，几乎可以包，可以覆盖我们刚刚建立的，那个能力体系的评测。

对于评测的方法而言的话呢，我们其实采用了两，采用了两种方式，一种是客观评测，一种是主观评测，那这两种方式的话呢，它各自有各自的一些目标，以及它的特点，那从客观上来说，客观评测的话，也就是说。

它的这个任务主要是，以客观的这种任务，也就是说有标准答案的，这个任务为主，那它的评价方式的话呢，我们是以一个，in context的一种方式，那我们可能给它一些，通过prompt。

给它一些few shot，或者是zero shot的一些提示，让它去完成相应的任务，那在这个评测过程中的话，我们需要注意的，其实是为了保证公平性，我们对于数据，对于算力，都对于各个模型的话。

都需要进行统一的处理，那它的整体的一个特点是说，它的评测的数据量很大，快速 非常快，那它适合的话是，方便对我们的模型，进行快速的一个验证，但是呢 它的缺点在于是说，它能评测的这个能力，其实是有限的。

它能评测的维度，是比较有限的，那我们的一个资源来自于是说，有丰富的一些，现有的一些benchmark，以及一些自动化的一些工具，可以支撑我们去做快速的客观评测，那仅有客观评测还不够。

我们需要做一些主观性的一些评测，特别是对于这种对话的模型，那所谓的主观评测的话，指的就是没有客观标准，或者是说就是没有标准答案的，这样的一些试题，那这些试题的一个评价的话，还是要以人为主。

以这种形式的话，就是以这种对话，Zero-Short的这种形式，去进行的一个评价，那我们的评价的话，是严格遵循着这种背靠背的，整体的一个标注去进行的，而且我们的标注人员，一定是会经过了多轮的评测的。

这种标准的一个培训，大家都对于标准比较的熟悉，且对齐，但这种主观评测的一个特点是说，它的能够评测的数据量，其实是有限的，然后速度是比较慢，但是它的优点就是说，它评测的能力的维度会非常丰富。

而且我们可以及时地发现，模型的一些特点，那目前的话呢，我们智原的话，已经建立了一个比较，非常有经验的这样的一个标注团队，来支撑我们做人工的评测，那对于回到模型而言的话，我们对于模型进行了，拆解成了两种。

一种是基础模型，我们认为是一个基座的模型，那对于这种基座的模型的话，不太适合用主观的评测方法，去进行评测，所以我们使用的是客观的，一个评测的方式去评测，这个里边的话还特意加了一些，提示学习的这种评测。

也是要看这种大的模型，它在什么样的程度的时候，是可以涌现出，in-context learning的这种能力的，那对于微调的，也就是大家，例如说现在的一些对话的这种模型，这种的模型的话呢。

我们是除了用客观的，跟基础模型一样的评测方法，进行评价以外，那我们又加了一些主观的评测，主观的评测的话，也分为大概两种，一种是说全人工评测，全人工评测就是多人评测，加仲裁得到最终的一个结果。

或者是人机评测，因为人工评测的这种，实践周期，以及它的工作量确实是很大的，所以我们尝试着使用了GPT-4，来辅助我们进行一个评测，但是GPT-4的结果，也一定会和人进行一个对齐，那在我们智原的话。

本身是有Aquila的模型的，(读者问问题)，智原本身的话，是有大模型的一个训练的，那在这个训练的过程中的话，我们的评测的话，是有相关的自动化的评测机制，也有一个自适用的评测机制，那在这个整体的一个。

自动化评测的机制的过程中的话，我们的主客观的话，是完成了整体的一个自动化的，流水线的这样的一个评测，那我们可以做到一个，全自动的一个衔接，那对于各项的一些效率，就这种评测的效率，也是有逐步的一些提升。

我们使用了很多的，例如说推理服务的并行化，还有评测的各个阶段，以及评测数据的，不同的一种并行化的处理，提高了评测的效率，另外的话我们也做了一些，自动化的流水线的一些评测，那根据模型的这个类型。

还有它的这种状态，选择不同的评测的一个策略，最终是形成了一个，整合成一个最终的评测的结果，那我们在评测的开始，以及结束，以及它的整个生命周期，我们如果发现会有一些错误的时候，那我们都会把。

写成一个自动化的通告，那相关的这些，报警的这些机制的话，也会同步给相关的一些同事，那这样的话，还极大的提升了我们在训练过程中，的这样的一个衔接的效率，那也就解决了我们刚刚说，大模型就是开头。

这个大船难掉头的，这样的一个问题，最后的话呢，我们现在FlagEvo的话，目前支持多芯片多框架的一个评测，也就是说上来的这个模型，我们不局限于是说，一定要是在英伟达的，芯片上训练的。

也不局限于它一定是要PyTorch和TensorFlow，我们其实是要积极地去，推动国内的芯片，以及框架的相关的一些发展，那目前的话呢，从AI的这个框架上来的话，我们目前是支持。

PyTorch和MathBot这两种，后边的话我们会继续地去扩充，然后芯片的话呢，从平台上面现在已经集成了，像韩5G、昆仑芯，还有声腾，就是那个鹏程云脑的声腾的集群，相关的给我们提供了相关的算力。

支持我们进行相关的，大规模的这种评测，那最后的话呢，是说我们现在在对开源的一些，小规模的语言上的，语言模型上进行了一个评测，那目前的话呢，我们的Aquila Chat是目前，支持中英双语，并且性能最优。

并且它的开源是商用协议许可的，一个对话的模型，那我们其实是，大家可以看一下，就是说我们，我们整体的一个，数字的第一列，就是min的主观加客观的那一列，我们是已经达到了，所有模型当中是最好的。

特别是在中文的客观和主观的领域，都是所有的模型当中是最好的，特别需要提到的一点是，我们的base模型，只用了不到600B的token，然后我们的SFT的数据，只用到了14万，和同类同级别。

同量级的这个模型来比的话，我们使用到的数据是非常少的，所以是说我们用了，相对少量的这种，优质的数据，就可以训练出一个，比其他的那些，多数据训练出来的那些，模型还要好的一些效果。

不过当然的话我们在英语上的话，确实还是有一定的差距，这个也跟我们使用的数据，数据的这个构成是有关的，我们在我们的整个的500多B，不到600B的token当中的话，英文占了不到一半。

所以是说能达到这样的一个水平，也是很了不起的，目前的话呢，天鹰和QR模型的话，现在已经在，已经在Flag AI，就刚刚刘广博士，介绍的那个项目里面，已经集成进去了，欢迎大家去使用去下载。

然后我们的开源协议都是最宽松的，那最后的话呢，就是说我们大模型的评测，刚刚我们也提到了，有面临了很多的问题，现在虽然有一些初步的探索，和初步的成果，但是呢其实还有很长的路要走，那首先是说我们会要。

后边的话是说要集成，和自研很多的维度的，评测的数据，以及评测的工具，那我们要覆盖更多的，例如说多模态视觉啊，语音啊领域的这种，大模型的这种评测的能力，我们也需要去进一步去提升，另外的话呢是说。

对于语言模型而言的话呢，其实我们现在的，评测其实仅仅是，触碰到了语言模型的，很表层的一个部分，并没有有很多本质的问题，并没有完全的能够去解答，例如什么它是不是具备智能，它是不是有心智。

它是否有真的是有认知能力，以及它的认知能力到底是什么，其实这些问题，这些本质性的问题还没有，真正的去刻画和完成，所以我们希望在接下来的一个工作中，能够与很多，例如说社会学，例如伦理学。

例如心理学这样的社会学科，能跟我们一起去探讨，怎么样的话能够把大模型的，这个评测做得更加的全面，更加的科学，然后呢这里边，是我们上线的一个，上线的平台欢迎大家注册，那边是我们FlagEvo的一个。

开源的工具也欢迎大家去，下载去给我们提相关的，issue和PR，期待跟大家有进一步的合作，谢谢，以上就是我的，报告谢谢大家，谢谢杨夕老师的，这个精彩分享，接下来有请Hugging Face。

集体学习工程师，黄敬亚，为我们分享主题演讲，AI快速增长年，来自Hugging Face开源社区的新进展，掌声有请，谢谢杨老师的介绍，今天也非常高兴能够参加，志愿大会的AI开源论坛，然后有机会。

为Hugging Face开源社区，在去年一年做出的一些，进展做一个非常，简单且快速的汇报，那首先，介绍一下我自己，我叫黄敬亚，我是Hugging Face的一名Machine Learning。

机器学习工程师，然后我在Hugging Face的，主要工作是一些，针对硬件方面的，对开源模型训练，和推理部署的优化的工作，然后同时我也是Hugging Face，开源模型。

提速库optimum的维护者，首先什么是Hugging Face，我相信在座的大家，可能或多或少已经了解，Hugging Face或者，看过我们的平台我们的工具，或者是看到过我们对，机器学习知识传播。

做出的一些分享，那对于还不太了解，Hugging Face的朋友，我这里简单的做一个，关于Hugging Face的介绍，那对于Hugging Face，我们的理念非常简单，我们的目标就是我们想要。

民主化好的机器学习，那这就意味着四点，首先第一点就是，我们的机器学习，是非常透明的，第二点就是我们非常注重，去开发高质量，且非常易用的工具，来帮助大家更快的去推动，机器学习的工作。

同时降低机器学习的门槛，那第三点就是，我们知道今天的，这个机器学习的，是一个非常快速，发展的一个，一个Domain，所以我们希望，通过合作的方式，和不同的Organization，以及社区的贡献者。

能够快速的迭代我们的工具，使它永远保持在前沿，然后最后的最后，也是最重要的一点，就是人工智能的，道德标准永远是指导我们，整个开源开发过程中的，一个很重要的信念，那这就是Hugging Face。

的开源生态，那接下来就进入，今天汇报的正题，那就是在过去的一年当中，Hugging Face做了些什么，这是一个非常简化版本的，Hugging Face的开源生态，在去年的今天，与今天的一个对比。

我们看到我们在去年的一年当中，我们有了新的合作，我们有了新的合作，我们有了新的合作，我们是一个非常，开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具。

我们是一个非常开源的工具，我们是一个非常开源的工具，我们是一个非常开源的工具，接下来我们有请，接下来我们有请，AnyScale ML Tech Lead 龚俊，来给我们带来。

Open Array and Open Source Framework for Distributed ML，原本这个环节分享的，原本这个环节分享的，Ray Data的团体经理苏成，因为因病来不了。

希望他能够早日康复，希望他能够早日康复，(音乐)，早上我在AI System，早上我在AI System，已经介绍过Ray，但是那个talk可能更注重的是在，整个机器学习，整个机器学习，主要介绍一下。

我们在Open Source最近的一些工作，我们在Open Source最近的一些工作，我昨天晚上才知道我要cover这个事情，所以如果大家觉得，有一点准备不充分的话，敬请原谅，OK，开源项目。

一开始就讨论，GitHub Store的事情，我也不知道为什么，刚刚我看Hacking Face的图，也是拿Spark和Kafka做对比，也是拿Spark和Kafka做对比，我们这个图也是用的。

Spark和Kafka，到上个月为止，到上个月为止，到23年5月份为止，我们的Star数量已经超越了Kafka，现在正在朝着Spark方向接近，现在正在朝着Spark方向接近。

可以说Community的Adoption还是不错，可以说Community的Adoption还是不错，然后也正好是借助大模型的风潮，然后也正好是借助大模型的风潮，AndySkill一直的bet。

AndySkill一直的bet，这个Model，这个机器学习的模型的增长速度，应该是远大于物理硬件的增长速度，所以我们一直觉得，所以我们一直觉得，你要从单机走向多机，你要从单机走向多机。

从单机的训练走向分布式的训练，是一个不可避免的事情，是一个不可避免的事情，这个也是整个Company的Thesis，这个也是整个Company的Thesis，最近由于大模型的出现。

这个速度好像愈发加快了，所以我们看到，它的Adoption的增长还是不错，它的Adoption的增长还是不错，然后也很幸运的，因为他们的创始团队，还算是比较有分量。

所以我们有很多Open Source的，User和Partner，和我们一起共同建设这个Library，和我们一起共同建设这个Library，其中蚂蚁一直都是，从很早的时候，就是一个非常大的支持者。

也给了很多的工作和帮助，也给了很多的工作和帮助，可能待会会，come back to these，come back to these，然后，还是一样的介绍一下，Ray的产生发展的历程。

Ray的产生发展的历程，Ray其实一开始的时候，是在Berkeley的，一个叫Rise Lab的，一个Computer Labs，然后我们的两个Founder，当年都是做家乡学习的博士。

然后他们做家乡学习的时候，就会发现，就是说你做家乡学习的时候，相对简单，算法也并没有那么复杂，然后它的scale也比较小，然后他们就在不停地写，分布式计算的code，老是在那边跑那些物理的。

模拟啊什么之类的，老是用JRPC去，从远端的机器上，从远端的机器上，去把training data给它fetch过来，然后做training，然后他们就说，要不就去写一个framework。

这个就是整个，Rise这个library的开始，也是为什么，就是说这个，作为一个分布式的，框架，他们第一个做的这个library，居然是一个家乡学习的，library 这个是很奇怪的一件事情。

但是historically，there is a reason behind it，然后家乡学习这件事情，其实如果你做过的话就会知道，它这个是出了名的不稳定，你如果做家乡学习的话。

这个hyperparameter tuning，就是第一件要做的事情，所以他们写完了RLIB之后就，写了一个hyperparameter tuning，的这个Ratio library，然后。

做完这两个library之后他们就，很快就意识到，他们写的其实是一个很通用的，分布式计算的一个框架，可以使用在家乡学习，以外的很多地方，所以那个时候就会，NACGO就诞生了，然后NACGO就成为了。

这个Ray背后的商业化的，公司，然后在此之后就增加了，deep learning，也补全了，那个，influence和，data ingestion的那些部分，如果你现在，今天去看的话。

其实Ray这个ecosystem已经是，涵盖了端到端所有，你，机器学习需要用到的，最重要的那些部分，包括数据的预处理，输入，然后，深度学习，模型的这个，部署，超参的这个，有些单词不是很熟，超参的那个。

调整和那个，一些workflow的那个，logic，这个就是，我们想要做到的事情，就是你如果看，这个Machine Learning open source的，这个ecosystem的话。

其实现在是处于一个百家争鸣的状态，也并没有说哪一家独大，或者整个行业，consolidate到哪个地方，所以，我们就想就是说用一个完整的，ecosystem使得他们，库和库之间的交流会，结合会更好一些。

user用起来的话会更流畅一些，然后简单介绍一下这个，Ray设计的思想，这个其实也是我当年，为什么，选择加入这个公司，我觉得他们的设计理念还挺，叫什么 elegant，他们就是把那个。

Python的一些概念，把它映射到分布式计算里面，比如说那个Python的function，其实你可以把它想象成一个，一块计算，然后你如果把这个function decorate一下之后。

你这个计算就可以被，移到其他远端的机器上去执行，然后想象，这个micro service，或者说你一个deployed service，其实就是一个actor，就是这个它把map到。

Python的class上，就是一个class instance，Ray就会自动把你deploy成一个，一个service，然后有了这两个piece之后。

他们就在build了一个shared memory，space，叫object store，这样的话就是handle，分布计算中间的，通信的问题，然后它还可以顺便，去优化这些数据的流向。

帮你节省不必要的，优化和那个，theorization和de-theorization，这个其实是整个这个Ray的thesis，就是这么一点，它的API其实还是蛮，简单的。

比如像现在这里的两个function，然后你有一个Python class，然后，all you need to do，你只需要加这么几个，decorator，Ray到Raymode。

然后突然之间它们就变成了一个，一个service和一个，一个computation，然后，instead of，本来你要写很多的那些boilerplate code，对吧，然后现在你都直接，他们就帮你。

藏在Python的source code，背后，然后你只要做的事情，其实就是instead of calling，这个function，直接去call这个function。

然后这个时候你其实是在一个远端机器上执行的这段code，然后你拿回来其实也是一个future，就是说你不会直接把这个数据拿回来。

in case 你下一个computation 还是在远端的那个机器上的话，那这样你中间就不会incur 那种多余的那个通信，that's the whole idea 其实。

然后我觉得这个还是蛮elegant的，也比较generic 然后它handle了一些，我们一直说这个ray的abstraction，感觉是在一个right level上。

它是generic enough 但是也high enough of the pain points，然后for use case wise，你如果看那个open source的使用的情况来看的话。

最常见的一种做法是体量相对较大的公司，会拿我们的开源的部分，在他们的公司内部就搭一个平台，然后这个平台的作用就是使得他们公司里面的那个，数据工程师或者是咱们的机器学习工程师。

他们只需要去考虑这个python side of things，他们就是只写python code 也只跑python code，然后相同的python code可以在本地机器上调试。

也可以直接就submit给ray cluster，然后就可以被deploy成一个很大的computation，然后这样的用法其实包括Spotify，Cruise。

 Shopify 他们其实都发过那个blog post，如果有兴趣的话可以读一下，然后更加相对重磅一点的其实就是open AI，因为他们其实开发GPT-4的时候，其实是在ray的cluster上开发的。

feedback是说开发的效率会高很多，然后他们的scale也比较吓人，我们经常给他们hack，然后如果你看benefit wise，就是说这一张图是instacart share给我们的。

他们在没有用ray之前，他们的kubernetes的cluster utilization一直都不高，然后因为有了ray比较灵活自由的调度之后，cluster utilization最高的时候。

就真正在被人使用的时候是可以到百分之八九十这样，另外一件比较有意思的开源活动，最近发生的是我们的core team上有一个Berkeley的博士生，然后他的博士thesis其实跟ray也很相关。

然后他在ray上做了一个benchmark，他就是terabytes sort，这个好像是他们这种大规模，病情计算的人一个比较重要的benchmark。

就是看你可以用多少钱去sort100个terabytes的data，然后用ray来做的话，我们是第一个把这个cost做到一块钱以下，每tb的框架，而且它的code其实是很简单。

就是说因为ray天生就比较适合做这种，控制和运行无数的小的计算的这件事情，所以我觉得可以拿来在这里提一下，然后另外一件比较有意思的事情是，我们最近做了很多的对ray dataset做了很多的优化。

data set本来的用处是给机器学习做输入，就是data loading和data ingestion，然后我们在调优的时候发现，它跟市场上其他的solution比还是有一定的优势。

因为首先它是python native，然后它build on ray的话，对这种比较大量细缩的工作，它就有一定的天生的优势，然后也是一样的我们做了一些benchmark。

和比如说SageMaker的batch inference比，它就快了大概有十七八倍，然后哪怕和Spark的，调优过的Spark cluster比，我们都快了大概三倍左右。

究其后面的原因其实这个也是一篇发布的blog post，你们也可以去看一下，究其原因里面主要的原因就是，因为你这个machine learning的时候。

这个model都stay在python land，然后Spark其实是在Java land，所以Spark在做这些事情的时候，它的数据在python和Java之间不停地在那边。

serialization和deserialization来来去去，外加Spark对不同stage的fusing，做的有一点不好，就是它会很aggressive的把所有的stage都fuse在一起。

这样的话如果你有一个GPU的stage，它是bottleneck在那里的话，你的CPU就不能fully utilize，但是Ray在execution plan上就稍微自由一点。

对然后另外一个事情就是我们最近，上个月吧，上个版本就把Ray dataset的execution mode，变成了streaming mode by default。

这样的话我们经常发现你在帮一些大的用户来调的时候，他们的data ingestion经常就会误，因为这种大型的deep learning。

data size基本上你很难凑到这么多量的CPU memory去hold，所以我们自从把它改成streaming之后，就彻底解决了这个问题。

Ray dataset会自动的去apply backpressure，然后make sure你的CPU和GPU的utilization是proper的情况下，它就尽量的帮你去prefetch足够的数据。

最后讲一讲generative AI的事情，generative AI其实Ray参与的也比较早了，大概一两年前就有很多的合作，像Cohere和OpenAI的talk都是发生在去年的Ray上面。

然后more recently，就是最近的话，在刚刚结束的GTC，就是NVIDIA的conference上我们也有一个talk，就是Alpha的performance做得其实也很好。

然后Alpha整个的orchestration，就是它这个大模型的执行，它是build在Ray上的，然后基于这些，我们最近在2。4版本里面也发布了很多的off the shelf的事例。

告诉大家怎么在Ray上面跑那些大模型的事情，然后我们take了一个稍微有一点不一样的角度，就是说我们在做这些事情的时候，发现其实现在高端的GPU卡是很难找得到的，所以我们就利用Ray的特性。

所有的事例都是让这些大模型跑在多个小的node，用小的GPU来train一个很大的模型，然后有的时候这样做的反而在成本上也有优势，大家如果有兴趣的话不妨去看一看。

然后for serving我们其实也做了一些工作，来增强它的scalability和streaming support，for大模型的use case，然后就在上个礼拜反正我已经在中国了。

然后他们在美国的团队，他们发布了一个叫Aviary的东西，其实有一点像是炫技的感觉，我们claim说我们有Ray serve这个library，是可以serve大模型的，然后他们就发了一个算是样板。

就告诉你我们不但可以serve一个，我们还可以serve很多个大模型，你可以一起跑，然后那些东西都是会自动的scale up，scale down的，根据你的traffic。

然后如果你有一个Ray cluster在跑的话，你可以试一下这个，基本上就可以拿来serveopen source model了，然后加了一个role map的slides。

其实这个是我今天早上自己加的，我也不知道准确不准确，从Ray的角度来说，我们这个ecosystem因为比较广，它cover了很多不同的事情，所有的这些东西都在不同的stage里边，core的话。

他们的最主要的问题还是增强稳定性，还有就是大模型的时代，你必须要把scaleability做强，然后data set因为相对矫新，他们里面有很多performance optimization可以做。

也会是接下来的重点，然后rllib的话有一点奇怪，因为rllib一直都是以包罗万象，各种各样的algorithm拿起来随手就用，然后我们最近发现其实对研究者的支持不是很好。

就是说因为research的同志们，他们使用rllib的方式一般都不是你现在写在里面的，他们都是脑洞大开，改的地方都基本上都不是特别的off the shelf。

所以我们的方向是要把rllib搞得moderalize一点，这样的话你可以把它pick出来，然后去build你自己的algorithm，serve基本上就是为大模型服务了。

因为inference是一个很重要的大模型的use case，train和tune的话，其实我们的goal是在今年的3月就9月份之前，想要把APIstabilize下来。

因为有些地方感觉还是不是特别舒服，然后最后也是要做个广告，因为Ray是完整开源的项目，其实公司也很小，都是靠大家帮衬，请大家contribute，就是这样，感谢龚军的精彩分享。

接下来我们有请张军百度飞奖框架产品负责人，开放原子基金会TOC委员来跟我们分享，产业及深度学习开源开放平台飞奖及其开源社区，有请，很荣幸参加志愿大会，参加AI开源的论坛，到现在这个时间下午三点半。

我看大家可能有点困了，其实我跟大家一样也多少有点困了，今天听了很多关于AI的东西，尤其是很多大模型相关的东西，我的PPP里面也一样，既有AI的部分也有开源的部分，但是考虑到大家现在比较困了。

所以我准备多讲点开源的地方，少讲点AI的东西，因为开源就像刚才HoneyFace的小姐姐分享的一样，开源可以让大家开放，这是开源最重要的部分，今天所有的会听下来感觉，大模型的东西讲的特别多。

这一切的开始其实就是从，OpenAI，当然大家现在可能很多人吐槽它，因为它不够Open了，因为OpenAI开始，把很多东西它都闭源了，不开始开源出来了，从他们的Chad GPT开始。

他们出了Chad GPT之后，Google做了一个Bard，也是一个大模型，当然也没有开源，国内大模型也是非常火，国内最早，百度做的是一个大模型的产品，因为现在应该有不少人已经拿了它那次账号。

已经可以开始测试了，叫文心一言，可能有不少的人听说过，我今天要分享的部分，其实是训练出来文心一言的，我们的一个开源的深度学习平台叫飞讲，当然了，文心一言出来之后。

其实国内很多的公司也在做这些大模型的产品和服务，阿力有一个叫通译千问，有点像是那种，你出上联我出下联的感觉，你出了一个文心一言，我的下联就是通译千问，听起来还挺对账的，是不是。

其实在深度学习框架和平台这方面，也一样的是，国内一直是做了很多的事情，可能很多人在知道说是在训练大模型的时候，用的是一个叫PyTorch，很火的一个深度学习框架。

包括像Hugging Face的生态里面，有非常大量的模型是用了PyTorch训练出来的，国内其实也一样，也有很多现在开源出来深度学习框架，刚才华为的小姐姐分享了，叫华为的MathsBoard。

其实MathsBoard也有个上来，是Petapad，Petapad是国内最早我们开源出来的深度学习的框架，首先先跟大家简单汇报一下，其实百度一直在AI的开源，甚至在开源领域我们做了很多的事情。

从最早我们去contribute的一些很有名的开源项目，像大树玉领域的Hadoop，到后来我们开源了深度学习平台Petapad，同时我们也是很多很知名的这些开源基金会的成员，在整个AI开源上面。

其实也是有一系列的开源动作的，最下面其实是今天我要给大家介绍的是，开源的深度学习平台Petapad，它有一个中文名字叫飞角，在这事实上其实我们有很多基于飞角做的，全面通用这些AI的能力，包括视觉。

自然元处理，还有语音技术这些，其实这当中我们很早很早就开源了很多的这些模型，我们最早的算法库里面大概有600多个算法，都已经开源了出来，当然因为那时候还没有像现在大模型。

大家一说好像你搞模型算法都必须得是非常大的模型，因为在这里顺便稍微提一下，因为大家可能知道如果训大模型的话，其实是需要有很多很多的东西的，首先算力，一个大的模型是需要有一个大概上千张卡的这样一个集群。

你才能训得了的，那要有数据，这些数据可能一些互联网上的这些数据，可能还是比较容易能拿到的，尤其用来训Foundation Model，但是SFT阶段，包括后来的RL。

HF那部分也是需要非常多标注的这些数据，同样还需要很多很专业的人才算法，还要有很重要的技术设施，深度学习的这些框架和平台，所以需要这么大投入的情况下，其实各家如果把大模型开源出来，是有很大的顾虑的。

包括国外大家可以看到的，开源出来的一个模型，现在公认的比较好用的就是Facebook Lama模型，它是一个65B的模型，最近可能有些人知道中东有个土豪，有一个土豪的机构放出来一个大模型。

据说它只有40B，但是据说比Lama还要好用，今天听说是智原也放出来一个中文领域很牛的一个大模型，我是感觉智原在做一件非常了不起的事情，回过来我们看看，如果训练大模型的一个深度学习框架和平台。

Pedal-Pedal到底是什么，其实Pedal-Pedal首先是一个开源的项目，我们现在在Pedal-Pedal的GitHub组织上，有90多个开源代码仓库。

也有很多通过CodeCountryBuilder，或者跟我们讨论的一些贡献者，有两个数据，第一个是国内的有一个项目活跃度是我们在保守，第二个其实我们也是去年才注意到。

Linux基金会下面的CNC App它会做一个统计，这上面它有一个榜单，是Pedal-Pedal也很荣幸上了这个榜单，虽然我们排名是比较靠后，但是也是很荣幸在那个榜单上，能跟世界上很多很知名的开源项目。

像Python编程语言，或Linux kernel这些开源项目一起在这个榜单上，同时大家知道，在深度学习框架和平台上，其实开源的东西也很多，有的人可能会问说是。

那你这个FidgetPedal-Pedal，到底跟PyTorch有什么差异呢，其实我们一直以来有一个很重要的一个价值主张，其实FidgetPedal-Pedal是一个，源于产业实践的深度学习平台。

所以不光有开源出来深度学习框架，我们也有很多的产业级的这些模型库，为了让很多的用户，他真正把AI落地到他的应用场景，所以我们也有一系列的开发套件和工具的组件，这里有一个大概的一个全景图。

那跟刚才其实介绍的也比较类似，从下面的核心框架到上面的工具组件，其实都是开源出来的，当然最上面的两个不是开源的，因为那是要商业要卖钱的，但是下面这一大头的东西都是开源出来的，发展的历程其实是比较早的。

因为最早其实做FidgetPedal-Pedal，就是在百度公司内部，为了能够让很多的工程师，能够很好的使用一个共性的一个平台和一个算法，因为最早如果没有这些深度学习框架，或平台这样的工具的话。

那你如果做一个深度学习算法，要从CUDA kernel开始写起，那你有可能说是我的前线计算写对了，但是我这个backpropagation就算错了，会很头疼，所以才有了这些工具。

百度也是在当时内部建设了，很早是12年就开始建设这样的平台，16年的时候开源出来，到现在其实还是一个比较成熟的深度学习框架，百度内部几乎所有的业务都在使用FidgetPedal-Pedal。

所以这里面也是一个百度的文心语言，跟其他一些大模型不一样的地方，百度的文心语言这个大模型是用Pedal-Pedal训练出来，是我们自己的开源的一个深度学习平台，那有哪些就是我们自己觉得比较有优势的地方。

第一个其实Pedal-Pedal还是一个开发便捷的深度学习框架，如果你用Pedal-Pedal去写一个模型的话，你有可能你会觉得代码长得怎么跟Pedal是非常像的，甚至有的人开玩笑说是。

我是不是import Torch as Pedal，然后你就可以直接把那代码跑起来呢，可能对一些简单的模型是可以这么做到的，但是稍微复杂一些可能还是做不太到，因为框架有它自己本身下面那些技术设计的差异。

会导致你前端的几个接口的交互可能也会有不一样的地方，第二个其实是在我们超大规模深度学习模型训练技术，我们其实是一直以来比较自豪的，因为其实在今天这个时代有大模型这些技术出来之前呢。

那已经在很多地方已经在用大模型解决很多的问题了，只不过那些模型呢可能跟现在大家今天谈到的那些大模型可能还有些区别，比如说是在百度自己的搜索这些业务上，其实最早就用的是大模型，但是它的模型跟现在的。

大家看到的TreeGBD这类的可能还有比较大的区别，比如说是我如果用百度搜索上的我的模型的context不用那么长，因为很少有用户说输入一个很长很长的query。

你在百度里面搜词的时候可能你输十几个字也就差不多了，但现在的TreeGBD这类模型它的context甚至现在可以达到32k这样的长度，那同样的在搜索领域我们的那个词表可能会非常非常大。

大概一两百万的词表也是有可能的，但是现在的大的语言模型可能不需要那样大的，但是它们背后的技术有很多是相似的，那飞甲是通过在百度自己的业务实践上，积累了很多的超大规模深度学习的模型训练的技术。

第三个其实是在很多场景下它是可以推理的，因为一个模型它不光是在云上的服务器当中可以进行推理，同样在边缘设备和手机上也能够推理，那最后一个其实是我们比较大的差异，其实是我们开源了很多的模型库。

这些模型的size可能不像现在动辄说一个模型，你的那个模型大小，你如果没有一个百亿的模型放出来，感觉都不像是大模型，不像在开源模型似的，当然还有很多的模型其实像视觉领域的很多模型。

它其实可能目前真正在产业当中落地使用的时候，它那些传统的那些YOLO的那些算法，Detection的那些算法，或者SEG的那些算法，那些模型虽然没有那么大，但是在产业当中使用的时候。

还是需要有这样一个开源的模型库，它直接拿来可以用的，那到底谁在使用飞讲呢，首先百度公司内有基于飞讲的计算集群，每个月大概20万个训练的任务，那还有很多推理的服务，飞讲所有的搜索推荐这些业务都在使用飞讲。

这个PPT里面可能忘了加上文心语言也在用飞讲，那同时公司外因为我们一直以来的价值主张，是做产业级的深度学习平台，所以很多的这些产业当中，它可以用深度学习的模型去解决它整个产业当中的实际问题的时候。

他们也会选择用飞讲，这里面有一些具体的例子，比如说是中国商飞，我们刚很自豪的自己的大飞机，飞行的那家公司，那他们使用飞讲去做这些投降损伤的检测，那比如说是工商银行用飞讲去做卡证的这个电子的这个识别。

接下来的部分呢，是说作为一个开源项目，到底哪些人为飞讲PetalPetal做了contribution，尤其是代码上的contribution，那可能有些人可能会觉得说。

那你PetalPetal是不是就是你们百度的这些工程师在做呢，这当然是很重要的一方面，因为从2016年开源到现在这么长的时间，百度其实一直在这上面为做这个开源的项目，有非常非常大的投入。

当然这当中最主要的其实是研发的投入，我们有专门的研发部门去做飞讲飞讲这个项目，当然，另外一个很重要的对PetalPetal做contribution的部分呢。

是corporate contributor，那尤其是飞讲为了能够运行在很多这些硬件上，这些来自硬件公司的很优秀的工程师在contribute to PetalPetal，那就包括像英伟达。

英伟达在我们祖国的宝岛台湾，有专门的工程师在做PetalPetal，那Intel呢，Intel呢，当然中间发生过一些变化，那最早是上海的团队，后来是波兰的一个团队，就下面中间这部分。

很可爱的工程师在做PetalPetal，当然最近为了更加local的合作呢，其实是把这部分工作是转到了Intel上海的团队，那还有像很多的硬件的公司，像百度自己的昆仑星，还有韩武技。

还有等等很多的硬件的公司的工程师，在为PetalPetal做contribution，另外很多很大一部分，其实过去这一两年我们看到，为PetalPetal做contribution。

很重要的一部分来自于，高校的学生和企业的工程师，这些individual contributor，他们占的比例也会非常非常大，尤其最近我们观察到他们的比例，一直在持续地增长。

那当然也会有些社区的一些技术交流的活动，我们一般把它叫做线上的meetup，开会的有一个截图，那做这样一个PetalPetal这样一个开源项目呢，那有些最近有这么多社区的个人开发者的参与。

其实也是看到国内的这些开源的环境越来越好，所以有很多的这些工程师，他们可以贡献到开源里面来，这是一个很重要的部分，因为大家不太可能说是我们所有的东西，都去关注到那些大模型领域上。

大模型所需要的这些基础设施 基础软件，其实是现在我们非常需要的，那如果能给PetalPetal做贡献呢，对很多的社区个人开发者他们参与的动力，我们观察到的是首先是技术上的提升，第二是来自于社区的认可。

这是我们在做Fidget PetalPetal这样一个开源社区的时候，观察到很重要的部分，那也有很多的社区的个人开发者，跟着我们一起去成长，当然做PetalPetal这样一个深度学习的平台呢。

其实对于很多社区的个人开发者来说，有一个很大的挑战，他可能自己没有GPU的开发环境，我们也提供了一些线上的GPU开发环境，这是去年的一个社区开发者做的一些项目的选集，从比较简单的一些。

code-based maintenance的工作，到很难的一些，像深层网络编译器的算法开发，都是社区开发者在做，整个社区呢其实去年我们，做下来是有很多的社区活动，比如说是我们有飞角黑格苏。

这样一个用来做编程挑战的这些活动，那它的难度还是比较大的，然后还有稍微轻松一些，主要是欢迎做一些first good issue这类开源贡献的活动，我们把它叫做快乐开源活动。

当然也有些社区的meetup，那如果谈到开源呢，其实今天是AI开源的，尤其是在中国作为一个AI开源的社区，到底跟国外有哪些不一样呢，正好前两天我看了一本书，它是讲开源的，它里面提到了一个说是。

如果你作为一个开源社区，最应该避免的事情是什么，最不好的一个工具是什么，最不好的一个工具在上面是说是微信，可能很多人都会这么说，那因为老外呢就说我们所有东西尽量要open，尽量用在没有list上来讨论。

但是呢如果国内做开源，没有哪家说不用微信的，那这是我们跟国外很大不一样的地方，比如我们有自己的local的这些特色，那我们做下来呢是觉得做非讲开源社区呢，其实我自己总结了一个词啊。

叫国际化思考本地化的行动，国际化的思考是说，一个开源项目你很难说是，离开国际上这些优秀的开源项目，你能自己能够独立生存的，你不可能忽略到那么多的这些开源的项目，所以就拿pedal pedal来说呢。

首先我们要融入到整个开源的生态当中，所以pedal pedal的distribution的时候，我们要去兼容这些下面的硬件，比如说cuda不同的这个版本，那上面的python的不同的版本。

像python其实如果大家注意到python最近几个版本，他们一直在致力于python本身性能的提升，甚至最近python有一个计划是要去掉python的jll，那这些都是很重要的一个工作。

要适配不同的OS的发行版本，那pedal pedal也要能够跑在很多的AI accelerator上去，然后如果是做influence的话，那pedal的模型格式要能跟其他的互通。

跟onisk也好或者跟tvm也好要能有互通，那同样在模型上，其实我们也跟hugging face一起做了一些事情，pedal在hugging face上也有一些模型，那如果但是如果开源这个本身。

因为社区的参与者都是本地的话，有很多本地化比较自己有特色的事情，那所以在非常开源社区的运营上，其实我们也有自己很多本地化的一些不一样的有特色的地方，那比如说是城市高校文大们的一些领航团。

还有我们自己的活动，还有为了促进pedal我们的云云产业实践，做自动化升级的话有很多企业，去跟里面的工程师一起去做很多的，很多的AI解决方案上面的一些探讨，那同样的是跟一些硬件伙伴也是从最开始。

我们先聚到一起，然后先一起做一些适配上的优化上的事情，然后再看怎么样去拓展，跟硬件公司一起去拓展硬件上的这些生态，这当中就包括开发基于跟pedal，跟硬件公司一起做的一些课程，模型库还有案例这些事情。

我就不展开讲了，嗯，对，这一页是吹牛的啊，我就不讲了，那个，我们另外一个比较有意思的是那个，Air Studio的学习和实习社区，那这上面也是有专门的教学培训，还有开发者生态，还有企业的实训，嗯。

对这是在教育上的我们做的一些事情，因为如果让很多人用起来pedal，我们观察了一个很重要的现象，是可以先让学校里面同学们先学起来怎么用pedal，因为大家在学习Ai的阶段，如果就已经接触到pedal。

他大概来到后来还能会一直持续用我们的pedal，所以我们也相应的也会有一些，课程和比赛的一些资源，那也有一些书，也有一些案例，那，这是整个吧就是，我们从下面是深度学习的平台，然后有文心的大模型。

然后跟企业跟技术伙伴一起去做基于飞讲的这个，深度学习Ai的这个生态，那最后这页呢，这是公司要加的公司的使命，那，这页多少还是跟大模型有点关系的，因为这张图是用，我们的大模型文心一格画出来的。

那训练出来文心一格这个大模型的，平台是飞讲pedal，也是希望通过今天我的这个简短的介绍呢，能够让大家知道说，我们有这样一个深度学习的平台pedal，在国内我们也可以用来。

做很牛的这些像文心一格这样的大的模型，那，我的这部分就到这里，谢谢大家，谢谢，好感谢张军的这些精彩分享啊，之后下一个我们有请Zilis合伙人，及产品总监郭仁通。

带给我们向量数据库面向AiGC的海量记忆体，OK，OK，今天看前面的主题主要都是这个聚焦在大模型啊，然后这块呢换一个话题，希望也是给大家这个调节一下，今天主要想跟大家聊一聊这个。

大模型和向量数据库结合的这一块，那我们Zilis团队呢，其实在全球范围内是最早做向量数据库的一支团队，那在这个向量数据库的这个领域呢，也是深耕了有超过5年的时间了，那近期呢像AiGC啊。

ChaiGBT啊，还有像这个咱们国内很多国产大模型的话呢，都非常的火，那，也带火了另外一个这个数据库领域这个新的品类啊，叫做向量数据库，那今天呢就是想借着咱们大会这个机会呢，和大家聊一聊这个。

AI大模型和向量数据库这个结合的点位，简单做一下我，自我介绍啊我叫郭文通，然后呢是Zilis的合伙人，现在负责整个公司的产品，那在在此之前呢，我主要是负责技术和研发的工作，在前面呢就包括像这个。

有一些主要的工作啊，包括像向量数据库的这个分布式原生的架构设计，还有像这个Toy这个项目，主要是解决非计划数据一条的这个工具框架这样的一个问题，我自己的这个兴趣领域工作领域呢，主要是在AI和这个数据的。

呃基础软件这一这一部分，OK先给大家做一个background，就是说讲一讲非计划数据，然后向量以及向量数据库这么几个概念和它们之间的关系，那我们知道说最早的时候呢。

这个数据主要是以结构化的这种数值为主，然后呢面向这些数据呢，我们有一个数据库生态啊，主要是以这个结构化数据那一套生态为主，那呃在后来呢，2000年左右这个文本这个数据都上来了。

文本成为这个信息的主要载体，那对应呢我们的数据库生态呢，又呃有一套新的内容，包括比如说这个比较火的，像ES呀，MongoDB呀，这些都是在这个生态里面，那现在的话呢，我们的数据的主要载体呢。

是以非计划为主啊，包括我们常见的图像视频音频或者领域内的，比如说前面这个几位老师讲过的这个呃我的呃生物质要相关的呃化合物结构啊，蛋白质结构啊，还有像比如说自动驾驶领域的点云啊。

这些都是属于这个非结构化数据，那对应的话呢，我们要把这些非结构化数据应用起来产生价值，那也是需要有一套数据库系统生态的，那项量数据库呢，就是在这个大的背景下，就是以非结构化数据为这个主要的呃数据基础。

然后呢，是以AI的能力去作为这个数据的这个分析和应用的主要手段，这样一个大的背景下面的这个数据库生态的一个核心的位置，OK，呃，再讲讲技术层面啊，就是说大概这个过程是呃怎么样的呢。

我们的非结构化数据本身，它的这个主要的语义是蕴含在这个数据内部的，其实很难去以一个比较低的成本对这些数据直接的做理解分析和这个应用，那做法是我们首先通过这个典型的通过像呃神经网络模型的能力。

把这个蕴含在非结构化数据内部的这些语义呢，先去提出来，然后编码成一个高维的向量，那等于是说我们把这些非化数据都映射到一个高维的这个项链化的语义空间。

然后在这个右边的限量化语语空间里面去完成对这些数据的理解分析查询这样的一些任务，呃，OK，那这个是讲到了非结构化数据和项链数据库，那和这个AI大模型有什么关系呢，这里我呃列了一个一个一个问题啊。

这是我头一阵子去问的拆GPD，这是4。0啊，呃，问了一个问题就是说拆GPD4，你你来给我讲讲你自己的这个局限性是什么啊，这第一条就是他讲得很清楚，就是说我的能力呢，边界完全呃取决于我的训练数据。

我见过的东西我能回答好，见就是没没见到的呢，这些呢，基本上就很难回答，而且很有可能这部分内容是我生成了，但是这部分内容说实在的就是一本正经活出八道，但是这个事呢，我自己又不知道啊，这是他最大一个问题。

那对应下来呢，就是说因为咱们呃国内很多包括呃这个北美地区呢，大家去做数据应用，那这个数据应用的话呢，就有两点非常大的挑战了，一个是我本身我的业务内呢，是有很多的这个垂遇或者私欲的内容的啊。

这些数据显然是不会出现在这个大模型的训练数据里面，还有一个呢，就是说呃我的实时的数据信息，因为大模型本身它去训练的时候，不管是频率还是成本都是比较高的。

很难说把这些实时信息都会去整合到它大模型的这个呃参数里面，本质上来说，就是说呃我们去把所有的这个训练数据呢，然后这些呃自然语言的它的信息或者叫做自然语言，它的这个分布呢，编码到这个大模型大模型里面去。

但是业务上面来说呢，这些数据呢又很难出现在数据集去造成的这样一个问题，ok 那呃像openai的话呢，他们在很早就意识到这个问题了，就呃一个直接解决方法，就是说大模型搞不了的话，我去做这个外挂啊。

那在今年上半年的时候呢，openai做了一个非常大的动作，就是结合它的GPC这些模型周边去做一个插件系统，那其中有一部分呢，就是这个限量数据库插件，也就是说我通过限量数据库。

然后把所有的这些语意知识性的内容呢去限量化之后呢，呃给这个通过插件给我的这个大模型呢，提供一个知识库的能力，这样呢就解决刚才说，你垂语也好或者是实时信息也好，去呃支撑这个大模型去完成这样任务的一个能力。

那我们呃ZLIS这家公司呢是有两套内容啊，在下面啊就是一个是我们的开源的MUSE限量数据库，这里呢近期应该在GitHub上面会超越有两万个star这样一个热度。

那还有呢就是近期我们呃提供的一套这个呃限量数据库的产品，这这两套开源的和商业化的呃这个能力呢，在第一批里面已经是被集成到这个OpenAI的这个插件系统里面去的，OK呃那从这一点出发的话呢。

其实是对于我们现在的这个数据构建的范式呢，呃提供了一个新的框架，啊这里我我自己总结就是MVP的框架，M呢这就指的是这个大语言模型，V的话呢是限量数据库，P的话就prompt。

那大语言模型呢更多的是解决呃这个分析和推理的能力，以及说很重要一点就是以这个自然语言为基础，呃做这个语言对接的能力，那限量数据库呢呃就是为这个前面大语言模型提供一个呃long term的memory。

以及知识库的这样的一个能力，那prompt呢是在结合在两个前两个基础上呢，呃完成一个解决一个这个我大语言模型加知识库，向呃具体的业务适配和对接的能力。

OK那这里面其实呃像包括我我观察我观察到就国内啊还有北美也好，就是大家其实呃在知识库的构建上面这一套架构已经有一些落地了，那这里呢有一个思考。

就是呃我们的大模型和这个限量数据库说白了就是我的这个分析和推理单元，还有我的这个外存我的记忆体之间的关系的事情，那呃现在我看到就是说不管是呃咱们国内北美或者全球化去做这个大模型的这个竞争。

其实还是在沿着呃一个传统路线，就是说我要去尽可能的把我的这个数据质量做高，然后呢呃训练数据啊训练数据的质量做高，然后训练数据的规模做大，然后呢我要找到更好的这个模型结构。

把这些语言的这些分布呢去编码到模型里面还是这条路，那呃刚才讲到了就是说这条路其实有很多的局限性，然后另外呃就是我们可以想象一下，就是说其实人的话人脑呢本身的这个参数要比现在大模型要多得多。

但是呃人脑并不会选择我把全世界的信息都编码编码，然后呢记录这个死记硬背到我的脑子里面，也就是说现在大模型OK能力是很强，但是我们观察到就是说大量的这些参数都被用在记忆。

这些这种常识或者是一些这种可以被检索到的信息里面，然后呢呃这些信息其实是可以外推到限量数据库的，也就是说如果我们有一种手段把限量数据库里面这些死记硬背的内容。

大量死记死记硬背的内容推到限量数据库这一部分，那我就有更多的参数可以去支撑更强的这个分析和推理能力，OK，呃另外一点的话呢就是我等会会讲啊，就是说关于整个端到端的效果。

因为我们现在看比如我自己用拆机POK，其实在工作中呢更多的是一个语言层面的，就比如说我写了一段话，他可以帮我把这些文字写的更好，但是在业务层面的话呢，其实是很难端到端的完成内容。

那像国内不管是我们知识库还是问答系统或者是什么呃，个人助手等等等等这样的一些应用，其实大家业务上面关心的是更多的是关心我端到端的业务效果啊，那也就是说如果我们把这个注意点从只是从这个。

从只是这个关注这个大模型单一的能力到呃关注三个维度，也就是一个是大模型本身的能力，还有一个就是我的知识库的能力，还有一个就是大模型去应用知识库的能力，这三个维度去做的话呢。

是极有可能在这个业务端到端效果上面超过一个单维度的大模型的能力的，OK，呃这里面是关于我们呃开源和商业化的一些工作啊，因为今天这个时间比较有限，我就快速讲一下，这里面我们现在典型的呃这个开源项目呢。

有这么四个啊，就Mules的话呢是解决这个呃限量数据库的内核能力，呃Tohi的话呢是解决非机构化数据的一条的这个框架工具的能力。

像GPT-Cache呢是解决一个就是限量数据库到这个呃大元模型的一个适配这部分的工作，那阿图呢是解决上面所有的这些生态的一个包括限量数据库的一个管控的能力，那像呃这个今年的话呢我们2。

0已经进入了一个呃Mules2。0呢已经进入一个分定呃非常稳定的一个状态，呃到今年应该是这个月左右在Github上热度应该会超过两万个star，呃这里就不展开讲了。

如果大家去呃呃这个关心我们项目的话可以到我们的这个项目的Github上面去了解更多的信息，然后呢另外一个就是TOKI这个项目是这个解决的是一个Pipeline的问题啊。

就是从最早的飞行化数据业务那边然后呢呃ETL然后再做再进到限量数据库，然后最后呢把所有的这些大模型啊限量数据库啊这些连接到一起。

呃值得一提的是我们呃今年呢在6月份就会在国内提供这个限量数据库的SaaS和PaaS服务，像在在此之前的呃比较靠谱一点的这个限量数据服务主要集中在北美，呃我们会在今年6月份登陆像阿里云。

然后百度智能云腾讯云还有这个金山云这样几个主要的云厂商都会去呃大家可以用到我们的服务，呃时间是时间是6月底，然后我们现在呢其实也呃陆续开启了就是呃企业用户的这个申请试用的阶段。

所以这一块不管是呃生态合作伙伴还有就是企业用户的话呢，可以开始联系我们去呃做这个限量数据库的试用，ok 呃现在呃到目前我们重要的一些合作伙伴包括像英伟达呀。

还有呃北美这边的这个大的模型的厂商还有国内的像百度文心呀minimax这些都是我们的合作伙伴，那呃云的话呢刚才讲过就是除了北美的AWS GCP以外呢，我们今年也会覆盖呃国内的主要的这个公有云。

ok 呃对这个广告部分说回来我们再跳回来调到这个呃这个MVP啊，就是大模型加限量数据加prompt安定性这一部分，那这里呢我们在这个呃框架下面做了一个小的项目，这个项目也是为开源的社区做的一个事情啊。

这个事大概是理念是什么呢，就是说现在的每一个开源社区它的这个知识其实散落在各个地方啊，比如说我有我的文档啊，我做了一个开源项目会有文档会有在赛克罗夫罗上面大家提的问题或者是我自己的forum。

还有就是说像slack或者是企业微信里面大家去交流的问题等等等等这些问题，那我对于一个这个项目的成员来说呢，我想去呃了解到这个项目里面的知识。

那这个项目呢就是说把刚才这些各种各样途径的这些内容都为每一项目呢去构建一个知识库，那这样呢每一个呃开源项目的成员，他都可以以对话的形式对这些开源项目以一种呃，这个问答的方式去获取有史以来所有的知识。

ok，这里举一个例子啊，呃这里是问的是GPT4啊，就说哎，这个这个Mules系统里面TTL是干什么用的，你能不能帮我解释一下啊，这个这个里面就是典型的，它出现这个一分钟就胡说八道这个问题。

就说前面一段我们看还是对的对吧，就是这个time to live这个事呢，前面解释挺好，然后接下来呢，他会给一段这个代码告诉你怎么写，但是下面其实大家如果了解Mules系统的话是属于纯发挥的内容啊。

其实按照这个下面去做的话呢，都是错误的啊，但是这个大模型的本身是不知道的，这是GPT4去做，ok，然后接下来的话呢，呃我们在这个系统内部其实是拿GPT3。5，然后呢。

加上一个结合一个知识库去支撑这样一个任务，那我们可以看到说他对于这个任务实际上回答的非常好啊，反过来呢，其实证明了我刚才说这个事，就是说呃我拿这个限量数一个知识库外挂的知识库。

然后去增强一个弱一点的大语言模型，其实面向业务效果来说是可以干得过这个最好的这个比GPT4的这个大语言模型的，ok，这里面这个是这个系统整个系统架构，我们就不展开讲了，然后呢。

接下来这个时间希望呃跟大家分享另外一个有意思工作，就是GPT Cache，就大家用GPT的话呢，会或者其他大语言模型会有这么几个问题，就是说GPT4非常好，或者是这些付费的这些大语言模型都非常好。

但是他是按token收费的啊，非常贵，然后呢，整个我如果是拿这个东西去上面支撑业务的话呢，呃他support其实也跟不上，latency都跟不上，一个字一个字往外蹦嘛，那不管是成本还是性能来说呢。

都是一个比较大的问题，那我们这个项目呢，就是解决这个问题呃GP开始他的思想，主要思想是这样，就是说我在这个大语言模型上面再加一层，那一方面呢，就是说OK这个图可能看得更清楚一点。

就是说我对外用户来了问题，来了问题之后呢，我先去查我的缓存系统，如果缓存系统命中了，然后呢就是缓存系统直接这个结果就出去了，如果缓存系统没有命中，然后呢再走下面的比较昂贵的大语言模型啊。

这个架构呃思路比较简单，跟传统的这个缓存没有任何区别，但这里面还是有些trick，就是说像传统的缓存系统，其实举个例子，我想是一个KV的，那key呢是完全匹配，我是拿出来对的，但这里面的话呢。

呃同样一个问题，它可以有不同的说法，或者是相关的话呢，这个问题对应的答案，它不一定是对的，所以这里面我们对于大语言模型返回来之后，缓存系统上面呢，是要再加一个这个小的模型的啊，在这一部分加上之后呢。

等于说是这个故事变成这样了，我在上游呢，用户问了很多问题，然后呢，我把呃高成本的这些拿回来答案，积累到缓存层，然后我在缓存层里面呢，再内置一个开源的，或者成本相对低的这个小的模型，那这个时候呢。

如果说我在上面的小的模型，加上大语言模型提前拿到知识，这两部分加起来能够支撑这个问题的话，那我就可以以一个非常低的成本，去完成一个大语言模型的任务，而且效果是基本是一致的，啊，呃，这个可能可能有点绕啊。

就就等于说是大语言模型，我们去呃不停的去问，然后拿到的答案，这些答案对于上层的这个缓存层的小模型来说，它是一个外挂的知识库，啊，然后我们拿这个知识库再加上一个开源的模型，就可以提供一个成本非常低廉的。

在缓存层提提供成本非常低廉的这个问答的能力，OK，呃，对时间差不多啊，主要呃分享的就是这些内容，然后这里呢是我们的开源的项目地址，还有公众号，如果大家对于这个我们在做的事情比较感兴趣的话。

可以欢迎下来去交流，嗯，好，谢谢，好啊，感谢观众经常经常分享啊，下边呢我们有请啊黄士宇第四范式强化学习科学家，Open RL Lab负责人来跟我们分享，Open RL 通用的开源强化学习框架，OK。

呃大家好，然后感谢这个独老式的邀请啊，然后我叫黄士宇是来自这个第四范式的强化学习科学家，然后也是这个Open Lab的负责人，然后我们这个Open Lab的话主要就是一个强化学习的研究团队。

然后我们主要就是做这个强化学习研究和一些强化学习技术的开源，然后我们现在团队人数还不大，所以的话也欢迎各位对强化学习感兴趣的同学加入我们，然后我们这个组织的话就是一个是可以去做这个开源。

然后去做这种社区建设，然后对如果对算法感兴趣的也可以来我们这里做强化学习算法的研究，然后去发论文，然后我们公司的话也会也有这种强化学习相关的一个业务。

所以的话如果都这个实际业务感兴趣的同学也可以去做这个强化学习的这些业务，然后在左边这个二维码是我们的这个repo，就是我们这个Open RL这个强化学习框架的repo，右边是我们这个QQ交流群。

然后欢迎大家能够进群进行交流，然后，嗯，动不动，嗯，嗯，嗯，嗯，嗯，嗯，嗯，然后今天主要讲三部分，就是第一个是简单介绍一下这个强化学习，还有我们为什么做这个强化学习框架。

然后第二个的话就是去介绍我们这个强化学习框架的这个主要的功能和特点，然后三个的话就是我们这个强化学习框架的未来的一些工作，首先是对我们这个强化学习还有我们框架的介绍，首先是什么是强化学习。

强化学习可能大家听过很多，然后强化学习是目前是这个技学系三大训练范式当中的一个，那么我们做强化学习也是为了最终的目标是为了去实现这个通用人工智能，也就是强人工智能。

为什么我们会觉得这个强化学习是实现这个通用人工智能的一个工具呢，因为在我们的这个现实生活中，我们这个其实训练不是监督学习训练的，都是大部分是通过这种反馈基于奖励反馈的这种训练。

比如我们训练一个小狗让他去这个捡我们这个扔出去的东西，那么一般是如果他捡回来这个东西我们会给他一个奖励，所以的话包括人我们人这个也是通过我们的金钱通过我们一些成绩去给我们做一些这种物质啊经验上的反馈。

然后让我们去提升自己，所以的话强化学习是一个更加通用的一个学习方式，这是一个例子，这是一个小小的游戏，然后用可以用强化学习去训练，然后就是我们把这个数字扔出去，然后这个小狗就可以把这个数字捡回来。

然后这个后面这个小狗的这个算法就是通过强化学习训练的，然后呢这个强化学习，除了做这种游戏，可能以前大家看看到比较多的是做这个游戏，但强化学习应该还有很多其他实际的用处，比如说这个机器人，机器人的话。

这是比如这是OpenAI，他们19年做了一个控制手机械手臂去完成这个魔方的这个求解的这样一个机器人，然后右边的话是一个自动驾驶的模拟器。

然后我们可以用强化学习在自动驾驶模拟器里面去训练我们的自动驾驶算法，然后另外的话还有强化学习可以用在这个工业设计里面。

比如说这个英伟达和他们是通过这个就是去年用这个强化学习去训练他们去学习他们芯片的设计，然后还有包括骨科的这个TPU也是用了强化学习去做他们那个placement的这个设计。

然后另外的话还有这个包括这个股票的买入买出也可以通过强化学习去训练，然后就是回到这个最近大火包括前面几个topic全是做全是说这个language model。

所以这个强化学习在这个language model里面也是非常非常重要的一环，其中一个点就是为什么要用强化学习这个里面的原因很多。

我说一个比较简单的一个点就是当我们比如说现在GPD 4它的那个tokenizer的长度能达到8000，如果我们要去通过监督学习去训练这个大模型的话。

我们要准备一个8000的response就是8000长度的content的一个response，那如果我们要专家去写这个response这个需要非常非常多的那个资源的。

然后但是我们强化学习只需要啥只需要reward，那我们让机器人或者机器模型他给出了这个response过后，我们让人去标个reward就可以了好与不好就可以了。

这直接把一个需要准备一个8000的token的这个长度的一个数据变成了只用准备一个reward model，或者只用准备一个reward。

它大大降低了我们这个训练大模型的这个难度提升了我们这个训练的效率和这个标注的效率，另外的话强化学习还涉及到多智能体还有这个对抗强化学习多智能体的话，就是说我们有多个智能体相互配合。

现在我们前面讲的不管是大语言还是什么的，他只有一个智能体是吧，训练的时候也只有一个智能体，但是我们可能在我们社会生活当中，他是有人与有多个智能体相互的交互，然后包括多个智能体的相互竞争的。

然后所有上面这些环境可以看到是多种多样的，然后我们做了一个这个open R的话，就是一个通用的一个强化学习的三R框架，就是他不仅能支持单智能体，不仅还能支持多智能体还能支持这个self play。

另外的话我们也是支持这个大语言模型的这个RHF的训练，所以的话我们是一个把环境进行了一个抽象的，抽象成一个模拟器的一个这样一个强化学习算法框架，然后他能更加的通用，另外的话强化学习本身。

他是有非常非常多的算法，首先按照大类飞机有很多的分法，单智能体多智能体，然后off-police还是on-police，是value based还是police based。

还是model free还是model based，你看就按照大的飞机他就能分出一堆来，如果切分到特别细小的这个算法的话，他更是特别特别多的强化学习算法，所以的话我们的话就是说。

我们的框架就提供一个抽象，就是我们对强化学习算法进行抽象，然后所有的这种小的这种算法，其实就是我们其中一种实现而已，然后强化学习的话，还涉及到这个分布式与非分布式，一般在学术界的话。

大家用的都是单机多卡，或者单机单卡的这样一个训练方式，然后可能在工业界又涉及到，要分布式的进行训练，比如说你RHF，你可能必须要上多机多卡进行训练，那多机多卡的话就涉及到，首先你有一堆actor去。

在一个集群里面去收集这个route数据，然后数据好了，这个数据过后你要发送到一个，data server里面去存储你的数据，然后你有一个another，就是你的训练你loss的一个模块。

然后要去这个data server里面去，拉取这个数据，然后拉取了数据过后，进行学习，学习完过后，他再把这个模型又把它分发到，集群里面去收集数据，那怎样这一个loop的话，其实是一个比较底层的一个。

数据流的一个实现，但是可能对于上层的算法工程师来说，他并不关心你这个是怎么样的，所以的话我们希望是提供一个，比较统一的接口，就是说他对单机单卡训练和，多机分布式的训练，他都是对于算法工作人员来说。

他都是调用一个同样的接口，然后我们去提供这样一个底层的能力，帮他完成这个分布式，那我们的话去年也是去做了，这种分布式的这种东西，然后能让科研人员，他实现代码过后，能够立刻在单机单卡上进行测试。

然后又能切换一个parameter，然后在单机多卡上进行测试，然后又能切换一个parameter，然后到多机多卡进行测试，这样的话就不需要，科研人员去改任何的代码，他只要算法设计好了。

他就可以从单机到多机的，这样一个简单的适配，然后我们的这个framework是长什么样的，我就是这个左边这个样子，就是环境肯定是要有的，不管你做NLP还是做什么，首先你要init一个环境。

然后环境我们肯定需要一个网络，然后就第二行，这个就是我们要去初始化一个网络，第三行就是我们要去初始化一个agent，然后agent的初始化稿好过后，我们就agent的点train，就可以开始训练了。

然后训练好了，我们下面就是怎么去测试，就是agent的点act，然后输入你的observation，然后你就可以去输出，这个action需要的动作，然后我们所有的这个编程模式，都遵循这个简单的一个模式。

不管你是选NLP还是选任何多智能体，单智能全都是这个样子，然后我们Open R的话，就是去统一了各种各样的环境，各种各样的算法，不管是off-police，就是离线的强化学习，也是通过这一套去训练。

然后还是单智能体多智能体，Sell play还是分布式的，还是单机的，我们都是去通过这样一个，简单的调用接口去实现，然后接下来主要是讲一下，我们这个Open R它的一些功能和一些特点。

然后Open R的话，它是一个Python实现的，一个强化学习算法框架，然后后面的话主要是基于PyTorch的，首先是我们这个框架的话，因为作为一个开源框架，我们希望有更多的同学能够来使用我们的框架。

所以的话第一个做的就是要足够的简单，在简单性的方面，我们第一个它是安装，我们是Python的库，所以很简单的Pip install Open R就可以装了。

然后如果对Docker或者KPS使用的比较多的同学，也可以直接通过这个拉取Docker镜像，我们为大家准备了基于GPU和没有GPU的镜像，然后另外的话。

用Onaconda同学也可以通过Connach进行安装，还有如果大家又不想，完全不想装就想浏览器看一下这个能不能用，我们也提供Google的Collaboratory这个方式，大家可以去试用。

然后装好过后的话，大家就Open R，我们提供一个命令行工具，能够快速的去验证你装好过后能不能Run，或者Run的怎么样，我们提供了一个ColorPool的例子，然后在大家的普通的笔记本上。

只需要几秒钟就可以训练好，然后这是我们刚才ColorPool的一个例子，这栏不像刚才那个我们是给了伪代码，这个是我们实际调用我们Open R训练的一个代码，就很简单，就Make通过我们的Make函数。

你就可以去得到你的Environment，然后在Environment里，传入你要的Environment训练的就是并行的数量，然后后面出发神级网络和Agent，然后就可以开始训练，然后其他更加复杂的。

可能需要做一些定制化，但是也是遵循了这样一个编程的方式，能够让大家非常快速的去使用我们的框架，然后另外的话，作为一个国产的强化学习框架，我们是提供非常详细的中文文档，还有手把手教学的Tutorial。

刚才那个是为了给初级用户，但是如果你说你是个科研人员，或者是一个强化学习的专业人员，那我们也给专业人员去提供了足够的定制化的能力，因为我们也是基于我们这个框架去做一些研究，我们也要去不断修改一些代码。

网络什么的，所以我们也会去考虑到这方面，首先我们提供了这个配置，就是我们自己训练的时候，需要不断的去调餐，还有Seed这些东西，包括我们去NOP的时候，需要加载Hackinface上的模型或者数据。

我们都可以通过一个YAML文件去搞定所有的配置，具体怎么去使用，这个使用也很简单，我们命令行传入这个YAML就可以了，大家可以在文档里面去找到具体的使用方式。

另外的话就是我们可以定制化的去输出我们的Log的信息，大家不需要去改我们底层的强化学习框架的代码，只需要去继承我们一个叫vectorinfo的一个类。

然后在类里面去定义好自己需要输出的这些训练的时候的信息，然后就可以去完成这个1DB上的一些Log的输出，还有这些它也会同步输出在你的Terminal里面，然后我们也可以一键切换到TensorBoard。

如果大家不喜欢用1DB的话，然后的话，刚才那个Ray的同学讲了一下，就是那个Ionib，为什么大家用的科研人员用的不太多，因为他没有做很好的这个抽象和这个模块化的这个设计。

我们的话也是就是做了很好这个抽象模块化的设计，首先就是reward model，以前的强化学习框架可能没有做太多这块的这个抽象设计，因为环境里面就自带了reward model，后来我们在做研究的时候。

我们做diversity area的时候，发现这个reward model是需要，这个reward需要通过另外一个模型去改的，然后包括我们做NOP的时候，这个reward也是通过一个模型生成的。

所以的话我们单独把这个reward model做了一个抽象化，然后大家只要继承我们的这个reward model的这个积累，然后就可以去实现自己的一个reward model。

然后去给出任意的这样一个网络，这个reward，然后其他的关于polis和这个venue network，还有这个algorithm，我们也是做了这个抽象和模块化，然后这样就可以。

这样是对这个科研人员比较友好，因为科研人员主要就去改这些算法呀，还有这个模型啊这些东西，另外的话，还有就是一些强化学习比较高阶的一些使用方式，比如说这个dictionary observation。

就是我们的强化学书是不可能不只是像量或者是图片，我们可能是一些混杂的，然后我们也是支持的，然后还有这个是串行和并行的环境，就是强化学习训练的时候，我们需要这个大量的环境并行。

但是如果环境一开始就并行的话，它是很难调试的，所以我们支持这个并行和串行的一件式的切换，然后这个切换的话，我们也是保证了它这个环境的这个随机性是完全一样的，然后我们也是支持多种多样的。

RNN和这些transformer这些结构，然后我们也是支持这个AMP的这种混合精度的训练，然后还有就是我们在这个做这个route的时候，收集数据的时候。

我们可能并不需要特别高精度的一个网络去进行数据收集，因为它收集数据的时候，本来就会去主动的去引入一些随机性，所以我们也支持这个半精度的这个，这个actor network，还有就是强化学习。

可能和language model一样，我们后面也需要站在别人的肩膀上利用别人的模型，别人的数据，所以的话我们也是支持从hackinface上去导入模型和导入这个data set的。

然后我们也提供了这个gallery，就是我们不是只是把这个框架代码给大家，我们也是提供了这个gallery，然后通过这个gallery，大家可以实际的去了解到我们怎么去用这个框架。

我们这个gallery里面有单智能T的多智能T的连续动作的离散动作的，然后还有NOP的例子，还有图片的例子，都是大家可以去找得到了，然后通过这些例子，大家可以知道我们这个框架该怎么用。

然后我们实际上这个框架也会关注它的performance，首先它要够快，然后我们刚才也说了，我们这个colorpool在普通的笔记本上不需要GPU，只需要几秒钟就可以训出来。

然后还有就是对一些language model的训练，我们也是有17%的加速能力，然后还有的话，我们也是去做了一些实际的一些任务，我们在那个football的一个比较难的一个多阵题任务上。

当时也是排第一名，通过我们框架训出来了，然后还有是去，我们在这个language model上，这个对比的是和superized learning区别，然后最高的话这个指标能达到，就通过RHF去训练。

能达到这个43%的这个性能的提升，然后未来的话，最后讲一下我们未来的工作，就未来的话，我们是去面向一个更大scale的一个强化学术训练，一个是更大的模型。

我们现在其实我们language model最大模型，我们采用到了那个GPT-2，所以的话我们未来会用更大的上币的模型，或者几十币的几百币的模型去训练，然后我们现在正在介入的就是deep speed。

就是说也是我们后面会开源给大家用的，就是加上deep speed，我们这个强化学区框架能够去做，非常大一个模型的RHF的训练，然后还有就是更大的这个cluster，因为大模型的训练。

涉及到这个超大规模的集群，然后我们后面也会去集成这个ray和这个k-by-s，去在这个大规模的这个集群上进行强化学训练，还有这个更大的这个population的训练。

就是我们也是这个酷式可以训练多智能体和这个self play的，多智能体的话我们去年的工作就是能在，首先我们是第一个在这个Google football全场游戏上，这种多智能体完成这个训练的工作的团队。

然后另外一个我们是第一个在50个智能体上，完成这个多智能体深度学习，强化学习训练的一个团队，然后所以的话我们未来会在更大的一个，智能体的构思上进行深度强化学习的训练。

然后我们也会去更加去注重这个开源的建设，然后包括是把我们训好的模型的，训好的模型放在hugging face上，还有我们的代码我们的demo也会放在，同步放在我们的GitHub上，然后还有一些实验结果。

我们也会通过那个1db分享给大家，然后这是我们的这个repo，然后这是我们是5月份刚开源，没有多久，但是已经是成为强化学习这方面，前期增长最快的一个repo了，最后谢谢大家，好感谢黄士宇的精彩分享。

刚才我看到就是大家对这个Zilis非常受欢迎，跟大家卖个广告了，就是说包括Zilis还有迪斯范是都是，Alpha&Data基金会的最活跃的核心成员，这个Zilis的创始人星爵是上一任的。

Alpha&Data基金会的董事主席，我们刚刚跟大家分享的，这个迪斯范是同样也是董事会成员，应该相信不久就会登陆香港股市了，今年之内大概就会看到登陆香港的stock exchange了。

另外在Alpha&Data，其实我们还有这样子非常有潜力的开源公司，有好多不少，刚才我看到这下面有很多在投资的有兴趣的，其实我们以后会有很多这样子的交流的活动，希望大家来多参与。

下面我们就有请徐南中科院自动化所副研究员，中科文革产业中心副总经理，跟我们分享浅谈大模型及眼镜技术的发展应用，有请徐南，下面我就分享一下我们在大模型方面的一些工作，主要是这三个方面。

包括我们对TEDxDVT和未来大模型趋势的一些看法，及最后分享一下我们中科文革在大模型方面的一些探索，以及我们在开源方面做的一些工作，其实大家都在接触TEDxDVT。

就是GPT相关的大模型给社会带来最大的一个变化，就是让知识的获取和调用越来越简单，成本越来越低，这个我就不展开了，回归整个GPT大模型的技术，可以看到其实到GPT-4，它所用到的所有的算法。

以及相关的这些技术，其实都是过去半个世纪以来积累的这些技术，是一个极大程的突破，是一个工程实践的里程碑，包括用到的像对抗 大模型，以及强化学习等等，其实都是其他公司产生的。

所以TEDxDVT就把这些技术做了一个大的集成，这张图可能我又提到这张图，今天已经第三次出现了，其实目前来看我们对于TEDxDVT的理解，还是一个黑盒，它为什么能做各种任务，它的涌现等等。

大家都是去猜测，虽然是一个黑盒，但是它给我们带来的一些启示，包括对于统一知识的表征推理，以及对于世界知识的认知和建模，这些问题的探索，给了我们一条路，就是大模型的这条道路。

整个TEDxDVT它的一个突破，其实人工智能的三个要素，就是算法 算力和数据，那么TEDxDVT，它其实也是在这三个方面发挥到了极致，刚刚也一直提到的，它的算法其实是一个技术密集型的工程，算力就不说了。

在这块的整个训练，投入的是相当多的GPU的资源，以及它在数据方面的搜集和相关的积累，未来我们针对，不管是从企业界还是学术界，我们研究就是，接下来在大模型发展的几个趋势，第一个就是。

信息技术应用和创新生态的一个新的范式，目前来说，大模型我们可以看作四个阶段，每个人基本现在都在使用大模型，不管是从个人 企业，从最早的开始上手，包括后续的我们结合一些Prop的构造，运用一些工具。

以及把相关的固化的工作流，打造成一些工具，比如说Chats PDF等等，以及最终的形态，结合大模型和相关的在行业，以及领域方面的应用，去做一些大模型的生态，比如说微软 百度正在做第四阶段相关的工作。

第二个趋势，也是大模型本身的演化趋势，就是从最早的Chats，自然语言的交互，到GPT-4实现多模态的，以及近期的用大模型和一些机器，做一些人机交互的，那当然未来我们甚至可以猜测，当然我比较乐观。

可能猜测未来GPT-4，未来的GPT-X，甚至可能会真的产生一些情感，或者交互意识等等，甚至可能通过这种广义的图灵测试，所以其实它的进化速度，也是越来越快 越来越智能，那最后第三个趋势。

从企业或者从应用这个角度来看，未来还有另外一个趋势，就是大模型它是面向这种更加垂直领域，或者说是往更加轻量化，小模型这个方面，也是一个发展的趋势，这里最近也有很多在领域，以及轻量化有很多开源的工具。

目前据不完全统计，当然这个是4月份陆奇博士的统计，目前国内已经有数十家大模型的机构，包括公司主要企业界 学术界都有，也有上千家依托于大模型去构建自己的应用，和以大模型生存的这些公司。

其实国内外现在大模型，其实是一个井喷式爆发的一个趋势，那么我们中科文革，为什么还要继续去做一个专属的大模型，或者说面向企业这个方面呢，其实我们也包括发布的这些通用的基础大模型，我们想去做一个什么呢。

做一个面向领域的垂直的一个大模型，另外我们要去探索，能够让用户可用的安全的，能够本地化部署的一个大模型，所以我们自己推出了我们的一个亚裔的大模型，我们重点就是去面向领域。

然后提供一些安全可靠的一个大模型，以及相关的工具链，那我们这个大模型呢，在6月3号正式的推出，当时这一周，刚刚到现在一周的时间，也是受到央视 新华网 环球 凤凰等多家媒体的报道。

说明我们在领域探索这条路上，得到了主流媒体的关注和认可，那么亚裔模型，目前我们主要聚焦的就是领域，深耕领域，那么提供了包括领域知识问答，复杂场景信息抽取等五大的核心能力。

那提供了上百种领域的这种特色的技能，那第一个其实就是大家经常考虑的，就是大模型在回答问题过程中，它的一些幻觉的问题，那解决幻觉问题最大的一个方式或者工具，就是通过互联网去做一些相当于是一个。

实时的数据的接入，那么我们举个最简单的例子，志愿也是在这个中国村论坛一系列的，那我们就问一下，今年中国村论坛的一个主题，通过搜索引擎就可以精准的找到，那今年是开放 合作 共享未来。

那开放一直是整个中国村系列，我们今天这次论坛的一个主旨，那么接下来，那我们真的面到一些领域，比如说我们在做新闻 媒体方面，那我们可能借助大模型，不是去问它一些通用的知识，那我们可能真的就关心。

比如说这两天高考刚结束，那我们就问 询问，通过亚裔去询问高考的热点信息，那它就会排列了这个，根据互联网的结果，找到了这个十篇高考相关的热点新闻，当然我们还做了很多工作，包括针对某个热点去询问它的摘要。

询问它相关的这个网民，以及不同媒体客户，对于这些事件的一些观点，这我就不再展开了，那第二阶段其实是，我们在这块做的一个重要的一个探索，就是领域的知识问答，那这大模型刚才提到了，就是它在目前已经开源的。

这些都是通用方面的基础的能力，那真正把它运用到领域，运用到实际的业务生产线上，那接下来需要怎么做呢，我们也在这块做了很多，插件和能力的训练，第一个方面就是我们在金融，金融领域的一个探索。

比如说我们在财经投研方面，我们对于一些原因，企业的一些风险，原因 风控这些，那我们就是把我们的模型，和一个非常高质量的一个领域知识库，进行了一个深度的一个对接，这是我们的合作伙伴财联社。

目前是国内财经领域的一个，非常top的一个机构，它们的一个高质量的金融问答的，一个相关的知识，融合在亚裔模型的交互中，硅谷银行，今年3月份硅谷银行倒闭，相关的它的倒闭的原因，产生的影响。

都可以通过亚裔模型进行一个，精准的回答，那第二个领域知识问答，就是刚才其实相联数据库的，那位老师也介绍了，就是关于这种离线的长文本，怎么和预训模型进行一个融合，那我们其实在做的就是，长文本的问答。

这一块就是更多的就是面向那种，私域数据的客户 企业 政府，它们有很多内部的数据，比如说银行，比如内部的政府，它们的数据是，第一 没有在互联网出现，二 也不可能通过OpenAI的接口。

通过互联网进行一个传递，那么这个时候就需要大模型具备，它内部文档的一个能力，怎么实现这些能力呢，这里我们做了很多插件，包括相联数据库，ES搜索 排序等等，结合我们的亚裔模型，比如说这个是我们基于两篇的。

一个是AAGC的一个白皮书，200多页，还有一个关于腾讯研究院，关于数字人的一个，PPT的一个PDF的文档，我们就可以根据亚裔大模型，进行一个交互，一次就可以阅读上百页的PPT。

当然这个演示系统只是支持部分，后台其实可以实现，百万级的这种文档的接入，整个回答都是在几秒以内，就可以进行一个回答，接下来就是真的，我们把大模型运用到我们实际的，具体的场景和我们工作流畅。

它带来的一些工作，带来的一些抽取，我们第一个就是面向，复杂场景的信息抽取，这里我举一个例子就行，这是我们面向，舆情安全领域的一个应用，我们每天互联网上有很多新闻的报道，我们作为一个舆情分析师。

作为一个安全人员，我们可能关注的是这些新闻，它的一些关键信息的要素，来方便我做下一步的重点分析，这个就是针对美国的一个枪击事件，我们通过亚裔进行抽取，它发生的事件的地点，事件的详情，人员的伤亡。

事故的原因的一个总结性的概述，然后精准地抽取了它相关的这些信息，当然我们不止这一个要素抽取，还有更多的能力和大模型进行持续的交互，就可以在十分钟的时间，就可以写出一份非常完整的预警报告。

涵盖上百篇新闻文稿的一个集合，当然都是通过模型和互联网速度引擎，与我们的知识库多次的一个交互完成，这是我们在领域上的一个探索，接下来就是在多语言方面的一个，我们知道大模型本身就具备天然的多语言的能力。

因为在预训练的阶段，它也使用了很多多语言的数据，我们继续在领域上的探索，因为可能不关心的是传统的翻译的能力，可能真的是我们的一个合作，我们的一个客户，他每天都要看全球的各种新闻数据。

大模型本来就在多语言能力上，就有很多鲜艳的优势，这是一篇俄文的报道，报道的是北京举办的世界环境日的一个相关的新闻，可能就是需要快速地总结这种多语言新闻，它的概述 事件发生的原因。

以及产生的结果 影响等等，都是通过我们在多语言训练之后，在事理方面做了一些解析，它也精准地找到了事件的概要等等一系列的信息，都可以支撑我们舆情分析师在海外媒体使用过程中的多语言的场景。

另外就是我们针对金融客户，他们对于企业 对于舆情负面这些方面的影响，经常企业很关注自己的产品，在互联网这些媒体报道的满意度，我们的模型在这块的应用，主要是针对去直接抽取他关心的这些相关的新闻。

对于他的产品的介绍，还有他是否满意，从情感原因以及意见方面的一些抽取，这是我们在直接抽取，然后并按照Jason的格式，直接送到他的业务线上进行一个抽取的任务，接下来就是在多模态内容生产。

因为本身大模型它的生产能力是它天然的优势，我们在这一块的重点的优化和训练，更多的是面向企业直接产生生产力的这块的一个方面的探索，这是我们在金融方面的一个问题。

每天各家公司都会有在他的平台和一些公开的渠道上，发布自己的快报，作为一个财经投颜的客户或者说用户，他需要快速的消化这些信息并生成一份快报，这个就是我们上线以后，直接运用在各大的企业平台上。

生成这种持续不断的业绩报告，然后股价的报告等等，都是可以通过我们的亚裔模型进行一个快速生成，过去这个过程可能需要十分钟半个小时，现在只需要部署上线之后几秒就可以快速生成这个报告。

当然诗歌创作这些都是正常的能力，这个我就不展开了，包括去调用一些AI绘画的插件，把刚才那个诗歌做一些绘画，以及我们把整个亚裔模型作为一个智能问答对话的大脑，以及各种AI类插件的一个大脑。

然后我们和智能的虚拟人进行一个实时的交互的互动，都是通过我们的亚裔模型进行一个交互和体验，接下来这是我们在亚裔模型的一些能力，其实提到了今天的论坛的主题就是AI的开源，我们也是希望能够在这块做一些贡献。

比如说今年五月份科技部最新发布的，中国人工智能大模型的地图研究报告，它显示了目前其实已经有国内超过一半的大模型，都是实现了开源，但是这其中主要的开源的主力还是来自于高校和科研机构。

目前中国大模型开源影响力前十，来自于高校和机构，比较代表性的像吉尔尔姆 复旦大学的MOOS等，目前企业在这块还需要持续不断增加影响力，目前前十的留有两家大模型，一个是百度的文心，还有贝克的Belly。

中科文哥作为一家科技企业，我们也是坚持开源开放的情怀，也愿意为这个市区在大模型这条路上，贡献我们的经验和数据，所以我们也把我们刚才展示的亚裔的大模型，进行了全面的开源，包括我们的代码模型。

以及在面向领域，包括金融 媒体 安全 运行等等，这些领域的高质量的数据进行了开源，我们希望通过我们把大模型开源的路，为中文预训模型的社区贡献我们自己的一份力量，针对我们本次开源。

当然我们发布会6月3号已经实现了开源，当然已经放到Git和HyperFeed上了，我们整次开源的是我们70亿参数的版本，它也是我们基于一个预训模型的权重，通过在百万级的领域任务上微调得到的。

当然在迭代的过程中，我们也有数百名用户的持续不断的测试内测，也是持续增加了它的忠实性和安全性，包括我们也把我们面向这些媒体，金融 舆情等等这些数据，也进行了全面的开源。

7月份我们同时会进一步增加模型到150亿规模，继续强化它在多轮对话 逻辑推理方面的能力，并增加更多的插件和多模态的能力，到7月底我们也相关的公开我们在亚裔模型，整个训练过程中详细的技术报告。

以及我们相关的评测的数据，以及相应的评测工具，当然未来我们还继续会探索通用的基础的模型，刚才也是林玥也提到了，就是智源的基础模型，其实我们还是希望在基础模型能力上面，去通过大规模的资金流训练。

以及选用更加安全可靠的中文的数据，来去做一个基座的能力，这是我们模型在评测的，我们自己搜集的一些，当然不是很全面，当然主要是为了做一些领域和基础能力的评测的数据，我们也是邀请了多位领域专家。

在这块作为我们人工评测，当然在对比的几个模型，包括主流的这些来自于企业的，Chats GPT 文心语言 通译 星火，以及一些开源的这些像GM MOOSE MECONA等等，结果在领域方面。

我们确实因为我们主要还是面向领域去做垂直训练，取得了一些不错的性能，当然在基础能力层面，目前我们也算是刚刚开始，在八个模型里边，目前我们的亚裔排在第六位，当然亚裔的只是一个初期的模型。

我们在面向领域大模型的构建过程中，也刚刚开始，我们也希望通过亚裔的开源，在未来进一步提升它的性能，共建亚裔以及大模型中文社区的生态，最后介绍一下我们公司，中科文歌是来自于中科院资通化所的一家企业的。

人工智能和大数据的一个企业，我们核心团队其实都是来自于资通化所，我们成立至今五年的时间，主要就面向社会计算，做一些媒体安全舆行方面的一些领域，目前我们是在全国已经有六家分公司。

总共是有接近八百人的一个规模，最后还是想说，我们相关的扫码都可以直接获取，我们亚裔的模型，也是希望刚好借助这次大会，也是宣传一下我们的一些工作，大家未来有兴趣，或者说是对于我们都可以多多参与。

或者给我们提一些意见，我们也希望通过它的开源，在面向领域这一条路上继续走下去，好的，感谢徐南先生的精彩分享，现在我们就到了今天的压轴环节，圆桌讨论AI与Data开源的趋势与展望。

我们首先有请今天的主持人李晨，李晨是Zilis开发者关系及市场运营负责人，交给你了李晨，谢谢杨先总，大家好，今天非常荣幸，然后能够在这边跟大家一个圆桌，来结束我们最后的AI和开源的这个分论台。

今天整体的议题也是非常精彩分层，然后到现在为止都留了非常多的同学在这边，非常感谢大家的一个热情，好，给我们工作人员几分钟的时间，咱们布置一下场地，今天的这几位嘉宾呢，也是我们开源技术圈。

还有AI大数据圈的一些专家和老师们，今天我们也准备了非常多的，跟我们热门话题相关的一些话题，帮助大家更好的去了解我们处于的这个时代，OK了，好，那有请这几位老师吧，首先邀请我们胡俊平老师，孟磊老师。

布林老师，菲菲老师，还有杨轩总，他那个没有对应，大家随便坐吧，对对对，都是老朋友，大家随便坐吧，OK，这个很尖锋的对话，其实很难聚齐这几位大咖，同时在一个台上去做，那简单的开个场吧，其实今年啊。

今年早些时候随着拆机弟弟航空出世啊，让全世界的目光再度聚焦到我们的人工智能的一个领域，李开复更是提出了这是一个AI2。0的一个概念，整个技术圈创业圈和投资圈迅速达成了共识，并快快速的进行了一个布局。

我相信2023势必是一个AI全面开启商业化的一个大年，今天在座的各位嘉宾都是开源技术圈的一个老兵，也是我们大数据和AI领域的专家，因此今天非常荣幸请大家坐在一起交流，在这个历史性的节点。

我们一起探讨一些大家关心的问题，那么先请大家给我们现场的观众朋友们做一个做介绍，从您开始，大家好，我刚才给过一个演讲，关于AI数据方面的开源，所以我是大概在有十几年在数据和AI领域的。

研发开源和业务的经验，然后我现在在ANData这个领域在创业，做一个初创公司，那么我也同时在LFANData基金会是去年和今年的board chair，我在Apache。

我是很早之前的Apache的Hadoop项目的committer PMC，等等等等吧，开源这个圈子，数据这个圈子，AI这个圈子，大概耕耘了十几年，谢谢军平总，好的，大家好，我叫孟伟，我是来自中兴通讯。

我本人除了在中兴通讯负责开源工作以外，也同时在LFANData董事会里面担任一些职务，同时在2018年LFANData成立的时候，当时叫deep learning，也是当时我们几家。

包括几家中国公司也是在帮助Linux基金会成立了这样的一个基金会，所以也是能够非常荣幸在这里跟大家分享一些观点，大家好，我是赵飞飞，我是中科院自动化所的服务员，也是中科文革的创始合伙人。

我们这个团队一直是在科学院体系做大数据，做人工智能这个方向的，然后成立的公司中科文革以后，刚才我同事也介绍到了，我们主要是专注于做领域的中模型，我们都不要大模型，7B 15B 20B，对就这样的。

然后也很高兴能够在这里跟大家分享我们的一些观点，好谢谢大家，大家好，我是杜琳，Bezaic 科技的创始人和CEO，Bezaic 科技起源于数据标注行业，但是我们在五年前就开始了我们的产品化和开源的历程。

我们发起了我们面向整个多模态的，全球首个的开源的训练数据平台，Xtreme1，并且在今年把它捐献给了Linux基金会，同时Bezaic 科技也是Linux基金会的成员之一。

很开心今天在这里和大家有机会交流，大家好，我是杨轩，我来自Linux基金会，亚太区，我主要负责的是，其中我主要的一个负责领域，就是在人工智能这个领域的社区的生态的发展，包括了LFAN Data这个社区。

包括了我们马上要开始推动的PyTorch基金会的社区，中国区的社区的建设，我们希望通过我们在中国开源社区的，推动开源社区发展的方向努力，希望让中国的更多的AI的公司，能够走向全世界，影响全世界。

我为LFAN Data再卖一个小广告，LFAN Data本身它现在是全球最大的一个，开源的AI的生态，而中国区的会员，是在LFAN Data里面有很大的话语权，我们有一半的董事会成员，都是我们的。

包括我们的主席，都是我们中国区的会员，所以实际上我们现在是一个，站在一个非常有利的位置，不仅是说能够让我们中国区的项目，中国区的公司走向全世界，同样我们能够把全世界的。

整个开源社区生态的这些专业人才专家，整个开源社区能够为我们中国的项目服务，所以通过专家活动，我也非常希望呼吁大家来加入我们，好 那我再打个广告，我是Zilence的市场运营和生态的负责人。

刚才我们的郭文通博士，也讲了一下我们做的一个产品，非常感谢大家的热情，现在郭文通博士还在外边被困住，非常感谢大家对限量数据库的支持，如果大家有什么想了解的，也可以来跟我一起去探讨，那我们快速开始今天。

因为今天的整个圆桌的话题，还是离不开我们现在最流行的，大模型和我们的数据，这两块可能因为很多专家都在提，在这个时代算力算法和数据，是最关键的几个要素，那我们先来谈谈，因为现在大模型之争是非常的火爆。

也是愈演愈烈，有业界观点认为开源大模型才是未来，那各位老师和专家怎么看待，目前整体竞争的一个格局，而开源又会在其中起到，什么样的重要的推动作用呢，那我们先从军平总开始，好的我先抛个砖吧。

因为现在确实我感觉像大家，不管是在美国还是中国，大家在进入了一个大量模型的时代，所以基本上每个公司，不管有没有必要，大家都为了秀肌肉，或者是真的有看到了这样的机会，都会去练自己的大模型。

那我觉得这是个好事，因为从这个OpenAI，从CHPT这里我们看到，模型给我们实际的这种环境，我们的业务 我们的应用，包括日常的生活，都带来了极大的一种便利性，那么所以这个需求市场都是真实存在的。

也是科技革命的一个引爆点，所以我们看到现在，即便它现在是一个春秋战国时期，那种感觉，但是仍然大家觉得很有价值，但是我们更加认为，开源一定是一个未来，因为大模型某种程度更像是，早年的操作系统。

最早的时候你可能写一个应用，那么就直接跑了，但是这个应用之间，很难有这些被共享被附用的东西，到后面为什么会有操作系统，就是它有一个common的组件，那么这个common的组件。

能够更好的去支持我们的各种应用，那同时这个操作系统早前它可能是避远的，是吧，然后慢慢的它也会走向开源，原因就是在开源的这个世界里面，大家可以更好的协同起来，可以更好的去支持，上面的应用层中间层。

刚刚我们也看到，它会分模型层中间层上面的应用层，一个开放的生态，显然是更有利于这个生态的发展，但是实际上我们从另外一个角度也看到，模型开源它有它的困难的地方，那它困难的地方，相对于传统的这种开源。

在于它比较难协同协作，我比如说你如果是一个，一个代码的这种开源，你的代码质量高和的低，一个有经验的程序员，他是可以看出来的，我可以判断你的外面的这些代码，到底是好还是坏，但是我一个。

比如说一个train好的一个模型，一个高质量的数据集，那在这个新的数据集，外部的这种数据集的这种fine tuning之下，它到底是变好还是变坏，其实这个是不确定的，就很多时候，这种开源的这种匿名性。

或者公开性 开放性，跟它模型本身它的一个，对数据的要求，对这种算力的要求的这个壁垒，其实它有点冲突或者矛盾的点，所以我觉得这个业界，现在没有看到说，每个模型社区非常非常活跃，大家都基本上都是。

比如说从拉玛的角度，是给它改过来 拿过来，然后衍生一个什么其他的模型，而没有看到说这个某一个社区，这个拉玛它可以，比如开放开源出来之后，其他的组织进来之后，让它在原有的主干上去迭代。

像我们传统的这种开源的这种方式，我觉得这种可能，逐步的我们要去解决它，这样的话才能充分地释放，开源的在模型这个事情上面的潜力，这是一些个人观点，OK 孟老师，好的 刚才那个杜老师抛砖，我这边再引个誉。

那个是这样，我这么理解的，就是首先刚才主持人那观点，就是说开源大模型是未来，我觉得这个我是赞同的，首先我先摆明自己的观点，然后从大模型这一块来看的话，我们又看到历史，惊人地相似的一幕就是什么呢。

就是碧源的老大哥，在那边吃肉，吃满汉全席，这个时候老二老三带着一帮老四，就把我们来做开源，把这桌子给掀了，我觉得这个是一个非常，很多历史上有相似的这一幕，很多人会说，喇嘛这一块开源之后。

不管喇嘛还是喇嘛还是羊驼，这开源之后，大家认为大模型的安卓的时代到来了，安卓时代到来了，但是我是这么认为，其实现在因为大模型这一块，还是有一些问题的，就是开源这一块有些问题，比如说喇嘛这一块。

它其实不管是羊驼还是羊驼的子子孙孙，其实它的商用性不是非常强，很多时候我们把喇嘛拿过来之后，我们是不可以去商用的，这存在一个问题，就是我们这个技术多先进，但是对它这个商用其实没有帮助。

我觉得这个是一个非常大的问题，比如说我一个扫二维码去付款，这其实是一个非常简单的一个东西，但是商用性非常强，我觉得从商用性的角度来讲，我觉得喇嘛这一块虽然是好用，喇嘛好用到一个程度。

就是说我用喇嘛非常爽，一直用一直爽，但是就是不能拿出去商用，所以这个会存在一个问题，现在能商用的一些开源的大模型，比如说什么Dolly这些，但是它的可用性，用的爽的程度又不如羊驼这一块。

所以我觉得因为也是大模型初始的阶段，所以我觉得从开源的角度来讲，我们只要继续给它一点时间，我相信开源在未来的整个技术的生态发展里面，肯定会有一个非常美好的未来，像前两位我们老师说的。

这个也是签模大战的这么一个时代，因为从我们看来，就我一家之言，我觉得现在大模型大概分成基础型的，大家叫通用型大模型，然后另一种叫领域型的大模型，大家现在开源的很多的一大部分，都还是说通用型的模型。

因为开源这块在通用型的模型上，我认为它是非常非常好的，就像之前咱们的很多老师讲的，第一因为大模型的算，它是一个算力密集型，数据密集型，或者说人工密集型的这么一个，大的投入的这么一个工作。

所以如果我们通用型的大模型能够开源，能够让我们的生态伙伴，或者我们的很多厂商，能够站在巨人的肩膀上，再去做进一步的领域的模型的工作，我觉得这个是未来的一个，很好的一个方向，那说到领域大模型的开源。

我觉得可能会遇到很多很多的问题，包括你这个领域里面的，你喂养它的数据，包括喂养的模型的情况，我觉得其实这个问题，未来可能还没走到，可能还大家在领域大模型开源这块，可能有一些比如说医疗等等，刚有一所尝试。

包括我们中科文革发布的亚裔模型，也是在媒体宣传，在舆情，在社会治理等等方面做了一些尝试，但是真的深入的应用，我们觉得还有很长的路要走，包括在开源这边，虽然我们也开源了，但是也希望大家都关注吧，多提意见。

好 谢谢大家，来 杜总，分享三个，我个人对于整个大模型开源的，一些粗浅的认知吧，第一我觉得，现在的这些并源的大模型，其实也是起源一些开源的技术，所以说在未来，我是相信依然会有两个很大的阵营。

就是并源阵营和开源阵营，并且开源阵营会随着它的开放，和更多的这些贡献者的加入，而变得生态非常地丰富，第二个认知就是，我觉得整个的开源和并源的演进方向，极有可能会变成，并源会面向更多的2C的领域。

因为它可以有比较直观的，一些通用的解决方案，或者是大模型对于C的体验非常友好，而这些开源的这些模型，基于一些内饰场景的数据的fine tune，包括一些面向2B的垂直应用，可能会有更多的繁荣的机会。

第三我觉得，最终其实现在目前更多的开源，可能还是一些底座的开源，或者是训练方法的一些开源，或者训练工具链的一些开源，但其实我认为最终整个大模型生态，真正核心的开源，只有走到数据层面的开源。

我想才能是整个从模型层面的真正的开源，最终其实大家觉得，训练方法也好或者算法也好，都是一些基础的东西，但更核心的其实是这些数据，我就做一个简单的，我自己的一个预测，首先刚刚我们大家都在讨论了。

为什么开源，我觉得开源已经是一个，现在是一个通用，应该说是一个norm，就是开发者的一个创新的一个模式，我觉得大家都在讲大模型，我觉得ChadGDP其实我觉得是属于一个，我自己认为是一个初始阶段。

就是说它打开了一个门，它打开了一扇窗户，打开了一个领域，但是通用型的大模型，它注定它解决不了太多的生产的实际的环境的问题，就像是我自己，我很喜欢用ChadGDP，但是我只能把它当成一个秘书来用。

但是就像今天刚才国文通来分享的一样，它可以睁着眼睛说瞎话，因为它没有那个数据，所以我个人认为，未来的大模型，应该说每一个企业，我不是一个懂命knowledge，应该说每一个企业，它可以都有自己的大模型。

它其实基于自己的企业，基于自己的服务，基于自己的产品，它建大模型是用来服务自己的客户，它可以极大的降低自己的运营成本，所以我觉得数据都不是问题，现在问题是什么，是工具，开源的工具，可信的工具。

能够让你去产生可信的结果的一个工具，包括了平台，为什么我们说，投资者这么青睐，投资者这么青睐，那些开源工具的厂商，AI工具厂商，因为未来的大模型，一定是以企业为单位的，这个是我的预测，这是我的认为的。

所以一定是有一个极大的蓝海，在等着我们所有开源厂商，再去一起来共建的，OK好谢谢，OK，为了解决大模型幻觉，可以用大模型加项量数据库的方式，来解决大模型的幻觉，或者说你私有领域知识。

或者说你私有领域知识，又打了波广告，很舒服，接在杨总后面说话很舒服，OK其实刚才杜总，包括很多嘉宾也提到过数据的问题，那我们就必须再聊聊数据，大家知道数据对于大模型，目前的训练是非常非常重要的。

而其他的开源的大模型，或者说其他的像这种，Infra这一层的工具，或者说未来的这些生态项目，更加是需要建立在，数据的领域之上的，那么大家认为，数据的开源对人工智能的发展，有哪些重要的意义。

然后又会在后面，大模型发展过程当中，催生出哪些机遇和挑战呢，来 军平总，对 这块，其实我早前第一个分享，就在讲数据对于，AI和模型的重要性，就某种程度而言，数据应该是低性的，比如说这个模型是在。

数据的一个投影，在一个平切面上的投影，然后你可以，你有高质量的数据，能产生很多的模型，但是反之不成立，当然你可以说，你足够多的模型往前回推，你可能能把全量的数据，能够推导出来，但是这会很难受 很辛苦。

所以数据的重要性，显然不言而喻，但是从另外一个层面上，数据去共享或者开源，也是一个很大的难点，因为现在很多的数据，隐私数据 敏感数据，它是人类活动的一个投影，或者是人类的业务。

或者企业它的业务的一个投影，所以这个时候，大家都天然对这部分，就是有很强的这种保护性，所以我觉得更多的是，未来我们看看，怎么样探索一个，就是说现在的私域数据，公寓数据中间，有没有一个，有一个灰度的空间。

这个灰度的空间，就是绝对敏感的，你比如还在私域里面，然后完全开放的，就是现在的这种，开放开源的，或者开放的数据集，那么其中还有一部分是，可以比如说，可以被交换，可以被这个租用，对吧这样的模式。

可以去推广，我就认为这个可能是，未来拓展，从拓展数据可用性上的，一个可能性和尝试，这是一个，第二个从另外一个层面上，从本身从开源的角度来说，就哪怕我们现在，能用到的这些开源的数据集，或者开放的。

其实本质上它是个，开放的数据集，因为很多时候它不能商用，或者是什么，各种各样的许可认条款，前段时间，反正一直以来，都有各种各样的，数据集的问题，从最早的Google的，ImageNet开始是吧，就不能。

大家都有很多的这种，商用条款的限制，那么我认为，从某种程度而言，就是我们要在这个数据，在开放数据这里面去探索，就是为什么，就是它的商用的边界，在什么地方，因为这个，如果不能用于商用，只能用于科研。

那实际上很多的这种，我们的这种技术的突破，尤其是这种技术，突破之后的应用，都会产生一定的问题，那么反过来我们会去想，为什么这些数据不能开源，开源到底有哪些风险，只能只是商业上的风险吗，还是这里面会有。

这个伦理上的风险，反过来我们要去反思，不是说大家因为稳妥，所以我不能商用，但是实际上，这种不能商用的一个，早期的一个简单的选择，其实会影响后面很多的事情，对我能想到的，大概是这些点吧，万老师。

刚才杜老师说的，其实还是蛮详细的，我这边简单地补充一下，就是数据要素的流通这一块，其实不必多说了，现在国家各个层面，都已经在鼓励，这个数据要素流通，这样的一些工作，那么其实，数据要素的流通过程中。

不管是内部还是外部，其实涉及到的问题，其实跟我们的开放源码，包括开放模型都不一样，因为它涉及到不仅仅是说，它这个风险程度也不一样，它不仅仅是涉及到，我把我的代码开放出去，那什么都OK了是吧。

它里面我选个许可证，不管是alpha还是GPL，选一个，那么这个数据这一块，我把它开出去之后，我一个贡献者和使用者，拿这个数据去用，其实这个里面，还是有非常大的一个风险的，我们都知道前段时间。

某个社交公司是吧，就被欧洲罚了个7个亿，欧元，其实很多公司在这方面，其实还是蛮注重，数据这个就是共享，或者是说流通这方面的，一些事情了，所以就像我们买房子，也是一个非常有风险的事情。

我们很少有人是买个房子，我自己跟房东就直接跑到，房产公司，那个房产局去成交去了，一般都会去找一个，专业的中间的一个机构，去成交这个房产，那么我们整个数据流通，不管它是开源的数据，还是闭源的。

那我们是不是未来，也要有这样的一个，第三方能够承担起，这样的一个规避风险的，这样的一个职能，这是我的一个思考，好 我也是非常非常赞同，刚才这位老师说的，这个数据的开源，其实面临的问题，不像代码开源。

或者框架开源，一样那么简单，这里面就是涉及到的问题，有很多，因为像现在说政府，也是在做这种数据的，各地方的大数据教育中心，教育所去做这方面的工作，实际上里面涉及到，像数据的确权，你的质量的管控审核。

是否合规，咱们的数据安全法里面，未来是不是把大模型的，这个数据，也会放到那个立法里面，这些都是未来要带解决的问题，所以可以吧，伴随着这个的解决，我觉得可能这个问题，会有所解决，未来也有可能。

咱们大模型数据的这个交易，放到教育所里面，我觉得也是一种方式，这是利益，所以从前面的各位老师，这个总结，我觉得未来的私有化的，专属的大模型，还是挺大的一个方向的，因为很多数据的问题，解决不了，它必须。

但是又想做，比如说企业或者政府内部，它需要大模型的助力，它需要去让自己转型升级，提高效率，降本增效，那怎么样，那只能说，做一股私有化大模型，好 谢谢，其实我觉得，从数据的角度来看，我觉得可能最后。

这个行业会分成，两种阵营的数据吧，就是一种还是，公寓可以获取的数据，或者是一些，偏开放性的一些数据，这一部分的数据，可以通过这种，开源数据集的方式，或者是一些第三方，这种平台的一些方式。

或者政府来去促成的一些方式，去实现它的一种交易或者交换，我认为这是这一类，然后另外一类呢，可能更关心到这些，企业内部的一些，核心的业务数据，或者是涉及到一些，真正的这些，个人用户的隐私数据。

这部分的数据，我是持着比较保守的态度，我觉得可能，很长的一段时间，它都很难做到这种，真正的交易或者是交流，这其实是，确实是这个，对于企业来讲，或者对于这些数据的，真正的这些。

Privacy的Owner来说，这是他们非常在意的东西，这是他们核心的价值，我觉得没有什么补充了，我觉得任何的，这种数据开源的讨论呢，我觉得都可以放到，在开源社区，为我们放到阳光下，一起来讨论。

我觉得是这样子，对，其实我本来还准备了，下一个问题，但是我觉得在大家，聊的过程当中，把这个话已经，聊得很全面，确实数据对于一个企业来讲，还是非常非常重要的一环，你让企业的私有数据，去开源这件事情。

就变得更加困难重重了，无论是从信息安全，包括提到的伦理的方面，道德方面，这些可能都会产生，非常大的问题，那我们也希望有更多的，借助于开源社区的这种，公开共享的这种方式，大家更多地去探讨，一个边界感。

然后找到一个，更合理的方式，建立一定的规则和标准，甚至是立法，因为伴随着大模型，我们对数据的确权和隐私，肯定会更加地关注，从而想办法去激励，鼓励大家，把这个东西能拿出来，然后贡献给我们的社会。

让更多的人去受益，这个跟我们开源的，无论是代码还是框架，这块的理念是不谋而合的，但是在这个过程当中，我们现在其实是有解决方法，就比如说在私有数据，把数据放到相关数据库里面，是不是。

这个方式我觉得还是OK的，就可能在这个过程当中，我们需要一个，这样的一个方式，中间方式去过渡，帮助我们去找到最终的，一条非常好的一个道路，OK，那我们在聊开源的项目，聊到最后都要去聊到。

一个商业化的话题，在这个过程当中，最容易引起争议的，其实就是开源续货账，这个是我们经常会有一些热点事件，在这块也是最容易翻车的一件事情，其实刚才有专家提到过，Firecoin的新的项目。

它在Hacking Face上面，然后去开源了，然后也号称是史上最强的大模型，它参数虽然没有拉马那么大，但是说性能效果都比它要好很多，然后它开源了，这是一个很振奋人心的消息，但是它有一个事情。

让它骂上了热搜，就是说它魔改了2。0，你要仔细读，它又遵循了，它又没遵循，在你商业化的时候，它又给你加了一些小tricks在里边，小新词在里边，所以它迅速地就被骂上了热搜，然后说它不是纯纯的开源精神。

违背了开源精神，甚至是褻读了阿帕奇基金会，然后褻读了阿帕奇基金会多年的一个品牌，也造成了非常大的一个争议，面对着许可证这样在开源圈，经常翻车的这样一个重灾区，各位专家站在大模型的时间点。

是怎么看待这件事情的，对未来的我们做开源大模型这块的，同道中人有哪些建议呢，来 杜老师，好的 其实还是回到那个问题，就是模型的开源和数据的开源，跟传统的代码开源还是不太一样的。

那么代码的开源我们传统上可能会更加开放，包括我的各项权利都是比较简单的，你有没有权利去再分发，有没有权利去再修改，包括你的其他的一些权利相对是比较简单，但是像模型，像尤其是数据级。

它能够 这个数据被篡改之后还能不能再分发，其实它有的时候不是一个简单的，是一个个人行为或者一个商业行为，它后面其实有各种各样的影响的，它会给它的原创者或者原来的贡献者，其实带来很多的这种风险。

所以也不能简单的说，这些加上了许可证，加上限制的这些条款，它的初衷一定是坏的，我觉得可能也不一定，首先它至少比没有开放出来的这些模型也好，或者数据级也好会更加的开放，我觉得这首先是一个要肯定它好的一面。

第二点它在这个领域，确实它也做了一些一定程度的创新，比如说它说它有个约定，当你的收入在多少多少之前，你是免费，你就跟Apache v2差不多，但是到了某一个商业的结算点的时候。

好像要跟它有一些利益的分成，我觉得这个也是蛮有意思的一个想法，我认为这个事情不是坏事，因为好像他们最后又改回来了，我看那个最新的进展，好像又改回到这个比较纯粹的Apache 2。0。

但是我觉得在这个开源和商业化，尤其在模型和数据这种新情况新问题之上，我觉得有效的，就适当的这样的探索，这个探索在于，一个是探索大家的商业的边界，一个是探索大家心理上，对于开源的道德或者伦理的边界。

到底在什么地方，我认为都是一个比较有意思的话题，我觉得大家也别一下一棍子打死，或者一棍子拍下去，因为在完全开放和完全封闭之间，其实还是有一些灰度的地带，而且灰度的地带也是有价值的。

好 这是我的观点 谢谢，好 下面我说一下我对这件事的一个看法，前段时间也是这个事情闹得沸沸扬扬，其实网上在开源圈子里面，骂的人还是非常多，因为从我的了解，因为我也做开源出身。

很多开源人的脑子里面都有一种乌托邦，那种理想主义的这样的一个理念在里面，但是我尝试从另外一个角度去想这个问题，我们在想这个企业为什么要开源，企业的本质是要干嘛的，企业的本质是要生存的。

这个其实是一个很现实的问题，尤其是疫情过后这几年，其实大家日子都不太好过，企业的钱也不是大风刮来的，投资人钱也不是大风刮来的，你肯定要跟企业说明，你开源这个东西对企业的一个回报是什么。

那么Folkcom它就采取了一种，简单粗暴的这种方式，相对来讲就简单粗暴，直接你用我这个东西OK没问题，那达到了一定的规模之后，你就得给我交份子钱，交买入费，这个确实简单粗暴了一点，但是怎么讲。

其实我从企业这个角度来讲，我倒是蛮能理解企业它当时，在乌托邦理想的状态，和它企业生存状态的这种情况下，在挣扎过，它做了这么样一个妥协，然后结果扔出去之后，还是被网上一片骂，所以在这里我也po一个问题。

可能也值得大家思考一下，就是说我们做开源这个事情，当然我们是一个理想主义者，对吧，那么它和企业的生存之间，它怎么产生一个平衡，我觉得这样，这也是一个，也一直是困扰我，我估计大家也是。

经常能够想到的一个问题，今天反正借这个机会，也再一次po给大家吧，大家也可以和我们一起思考一下，这个事其实我也是听说，我还没有那么详细的了解，我就觉得，可能是在现在的环境下，针对模型的开源。

也对我们的开源组织，提出了一个新的课题跟问题，大家需去解决这个问题，尤其是企业，刚才这位老师，企业为什么开源，企业除了要品牌 要盈利，其实咱们这些开源的组织，或者说我们整个社区，能够让企业鼓励它。

愿意去做这件事情，希望跟大家探讨吧，好 谢谢，我对这个事看，我觉得其实Funkone，其实设了一个很好的里程碑，我觉得在此之后，应该就再也没有这种状态的，license的出现了，因为它本身是破坏了这种。

开源的最本质的那一套，秩序和规则，然后也是承受了很大很大的压力，最后改回去了，整个事件我都从头到现在，还在follow他们，他们现在是特别的拥堵，开源和开放的生态，所以我觉得还是发自本心吧。

如果要是做开源，那就是真正的全打开，如果要是做商业化，那就是去专心地做商业化，这边我觉得是可以存在，这种共存的生态的，但绝对不是这种，通过改license的方式，来去做这些取巧的事情。

这个问题其实杨总最适合回答，来来来，我总觉得，坦白说，我总觉得这个，license是人家的选择，他有这个权利，但是呢，就是不要产生这个误会，就是让大家觉得你是要趴7 2。0。

因为大家可能做了一个assumption，一个前提假设，你是这个开源的模式，你是应该是免费，但我希望就是说，我们所有使用开源的人，我们用开源的人，不要做这种假设，因为那是人家的权利，人家的心血。

他就刚刚我们说，他用来生成这个，用来去赚钱，这个我觉得是，是对他投资人的一个尊重，是天经地义的，但是呢就说，就是像杜总刚刚说的，就说如果你是立心要开源，有的东西我们事先要做清清楚楚的。

不要给人家一个模糊的概念，这个我觉得非常重要，你可以自己另立一个license，你可以不叫apache，你可以叫一个自己的，这个没问题，在开源领域这是很正常的，你可以自己做一个，而避免了这类事情。

大概就不会有人骂你了，我心是支持，开源不是做公益，开源是一种商业模式，就是没有人说你做开源，就一定要免费的，这个是这样子的，然后我觉得杨总最后一句话说的特别好，就是开源并不是代表着，做公益去免费。

其实像Zilis这种，就是Day1去开源的公司，我们也面临到，从商业化的开始，其实我们也是apache，V2这样子，其实也在这个商业化过程当中看到，市面上有一些人拿你这个东西，自然而然的就和你去竞争。

其实你看到之后你是会有点，就会回想我当时做这个决定，到底是不是错的，但既然我们前期吃了开源的红利，那我们就会坚持的走下去，然后帮助社区里面的更多人去受益，这个也去遵循我们自己的一个，开源的一个初心吧。

还是把这个事情去做好，但是面临到商业化的平衡，可能我们唯独就是，只有一个办法，就是做到商业化产品比开源更好，然后给我们的企业用户提供，更加有价值的服务，提供更加企业化的一些服务，这样的话可能才能做出。

一方面不辜负社区，一方面又不辜负，信任我们付钱的客户，对 其实这个中间的点，还是挺难平衡的，我其实不太喜欢，网上那些键盘侠，抓住一个东西，你原来免费，你现在为什么不免费，或者你还是有条件的免费。

你干嘛不直接就这样，其实这群人往往可能，他们没有真正做过开源，也没有自己去run一个开源项目，甚至是在这上面，去付出了更多的精力，他不知道那些人的艰辛，是怎么样子的，就怎么样去维持这样一个。

大的一个系统，去很好的去运转，但是其实开源的初心，一定是利他的，我觉得一定是利他的，但是后来由于这样那样的方式，所以我特别同意，开源是一种商业模式，大家保持这种好的心态就好了，OK，其实时间也差不多了。

然后我们今天可能话题也比较多，也没有办法很透彻的去聊完，然后我们这也是一个，最后一个环节，其实两天的志愿大会，非常的精彩，也非常欣慰的是，今年的线下活动越来越多了，其实对于我们做技术人来讲的话。

能坐在一起能够去聊天，能够去沟通，是一件非常幸福的事情，然后这场盛宴也马上就要落幕了，非常感谢还能坐在这里的，一些朋友们，然后那就请各位专家，最后用几句话的时间，来总结一下你们这两天，参会的一些经验。

然后包括未来的一些感悟吧，这两天志愿的会反正还是，我觉得规格还是很高的，基本上应该算是，国内乃至全球这个范围，大家人工智能这个领域，做的最先进的公司也好，或者是科研机构，其实大家都到齐了。

我觉得是一个很好的，一个技术的盛宴，或者是一个大的一个，这个技术的一个派对，我觉得蛮好的，就是这个完美的印证了，其实我们现在，整个业界的这个趋势热点，确实在往AI模型这个领域，在推动。

我觉得随之而来的是，我很期待在未来的几年里，看到这个我们，一直所说的这个起点，到底是怎么发生的，那么未来当我们，进一步变老的时候，我们回过头来再看一看，会想到这个起点之前的，那个时代，跟我们之后的时代。

好像完全不一样，有这么一条明显的界线，我认为这个明显的界线，可能就是在今年和明年，这么一个点，我觉得还是我们非常幸运的，去见证这样一个时刻，谢谢，我们一同期待吧，来 孟老师，我是今天下午。

才参加这个论坛，但是我参加这个论坛，论坛之后呢，刚开始听一两个之后，我就有点后悔了，我应该提前两天过来，来参加这个会，当然这个时光也不能倒回，只能后面看看那个视频了，有那个视频的录像吗，有，那行。

我现在心里面舒服多了，最后总结一下吧，就是，这个我今天下午，虽然只参加了半天，就是给我的感觉就是耳目一新，刷新了我的认知，谢谢大家，我是觉得这个，办的这个会，真是一个头脑的盛宴，让人感觉受益匪浅。

然后也预祝未来，咱们这研院的这个会，越办越好，然后我们一同为人类AI这个方向，做一些推动跟贡献吧，我是昨天晚上赶回了北京，然后幸运的是，我昨天在直播，我倒看了很多场，确实是这次的规格，真的是特别的高。

而且受到了很多启发，最后这个我觉得，再点题一下我们的开源吧，我还是相信，就是通过开源的方式，一个是两点，一个是汇集所有人的力量，让每一个人都能够有机会，参与到这一次变革中。

然后第二就是它的这种transparency，就是它透明性，让每个人可以在这个，整个开源的开放的这种体系内，去贡献，并且让别人能看到你的贡献，保证了整个的这种可控和安全，我相信这个整个的开源。

会成为未来这个在大外宣领域里面，甚至整个人工智能的进程，或者AI的进程里面，一个绝对会爆发的一股大力量，我们一起期待，我觉得大家看到了这个，XGDP的这个表象，就是AI人工智能突然之间。

成了全球最炙手可热的科技，很多说未来XGDP这一类，将会成为我们的这个操作，桌面的操作系统，门户类，但是呢我另外，我其实想让大家呢，更多的去关注的就是这个，AI的这个底层技术，在开源社区。

我们有很多非常好的这个开源项目，跟开源的这个底层技术，我希望有更多的人能够，加入到这个开源的行业，加入到我们这个，开源的这个软件的生态，只有掌握了底层的技术，今天我们其实开会。

大家听到了很多这个开源项目，包括华为的这个，例如Mineswap，Zilis，Mufis，这一系列的，今天大家听到了很多，其实这个才是，我觉得这个才是我们，中国开源能够强大的根本，这个我希望大家。

我在这里就希望大家能够，更关注的就是我们这些，开源项目，开源的技术，好谢谢，好 开发者 蒋泽 中国，大家都处在，AI2。0时代的这个，跨时代的节点上面，希望大家积极参与进来，那参与AI和数据开源。

最好的方式就是开源，希望大家多多支持，以后FAI的活动，多多支持智源的活动，多多支持我们这样，AI Infra的公司，好 谢谢大家，今天顺利结束了，谢谢大家，谢谢还留在现场的，各位朋友们。

看到你们的笑脸，真的好开心，谢谢，(字幕製作/時間軸:秋月AutumnMoon)。