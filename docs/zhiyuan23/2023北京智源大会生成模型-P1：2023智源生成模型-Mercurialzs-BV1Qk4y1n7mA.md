# [2023北京智源大会]生成模型 - P1：[2023智源]生成模型 - Mercurialzs - BV1Qk4y1n7mA

[音乐]，同学们大家好，欢迎大家来到2023年北京志愿大会的生成模型论坛，然后我是论坛的主席和主持人，我叫李重轩，然后非常高兴非常荣幸有这个机会，组织这样一个活动，在志愿的支持下。

去和大家一起分享生成模型最新的这种进展，然后今天我们是非常非常荣幸，邀请到了斯坦福大学，斯德弗诺·沃尔曼教授，浙江大学，赵州教授，志愿研究院，刘广研究员，UCLA周博磊教授，Stanford吴嘉俊教授。

给大家带来生成模型的前沿进展，然后最后我们会有一个非常简短的，一个大概半小时的一个原著论坛，然后还会邀请到清华大学朱军教授，和各位讲者一起，来跟大家做一个更详细的讨论，然后我们今天的第一个报告来自于。

斯德弗诺·沃尔曼教授，报告的题目是，Recent Advances in Scalable Diffusion Models，我会先介绍斯德弗诺·沃尔曼教授，沃尔曼教授是斯德弗诺大学的，协会教授。

他的研究专注于模型学习和AI，他获得了多次的顶级赞助赞，还有在上海访谈会上的伟大赞助赞助，如Triple AI和IKEA，他非常有名，因为他研究了，Scalable Diffusion Models。

所以现在是时候，让我们欢迎阿尔曼教授，来举行讲座，阿尔曼教授可以分享一下屏幕，谢谢，可以看到屏幕吗？，可以，很好，我能听到你吗？，可以，很好，谢谢，谢谢你介绍我，是我的荣幸，能在远处与大家分享。

这是我与我的前博士生，杨松的合作作业，他是深化大学的学生，他将是Caltech的，系统教授和数学学院教授，所以，好的，好，我将谈谈，我们在过去几年，在级别基础的Diffusion Models中的工作。

如你所知，在Generative AI中，有很多进步，有很多讨论，关于语言模式，或者语言模式的生成，但也有很多进步，在其他模式中的生成模式，如说，说话、音乐、图片，很多进步，在这些模式中。

都由Diffusion Models所引导，这是一个例子，你能够得到的结果，使用现代艺术，生成图片模式，这是一个很好的例子，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式，我会用这个模式去生成图片模式。

我们之前是ENCODE后面是LENS REGULATOR，我们可以看下这是VARIANCE ADAPTER，除了我们做我们的LENS也就是DURATION的预测，也是做了我们后面的音高能量等等。

其他的一些属性的预测，把DURATION PREDICTOR扩展到我们的VARIANCE ADAPTER之后，我们发现可以取得两个效果，第一个是它的Quality提升了，Quality提升非常多。

我们可以看下它的VERSION 2的版本，VERSION 2的版本是比它的VERSION 1的版本有一个很大的提升，甚至它的在评测的时候，已经比TRANSFORMER TDS的Quality甚至要好。

那么依然是在INFERENCE SPEED的时候，我们可以看一下，它INFERENCE SPEED是如果只用AUTO REGRESSIVE的解码的话，它使得是-1次方的INFERENCE SPEED。

那么合成1秒的音频是需要10^-1级别，但是在AUTO REGRESSIVE的话，从10^-1降到了10^-3，那么做了一个推理的提升，那么这个还是会存在一个问题，所以会继续再挖掘一下这个问题。

因为我们是发现，很多的时候我们是需要这个模型的SIZE尽可能小，那保持它的推理的速度和我们的推理出来的效果的时候，我们还是希望进一步压缩它的SIZE，所以有PORTER SPEECH这个工作。

PORTER是可携带的，我们希望是可能继续的小，那么我们看见我们这儿有两种的GENERATION MODEL，第一个是VARIATION GENERATION MODEL，我们通过实验发现。

如果用VARIATION GENERATION MODEL，它虽然参数不是特别大，而且可以CAPTURE它的整个的合成的一些韵理，但是由于它的LOSS的原因，它是带了一些的模糊性。

那么第二个是FLOW-BASED，可逆流BASED的MODEL，那么虽然我们在参数足够的情况下，我们可以把它的效果做得比较逼真，但是它需要很多的参数，那么怎么办。

我们把VA-BASED和FLOW-BASED加起来，CASCADE起来，CASCADE起来就是PORTER SPEECH，那PORTER SPEECH做到一个什么样的程度呢。

我们可以看一下PORTER SPEECH，做了出两个实验，两个版本，一个是NORMAL版本，一个是SMALL版本，NORMAL版本的时候，VA和GLOW模型加起来。

那么我们可以再看一下它的Quality在上面，是比STEPHART的Quality更好，那么第二个事情，我们是把它的SIZE进行压缩的情况下，我们可以看一下，它只用了大概四分之一的参数量。

依然可以达到非常comparable的performance，所以在这个程度上，做了一个模型的参数的压缩，那么使得更小的参数，可以取得非常comparable的performance，那么回归到中文。

因为我们做文本到语音的合成，往往会涉及到中文，那么中文有一个非常大的问题，大家都还没有去解决，就是说我们中文和英文来说的话，它的一个词汇，往往一个词汇在不同的语境的情况下，它发音是不一样的情况。

所以说我们在做我们的MELPENPRO的，一个映射的时候，从语音的空间到声学的空间，进行映射的时候，我们不仅是需要看当下的一个词，也是需要看它的context，也就是context，那么做这个的情况下。

怎么进行context的一个过程，那么我们当时我们是可以看一下，就是比如说它是输入了一个文本，输入文本，我们可以对于文本来说，我们可以看它的字典，因为我们可以看一下，听乐队，那么乐的话有几个发音。

一个是快乐乐观取乐，那么是第一种乐的一个pronunciation，它转成它的音数，第二个是音乐乐曲和音乐，那么pronunciation是第二种pronunciation，那么我们借用我们中文的字典。

中文字典可以把它的音数，给尽可能的map过来，尽可能的map过来，那么使得我们的dictionary放进去，那么是解决中文的问题，那最后一个，最后一个在这个事情是，我们是希望可以把它进行一些泛化。

因为现在大语言模型也是非常popular了，也做了一些基于我们的speech，转换为token，那种语言模型来做，但是我们发现，就是说不是说所有的speech，所有的模块都可以转换为离散token来做。

那么什么比较可以做呢，我们发现是从韵律的code，做我们的离散的token的一个language model，会比较好的一个性能，所以泛化的情况下，我们可以看一下这里是。

这个是Mail Spectrum的一个结果，我们只在韵律上面做了一个language model，那么和reference的是音质，那么合成我们的一个target的一个形式，那么我们可以试一下。

我们看一下他的demo，我们可以看一下，这个是一个奥巴马的声音，对 它是一个十秒钟的一个prompt，(英文)，OK 那我们可以试一下另外一个prompt，从Wordcraft里面，这个是比较有意思的。

(英文)，对 从Wordcraft里面来进行输出，作为一个prompt，是两秒钟，(英文)，对 我们就是通过它的jail，那么使得它的原生作为prompt，那么生成它的个性化的prompt。

personalized TTS，那么这个是natspeech，那么为什么是natspeech，我们依然是沿用了之前的pod speech。

之前pod speech的duration predictor，包括length predictor，因为它的duration这个层面的话。

其实是我们觉得是可以从language model里面进行jail的，所以中间这个backbone，依然用了我们之前的natspeech的一个backbone，那么这个是我们的第一个工作。

第二个我们是介绍一下，歌声sing的一个模型，那sing的模型的话是非常有意思，就是得益于diffusion model，sex to diffusion model。

那么diffusion modelapply到上面去，可以做什么呢，可以做一些非常高表现力的一个合成的一些工作，合成工作，我们首先看一下，diffusion model。

那么左边是diffusion model，右边是我们的application，当然是可以直接apply我们diffusion model，to our sing voice synthesis。

那sing voice synthesis的话，我们发现有什么样的一个interesting的idea呢，因为第一种我们是可以从我们的conditional来进行生成，noise来进行生成。

那么第二种的话，我们发现，沿用pod speech的一个思想，就是两个model进行cascade，那么第一个model是pod speech，第一个model是capture semantic。

第二个是capture它的一个音质，那么这也是一样的，那我们m manifold m是什么，m是我们的原始的original data，我们进行一系列的加造，我们加到t的step。

那么ampere是什么，ampere是我们用另外一个model，另外一个之前的model，我们生成出来的一个频谱，那么之前的model的话，是用我们的fast speech2和pod speech。

分别是生成了不同的一个频谱，我们进行加造，我们加到到这的时候，我们发现它都是converge到一个white noise，那么比较interesting的程度，就是我们发现在第七步的时候。

第k步的时候有一个overlap，那么也就是说我们可以换一种思路，就是说一种是我们用one single model去做这样的事情，第二个是我们是用两个model，two models。

我们第一个是用一个辅助model，辅助decoder辅助model，辅助model我们生成ampere，ampere的时候，我们可以用我们之前的net speech，我们capture它的语音的信息。

生成一个初率度的一个频谱，初率度频谱之后，进行k步的一次性加造和k步的降噪，那么我们生成我们的music，生成我们的music的歌曲，那么这个是这个performance。

那么我们发现一个比较interesting的事情，第一个是这个策略的事情，这个第一个策略，我们是可以把t步的一个降噪，t步降噪给它reduce成k步，第二个我们发现两个model。

沿用我之前的portal speech的一种思想，它的quality会比single model会更高，single model会更高，所以是一个cost to find的一个过程。

那么我们可以听一下demo，这个是一个我们放的一个，你说你不懂为何在这时牵手，笑就我长睫毛，是你最美的记号，它是可以是合成非常的比较表现力的，因为这个task比之前的speech要更难。

因为我们speech合成的语音来说，我们的音高抖动等等都没有这么一个表现力，那么接下来我们也是做了一个M4 Signal，M4 Signal是什么呢，M4 Signal是一个benchmark。

我们把我们之前的diff signal扩展到不同的一些application，那我们可以看一下这个合成我们刚才展示过了，还有什么呢，还有是一个变调，我们可以听一下它的一个原始的音频，暖暖就在胸膛。

我想说其实你很好，那么我们是通过之前的gail我们修改我们的音高，暖暖就在胸膛，我想说其实你很好，那第二个是我们又是给它进行降调，暖暖就在胸膛，我想说其实你很好。

对这个是一个我们可以听下一个比较有意思的一个事情，只是那种温柔，再也找不到拥抱的理由，声调，只是那种温柔，再也找不到拥抱的理由，包括是克隆用diff signal，亲吻着我额头。

这是目标人的一个歌声的一个音色，这是我们那个是目标音频，这个是我还不肯相信，我还不肯相信，对，这个是一个克隆，包括是合成变调等等，那么除这个之外呢，对于我们之前的基于我们的声调和降调之后呢。

我们还是可以把有些的一些声调的没有没有上去的，我们可以把它进行一些自动的一些修正，那么是做了一个衍生成歌声的一个美化，我们可以看一下中文和英文我们的美化前，世界比你想象中朦胧，那么我们对他的音调进行。

世界比你想象中朦胧，这是英文歌曲，We're beautiful like diamonds in the sky，We're beautiful like diamonds in the sky。

那么除这个之外呢，之前所有的之前的工作是比如基于那种细腻度的一些乐谱，也就是说我们是基于音速级别的一个音高的一个输进去的一个文件，那么对于我们的乐谱网上的乐谱能否直接是放一个乐谱来做这个事情。

这个事实际上是建模了一个词级别的乐谱的一个性能，从直接的realistic的乐谱来过，我们可以听一下他的一个performance，光阴如锁一锁才去一锁之，青丝白砖丝丝缠乱又不知，对除了这个之外呢。

我们还有一个非常好的一个基于之前的好玩的东西，就是说我们是不仅是可以从我们的speech到声音的一个转化，我们可以speech直接是转化为歌声，我们可以听一下他的一个歌曲，这个是一个讲话的声音。

It's so very cold outside，我们可以看一下他的alignment，It's so very cold outside，从我们的一个speech到声音的一个映射。

那么因为之前的Deepthinner是开源的，那么我们可以看一下有不同的一些其他人用开源的做出来的一个效果，那么这个是一个b站，我们可以搜索一个b站的Deepthinner的一个创作。

那我们可以看一下放一段我们的b站的一个第三方的一个用工具来做出来一个效果，这个是有问题，所以我们下期再会，拜拜，拜拜，拜拜，拜拜，拜拜，(音量調整中。。。)，(音量調整中。。。)。

這個是第三方的歌唱一個完整的歌曲，(音量調整中。。。)，(音量調整中。。。)，(音量調整中。。。)，(音量調整中。。。)，對，其他有很多的case，直接進入B站搜索，(音量調整中。。。)，對。

其他有很多的case，可以搜索進入B站，點擊keyword Deepthinner，這個是一個搜索頁面，大家可以try一下，有不同的第三方用Deepthinner的工具做出來的工作，我們可以看一下。

上面是一個FineGrain的樂譜，基於這個來進行一些合成，整首歌是完整的一些合成，大約是四分鐘的合成現象，(音量調整中。。。)，也有更多的一些例子，因為我這次demo的例子，並不是排名最高的例子。

我們可以看一下，排名最高的都有非常多的瀏覽量，是今年的，突然大家對這個是非常感興趣，非常感興趣，有非常強的一個瀏覽量，用Deepthinner的開源的model來製作這麼一個歌曲。

放到B站上就會有非常強的訪問量和觀看量，(音量調整中。。。)，我們可以感受一下，現在目前的生成式模型，在我們的歌聲做出的一個效果的程度，之前是在Deepthinner上面可以。

在B站上面可以有更多的Deepthinner的一些demo，接下來我們再把生成式的模型從語音擴展到，比較有表現力的歌聲，再擴展到更難的音頻，音頻的話對於歌聲來說，它不僅有表現力，它也有非常強的開放能力。

在我們的歌聲上面，我們可以看一下，歌聲的audio上面，它是有更強的開放語，所以我們是從第一個開放語來做，開放語做的話，首先是介紹一下第一個工作，也是基於Diffusion的一個application。

叫做Make an Audio，Thanks to the Diffusion model，我們可以從text上面，給我們的音頻來進行配音，我們可以從圖片給我們的圖片來配音，包括視頻也可以給視頻來配音。

包括是音頻的一些修復也可以來進行配音，雖然我畫的是文本，其實我們support四種模式的音頻產生，第二個是對於我們的text來說，我們依然是做得不夠完美，因為對於我們的短的一些text來說。

我們是生成一個音頻，但是對於音頻來說，它可以類比於我們的video，Video的話有一個非常大的問題，它是有temporal的訊息，所以為了考慮這個temporal訊息。

我們有一個Make an Audio Tool，通過我們可控輸入文本的一個訊息，比如說我們先輸出一個鳥叫，再輸出一個卡車聲音，再輸出什麼，它有一個order的訊息，order的一個建模。

在進行Make an Audio Tool，Make an Audio Tool是Make an Audio的一個升級版，在Make an Audio的時候，已經可以支持我們不同的一個模式的過程。

所以在這個上面做了Make an Audio Voice，Make an Audio Voice除了我們把Make an Audio之後的文本，包括其他的進行理算化之外。

我們通過理算化和音頻表徵的雙重的一個結合，那麼先map到semantic model，再map到acoustic representation再進行合成，不僅是結合，也是進行理算化，在理算化之後。

第三個就是做audio GPT，因為audio的話，我們無論是輸入speech，無論是輸入text，有不同的一些任務，不同的一些任務，有不同的一些response，可以是讓它產生audio，可以是唱歌。

可以是做speech translation，可以是做speech to talking face的synthesis，那麼不同的一些任務，不同的一些任務，不同的一些任務好了以後。

這個是audio GPT 1，audio GPT 2是使得audio GPT 1再更進一步，audio GPT 1的話，主要是基於chat GPT。

來這個foundation model進行構建不同的生成式的模型，那麼audio GPT 2是做了一個unified language model。

可以支撐不同的模態到不同的模態的translation，translation和synthesis的一個過程，那麼我們首先看一下這個Makenaudio。

Makenaudio的話是一個在我們的音頻生成裡面的一個第三個賽道，它因為是有著非常廣闊的開放域的這麼一個事情，它可以用到比如說有聲讀書，配聲等等等等，都可以有非常強的一些用處。

那麼得益於我們現在比較好的大的語言模型，一個是clip，一個是clap，那麼clip不陌生對我們的視覺和語音進行編碼，那麼clap的話是對於我們的音頻和文本之間的一個編碼來進行指導。

那麼包括是再利用了latent diffusion的model，我們創了一個classify free guided model，那麼對於我們的音頻來說，最大一個問題在於我們對於音頻來說。

我們需要非常非常強的一些data，以至於我們可以生成的音頻更加有開放性，但是在我們的website來說，我們並沒有這麼強的一些data，這麼多的一些data，那怎麼辦？。

在Makenaudio 1的時候，我們是做了一個基於pseudo prompt enhancement，那麼這裡是design了非常多的一些rule，非常多的rule。

那麼對於我們的audio和text來說，可以進行不斷的一些拼接，比如說這裡是鳥的叫聲，這裡是腳步的一些聲音，那麼鳥的叫聲和腳步的一些聲音，可以通過不同的一些拼接，不同的一些拼接可以把它組合成。

更多的一些音頻用來它的訓練，那麼對於data來說的話，我們是做了一個pseudo prompt enhancement，那麼通過拼接的形式產生更多的一些data。

那麼最終來train這個model的時候，是用了3000個小時和100萬個audio taxpayers，來做這麼一個model的訓練，那我們可以看一下它的Makenaudio 1。

Makenaudio 1它的輸入prompt相對來說比較簡單，我們可以看一下它的一個打雷和下雨，這個prompt輸進去，能不能幫忙放一下，對，對 當然我們可以支持打雷和下雨，以文本的prompt輸進去。

第二個是我們是支持圖片，那麼，對 這個是一個開車的聲音，那麼除了這個文本之外，那麼也有圖片的一個prompt，請點擊一下圖片的一個prompt，聽說話，請點擊一下這個視頻和圖片的一個。

對 這個是右上角的圖片生成的audio，那麼以圖片作為prompt生成了audio，那麼下面的話是我們的video，那麼點擊一下這個是我們的煙火，作為一個short video。

對 這個是支持它的一個video，short video作為prompt來產生這種audio，那麼音頻就沒有放在這，那麼支持不同的modality，但是它這個還有一個問題，我們可以看一下。

它的prompt現在是輸進去，它的關係來說的話，它都是一個，基本上都是一個並列的一個關係，而且它的prompt的話，我支持的一些prompt不是特別的複雜，就不是特別複雜，但是如果我們是希望能夠生成。

更加複雜的一個，生成更加複雜的audio，也就是說可以support，更加複雜的一個prompt，所以有一個MAKER AUDIO 2的一個版本，那MAKER AUDIO 2的版本。

基本上是沿用了MAKER AUDIO 1，但是呢，在對於我們的增強的數據增強的時候呢，並沒有基於規則的形式，而是用了現在的大語言模型，那麼進行大語言模型的時候，大語言模型來進行增強，第一個。

第二個是對於我們的，我們的一個，我們可以看一下，這個是man speak，首先，然後是狗叫，然後是，那麼這個生成是，man speaking，然後狗背後有鳥在背後，那麼這個是，對我們的prompt來說。

比之前的要複雜的多，複雜的多，這個是MAKER AUDIO 2的一個prompt，那麼總的來說是，用了3。7K的hours，3。7K的hours的數據，那麼performance我們不看了。

我們看一下幾個量力，那麼我們可以看一下，這個是一個，(音樂)，(音樂)，那麼這個是一個比MAKER AUDIO 1，更加complicated的一個prompt，來進行支撐，那麼這裡也可以看一下。

這個是vehicle engine，那麼idle，(英文)，(英文)，它是對於我們MAKER AUDIO 1的一個增強，那麼這裡面是用了很多的一些，理解及技巧，或者是理解順序。

它的時間發生的事件的先後的順序，先後的順序，那麼來更好的一個來做這個事情，那後面的話是做了一些，更加general的事情，就是說我們是希望，把我們的model，可以做更加通用化的一些模型。

那麼不僅是我們的text，包括我們的speech，進入semantic token和acoustic token，進行一個非常好的一個結合，那麼跟我們的public speech idea一樣。

semantic token我們是capture，我們之前的語意的信息，那麼這個是音頻的一個信息，那麼跟我們的meta TDS，和我們的public speech一樣，做語意方面的結合。

和音頻方面的結合，而控制的話，基本就是加在我們的音頻的，acoustic condition，acoustic condition，然後semantic meaning的話是。

是固定的一個結合的一個方式，那我們可以看一下它的一個，(英文)，support不同的一些task，同時support，Zero-Short的text to speech。

包括Zero-Short voice conversion，包括Zero-Short sing voice，我們可以先聽一下歌聲，(英文)，這個是輸進去的一個prompt，這個是prompt。

這個是generator，(英文)，這是一個Zero-Short voice conversion，這個是sauce，這個是sauce，(英文)，這個是一個make a voice的一個版本。

那麼最後我們是介紹一下，後續的工作是audio GPT，audio GPT的一個工作是，把我們之前的工作進行集成過去，它是可以support不同的一些task，就像我們這兒可以看一下。

我們是從audio to text，audio to audio，audio to event，audio to video，以及是text to audio，包括audio to text。

image to audio，以及是music score to audio的，一些不同的一些工作，它是一個capability的一個能力，那麼這兒的話我們可以，放一個example。

來show一下它的一些能力，請播放一下這個example，(音樂)，這個是generator一個music，這個是generator audio，那麼這個是一個write caption。

包括一個audio，(音樂)，這是我們剛才看了一個demo，它是一個從我們的text to speech的，語音合成模型，跨越到更加泛化和更加通用的model，那麼在一個。

其實speech對話場景的情況下，我們可以讓它可以做，不同的一些task，那麼這些我們的工作呢，也是放在了GitHub，大家也可以在HackingFace，try一下，那麼這裡也是有demo page。

那麼後面的話，後面的話我們是在，現在還沒放出來是，AudioGPT2，AudioGPT2，那麼我們在編碼的時候，我們用了自己的一個，一個離三化編碼的一個框架，和unify的language model。

那麼對我們AudioGPT1，進行一系列的一個升級的工作，那最後總結一下，我們今天主要是，給大家分享一下，我們這幾年的一個，在音頻合成的，一個幾個系列的工作，那麼主要是三個系列，解決三個問題。

第一個系列是，主要是做語音，人說的聲音，人說的聲音怎麼工作，那麼做了一個fast speech，是做定型的推理，那fast speech 2，是做多峰性的映射，那pod speech是做了輕量化。

那DigTDS做了多音詞，和基於語言模型的是MegaTDS，那除了這個之外呢，我們也做了歌聲，那基於Diffusion的一個應用，那是歌聲，和基於Diffusion model的應用。

的Make an Audio，那麼歌聲的話，有不同的表現力的工作，Audio的話有，不同的一些開放語的工作，那麼謝謝大家，非常高興有這個機會，在志願大會上，分享一下我們近期的一些工作，謝謝各位老師。

也謝謝各位同學，非常感謝趙州教授的，這個精彩的報告，然後我們這個，一個問題吧，我們有一個現場的Q&A，好請你，不好意思，我們可以後面還有圓周會議，我們再跟趙州老師，OK謝謝趙老師帶來的工作。

然後我也關注過你們，那個DiffSinger和POPCS，那個數據集，就我的問題其實很簡單，就是說在A6GPT2。0的時候，在這個，就是通過這個VQ嘛，就是通過這個，InCodec的VQ，那其實。

我就在好奇就是說，這個VQ它的，會不會損失一些信息什麼的，然後剛好您在前面說，Mega TTS的時候，發現這個Token是，適合在韻律上面去建造，整個語音的，謝謝你這個問題，其實是，有一種做法是。

現在做法是，比如說我們直接把，Speech轉化為Token，但是呢，我們發現有一個問題，就是說，我們的Speech是一個音頻，音頻它是跟文本不一樣的，一個Modality，因為文本的話。

它只是涉及一些語音的信息，所以轉化為Token，是很reasonable的這麼一個過程，但是音頻來說，它比文本要更加複雜，它有語音的一些信息，它也有Audio的一些信息，包括Audio的信息。

比如說是有韻律信息，包括它時長，能量，音高，等等等等，一些其他的一些Attribute，那麼我們在做這個的時候，那麼參考我們的，MAC TTS的一個事情，我們做了很多的實驗，我們發現就是說。

直接把我們的Speech，直接轉化為Token，用語言模型，在Audio和Speech這個場景，效果不是那麼的好，因為不是說，它裡面的所有的屬性，都適合用作離散化的，這麼一個過程，那它會損失一些。

比如說音質等等，其他一些問題，所以說我們做我們的，Tokenization的時候，我們採用了第二種策略，我們是對我們的Speech，進行一系列的一些結合，就是說我們現在是用了，我們的韻律。

包括我們是用我們的音高，這一些屬性，我們是做它的Tokenizer，用我們的離散化表徵，那麼有一些的話，比如說像Duration，這種Duration的話，我們依然是用，我們之前的NetSpeech。

這種框架來做這樣子一個預測，所以說在我們的工作裡面，我們是先進行結合，不同的一些屬性，那麼作為Duration，繼續用Duration Prediction來進行預測，那麼音高或包括韻律的話。

我們是用Token來進行預測，那最後我們又是Fusion，成了我們的這樣子一個Model，所以說我們不是很簡單的，就是用現有的一些工作，對它的直接作為的一些結合，和離散化工作，因為這個效果。

音高或音質不是特別的好，謝謝，我們再次感謝，趙州教授的精彩報告，我們下一個報告，是來自於，北京智原人工智能研究院，NLP與多模態研究中心的，劉廣研究員，劉廣博士呢，他是FLAG AI的核心貢獻者。

他的主要研究方向，是大語言模型，和多模態文生生圖的方向，今天這個報告也是非常非常有名，他給大家彙報一下，低資源的多語言文生生圖模型，Auto Diffusion M18，是一個非常漂亮的一個。

多語言的文道圖的生成模型，好 我們歡迎劉廣博士，非常感謝大家今天來聽，我們的討論和分享，我這邊分享的主要題目就是，一個基於低資源的多語言的，文生圖模型的這麼一個改進，首先大家對於文生圖，肯定都不陌生。

但是這個領域呢，其實發展是非常快，是在最近的時候，其實在一年以前，大家其實對這個領域，可能還不太了解，都沒聽說過，但是在最近，就是去年五月份，OpenAI推出Dell E之後，文生圖的發展就非常的迅猛。

但是OpenAI它沒有，Open它的那個Source Code，和它的Model，所以說後來很多公司，在Follow這個工作的時候，都是幣源的狀態，就比如說百度，谷歌，Mid Journey。

它們的效果是非常好，但是效果非常好呢，同時也帶動了一大幫，社區的一個用戶，跟他們進行交互，然後幫助他們的質量提升，但是呢，沒有一個開源開放的代碼，所以說在這種情況下。

就是Stable Diffusion橫衝出世，它把它的模型權重，和所有的代碼，都已經公開出去，效果非常驚艷，然後從去年的九月份開始，到現在的話，就不斷的持續的，有很多的新的模型出來，這邊的話就是一個。

它的一個新標的增長，現在是肯定有增長更快的，像OpenAI的那個Chair-DBT，所以說現在很多的開源社區，基於Stable Diffusion，做了非常非常多的一些改進。

和大家都是用Stable Diffusion的，一些衍生的產品，或者是衍生的一些模型，做很多有意思的一些應用，然後所以說我們其實工作也是，基於這個Stable Diffusion，去做了一些改進。

那我們在這裡就先簡單，跟大家再重新回顧一下，Stable Diffusion是一個，什麼樣的一個狀態，會是一個什麼樣的一個組件，最開始是Stable Diffusion，有三個主要的組件。

就是右上角的一個，Frozen的clip，就是clip模型剛才，剛才趙州教授他也說到了，clip模型其實是一個，比較強大的一個文圖匹配的，這麼一個模型，它可以提供文字到圖像的，這麼一個相關的一個表示。

但是這個模型，它是把文字編碼成一個embedding，到一個影空間，然後這是第一個組件，第二個組件的話，是說把文字和用語，去做一個Denoise的一個操作，就跟那個，就是把一個白噪音。

逐步的還原成一張圖片，所以說這個clip模型，它實際上是提供了一個condition，就是讓這個圖片知道，往哪個方向去做Denoise，然後使得生成質量符合，我們文本的輸入，除了這兩個組件之外。

就還有一個叫做，Auto Encoder的一個組件，就是把一張圖片，壓縮到一個影空間，從一個影空間，再還原成一張圖片，就基本上這是主要的，三個組成部分，對，現在那就是文神圖這方面，我們跟進了很長的時間。

其實它本身的周期，也不是特別長，但是我們一直在跟進，跟進的話就發現其實，在文神圖研究領域，有三個我們認為的主要問題，首先是一個是，高質量數據級的缺乏，大家都會覺得，高質量數據級，其實Lan他們有一個。

2B的英文的數據級，2B是20E，然後5B的一個多語言的數據級，數據量是非常大的，那麼為什麼還會說，缺乏這樣的高質量數據級呢，因為是開源數據級，它的質量是參差不齊的，就不一定就是。

所有的開源出來的數據級，你都只能夠用語去訓練一個，高質量的文圖生成的一個模型，同時它的語言分布是，極度的不均衡的，就是絕大部分的文圖數據，其實都是英文的，然後其他的語言，比如說中文。

它的分布量就沒有那麼多，同時這個文圖的數據，獲取也是比較困難的，就是可能沒有那麼多，高質量的比如說藝術創作，或者是封面設計，海報等等，這種數據有一個，高質量的一個，可以獲取的渠道。

所以說這個高質量數據級，是我們去做一個文圖研究的，一個主要的一個挑戰，第二個是說可控生成，就是現在有很多研究，比如說Dream Boost，比如說Laura，它用這種方式去做一些。

快速的新概念風格人物的學習，但是這種其實也是，效果可能還需要有一定的提升，或者是需要額外的一些組件，或者是其他的模型進行一些組合，才能夠做得比較好，然後另外一個是在生成的時候，可控性它還不能做得。

非常高的精度，就比如說我們要去，重新生成一張圖片的時候，當然現在Control Network，等等一些工作，已經可以把這個精度，控制得比較好，但是對於背景，或者是我們要做到那種視頻，那種背景級別的。

完全可控的那種精度，其實還是有一些gap，第三個是說，怎麼樣去實現一個複雜的編輯，比如說我們要同時做很多個，一系列的操作的時候，怎麼樣把這個圖片的可控性，生成的可控性提上來，所以說這是另外。

我覺得是另外一個可控的挑戰，第三個挑戰其實是，文生圖的一個評價，現在有很多文生圖的模型，然後它看上去，生成效果也還不錯，但是我們怎麼樣去評價它呢，這是一個比較難的一個問題，就是生成模型共同的一個問題。

就是說自動化評價指標，跟人的主觀評價的指標，可能一致性比較低，但是人工評價呢，它的層面又比較高，所以說就是沒有一個統一的，評價標準的定義，所以我們就針對這三個難點，其實主要是針對第一個。

靠指標數據級的這個問題，進行了一些研究，首先就是介紹一下，我們做了一些分析，就是可以看到有中文的，和最左邊這個黑色圖，就是中文上面有洛亞啊，有Zero的這些中文的開源的圖文的，PAL的那種數據級。

它的數量，以及下面COIL和LAN兩臂的圖文，數據級的質量，它們是有一個明顯的一個差距的，然後LAN5臂裡面的，有一個數據級叫做MultiLAN兩臂，它是多語言的，那麼在這個多語言的分布上面。

大家可以看到其實是很不均衡，有的語言非常多，有的語言非常少，那麼針對這種問題的話，我們如果想要訓練一個，其他語言，就是可能是除了英文之外的，其他一個語言的一個文圖模型，可能它的數據量就不夠了。

那怎麼樣去解決這麼一個問題，我們剛才其實可以想到，剛才有兩個主要的組件，一個是那個clip模型，一個是那個unit，就是Denoise那個模塊，所以說我們就先為了訓練一個。

就是一個Diffusion的多語言版本，我們先訓練了一個，多語言版本的clip，就把clip模型通過一種，叫做Teach Learning，或者是針流的方式，把一個本身需要大量圖文對去。

經過訓練的這麼一個文圖表徵模型，我們基本上沒有用到圖文對的信息，也可以達到一個比較高的水平，所以說可以看到右下角，我們完全沒有用到圖文對的信息，只用到一個平行語調的信息，去做了一個這樣的處理。

這個方法是很簡單，我們其實也覺得它的效果，沒想到它的效果會有這麼好，所以說就是在只用圖文對的信息，把英文和中文這樣子分別去，做一個針流，它的效果就會非常好，這說起來很簡單，但是之前有很多同類型的方法。

也去做過這樣的實驗，但是他們有一個缺陷，就是他們在獲得了一些中文的能力的時候，英文的能力會有一個極大的降低，我們可能就是變成一個純的，其他語言的，比如說中文的一個模型。

但是在英文能力上就下降的會比較厲害，我們這種方法的話，就是說不管是在英文還是中文，英文上是非常接近於原版的clip的性能，但是中文上或者是其他語言上，是會達到一個Sota的效果，這篇文章雖然看上去簡單。

但是也中了一個ACL的findings，然後後面這篇，就是為了我們去做一個文身圖的多語言版本，所以說我們就做了一個多語言的clip模型，做了clip模型之後。

我們再把這個clip模型接到原來的Diffusion模型上，就做了一個擴展，相當於是把原本的Stable Diffusion的2。1，擴展成了一個18種語言的文圖生成模型，支持18種語言。

在這個過程中我們發現一個有意思的現象，就是我們用同樣一句話，把它翻譯成不同的語言，輸入到Diffusion模型中去，它的生成的圖像是比較類似，但是它又包含一點類似於那種文化背景的信息。

比如說用亞洲的語言，比如說中文韓語或日語，輸入到生成一個小男孩的時候，去生成的小男孩可能就是亞洲人的臉型，但是如果是用歐洲的西方的，甚至阿拉伯的語言，去輸入到這個文圖生成的模型中去，它可能會帶有他們。

某些時候它會生成，它帶有它那種民族特色的那種臉型，這個現象很有意思，就是我們其實不是預期它會有這樣，一種文化的氣息，或者文化的信息在裡面，但是呢，我們能發現這個現象。

後續其實我們也想去挖掘更多這樣的信息出來，這一點是說，如果我們有了一個非常高質量的英文的文身圖模型，是不是就已經夠了，我們其他語言的那些不同的文化的信息，是不是可以完全通過英文表達出來。

這是我們其實想去做的一些工作，就是說我們覺得其實有很多信息，或者是語意或者是文化，它是跟語言綁在一塊的，如果你用英文去表達，或者其他語言去表達，你就喪失了原本的那些文化的意味，比如說北京的四合院。

比如說北京比較著名的叫做什麼交圈，這些你用翻譯成英文，其實可能就完全不是它原來的意思，對，然後，下面介紹一下，有了AutoDiffusion M18之後，我們做了一些事情，就是我們分析。

發現它其實對於文化和語言，是可以有很好的一個理解和詮釋，然後我們就做了一些分析，然後去用一些promise，去激活它的一些中文特色的一些信息，我們待會可以看到，另外一方面是說，我們想去接入到開源的生態。

就是把我們的AutoDiffusion M18，可以接入到ControlNet，接入到Lora，我們也收了一些case，其實可以完全無縫的兼容，同時之前提到的一個可控編輯，高精度的可控編輯。

我們做了一些嘗試，當然這是另外一個研究工作，然後可以看到，我們只是在輸入中文，然後加上中國話這樣的情況下，就可以輸出很多符合中國文化的這種圖像，比如說中國的一些虎，然後荷花，然後就是海上日出。

這種生成其實是需要做一些參數的，或者是prompt的一些選擇，但是如果是基於中文本身去做的話，它這個工作量會稍微小很多，同時還可以基於這個base模型，再繼續在中文的數據體上進行一些。

continuous pre-training或者fine tuning，然後效果會進一步的提升，同時這個implanting的能力也是有的，就比如說，把這個戴著珍珠耳環的上語。

可以改成不同的一些風格和形象，比如說這個case，我們其實是一個中音混合的一個輸入，就把一杯水，其中mask掉之後，可以做各種implanting的生成，我們是覺得是這個效果還是挺好的，挺驚艷的。

所以說我們就拿出來看一下，大家可以就是，其實我們這個模型已經接入到，開源社區的那個web UI，就是web11的那個web UI，那個開源的工具中可以直接調用。

可以跟它現在所有stable distribution的，開源工具做一個無縫的接入，同時那個我們之前的版本，我們之前有過兩個版本，一個是雙語版，一個是九語版。

九語版是可以跟container做無縫銜接的，比如說我們可以去非常，汲取出一張圖片中的一些特徵信息，然後進行特徵信息進行高精度的可控生成，是可以利用現在的開源工具可以做得到。

同時那個這是另外一個case，就是從這個左邊左上角的一個圖片，提出它的那個深度圖，然後基於這個深度圖輸入到我們的M9模型，加上container的這個框架，這個額外的一些參數，直接切換過去之後。

就可以做這種基於深度圖的可控生成，還是比較有意思的，當然就是現在關於可控的，或者是個性化生成方面，是比較用的比較多的，就我們也做了一組demo的實驗，就是說把幾張圖片輸入到這個，把幾張就是大概七八張。

這樣左上角這種風格的圖片，當作輸入，經過我們的一個訓練之後，可以生成同樣類似風格的一些圖片，對我們其實也做了一些，剛才類似於趙教授說的，我們把Aquila模型，就是我們最新公佈的語言模型。

和我們的Diffusion模型做了一個對接，可以用文字輸入去做一些圖片的生成，同時還介入了一個叫做，多部可控編輯的一個模塊，大家可以看一下效果，它其實就是在編輯的同時，能夠很大程度，它做了兩個事情。

第一個事情就是Chain of Sort，就把一個複雜的多部的指令，就比如說我要把它的皮膚變白，把它眼睛變藍，同時要把它動漫化，三個操作輸入到語言模型中去。

語言模型首先做的第一個是叫Chain of Sort，就是把它指令分解，分解成多個指令，然後再調用我們的那個，基於指令的微調的一個指令，去做可控圖像編輯的這麼一個模型，然後可以在很大程度上保留。

所有的細節信息的同時，去對非常高精度的可控的那個部分，就是你文字中描述的那個部分的區域，進行一個高精度的修改，所以說這是我們現在做的一些嘗試，現在還在這個開發的過程中，對 然後剛才說到那個評價。

其實我們是經過了，我們提出了我們有一個圖文，多模態和紋身圖的一個評價的體系和指標，然後基於他們的評價，結果肯定是說我們其實在各個方面，都是非常強的，在各種不同語言上應該是現在是Sort。

多語言上面應該是Sort，剛才說到那些所有的模型和工具，和訓練的流程和微調的流程，都開源到那個FlagEye這個開源平台中，大家可以掃碼加入，然後可以大家一起去不斷地去。

邀請大家加入到這個FlagEye開發的社群中來，好的 我們感謝劉廣博士的精彩報告，然後我們有時間可能一個問題，然後歡迎後面在線下和劉廣博士交流，我們看到那邊有一個問題，能把麥克風遞過去嗎，麻煩你。

好 感謝劉老師的分享，其實我們也在Follow這個AutoClip，AutoDiffusion的工作，然後在這個進行之中我們發現了一個問題，就是說它在這個中英平行語料這一塊的。

不管是Clip能力和Diffusion能力，都可以和SD英文能力達到一個很好的對齊，但是我們發現它很難進一步提升中文理解的能力了，比如說像你剛才提到的這個交圈鬥爭的理解問題。

當我們引入了我們自己內部比較好質量的數據的時候，我們會發現它會急劇地破壞中英對齊那部分的能力，在這塊的話還有一些比如說，這個非常高效的方式可以達到英文水平，達到國外比較好的標準。

同時可以進一步提升中文能力的這種方式嗎，好 謝謝老師，謝謝這位問的一個問題，然後這個問題其實我們之前，這個問題其實我們也想到過，就是在把雙語擴展成多語甚至是18語的時候，我們想進一步做提升的時候。

會遇到這個同樣的問題，就是不管是AutoClip模型還是AutoDiffusion模型，在做進一步提升的時候，它會數據的不平衡或者是，把不同的語言混合到一起去進行對比學習。

或者是Diffusion模型的訓練的時候，都會遇到這樣的問題，我們現在還暫時沒有什麼特別好的方案，但是我們覺得可能就是在學習的策略方面，就比如說我們可能需要把不同語言的數據，混合到一起進行積蓄的訓練。

這樣可能會類似於語言模型，現在去做SFT就是Supervised Training的時候，你如果只是在一個任務上去做這個學習，它可能會導致模型的Encounter Learning急劇下降。

如果我們把它這個Diffusion模型比作，或者是Clip模型比作一個Base模型的話，那麼它如果只在某一個語言上去訓練，它肯定會破壞它的語言對齊能力，如果是把多種不同的平行的語言的數據。

放在一起去做訓練可能會緩解這樣的問題，我再繼續問一下就是說，你說這種數據混合擴充的方式，可能是一個正確的方向，但它會可能逐漸演成一個不是很高效的方式，對，這個問題很尖銳啊，我覺得其實是。

但是這個不高效的話，所以說現在我們是提供了一個很快速，能夠達到一個比較好的水平的基礎階段，但是後續該怎麼樣再逐步提升，我覺得可能還需要進一步的去探討和研究，歡迎以後交流，謝謝老師。

我們再次感謝劉廣博士非常精彩的報告，也謝謝剛才那位的提問，那麼我們這個，我們歡迎劉廣博士，我們接下來的一個報告是來自於UCLA的助理教授周伯雷老師，他的研究方向是。

計算機視覺和機器自主性的可解釋人工智能交互，然後他還對當前AI模型的各種人文屬性非常感興趣，然後這些屬性超越了他們的準確性，比如可解釋性、可控性、泛化性和安全性，那麼我相信大家都非常非常熟悉周伯雷老師。

他因為他很多很多的工作非常非常的有名，包括Cam、Network Dissection這些，那我閒話少說，我們把時間交給周伯雷老師，他今天報告的題目是，記憶這種鳥看圖的可控可交互的大規模場景生成。

好我們歡迎周老師，周老師您可以打開你的麥克風，可以聽見我說話嗎，可以的可以的，然後您可以再共享一下屏幕，好非常好非常好，我們很期待您的報告，OK謝謝李老師的介紹，然後也謝謝支援大會的邀請。

各位老師同學大家好，我是周伯雷，然後現在是在UCLA助理教授，然後之前是在香港中文大學，然後當了三年的助理教授，然後再到這邊美國來，然後我之前其實做了很多不同的工作，然後在港中大階段的時候。

做生存模型做的比較多，主要是在這個GAN的模型上面，做了一些可解釋性分析，以及可控圖片生存，然後最近在這兩年的工作，其實是逐漸在往這個決策分析走，然後今天我是想給大家分享一下，我們在這個場景生存。

以及這個場景網偵的一些研究工作，然後就是想把生存模型，跟這個機器決策兩塊結合起來，這樣也可以給大家提供一些新的思路，那麼我這裡從這個場景生存講起，就是我們這裡關注的是，這種有條件的場景生存。

就像我們需要給定一個輸入，比如說我們現在希望生存這些街景圖片，那麼之前有這種P2P HD的方法，它就可以給一個這種語意圖，然後作為一個輸入，然後它就相當於每個顏色都代表了，這個你想這個區域是車。

然後這個是樹，然後可以生成一個街景圖，然後現在也有這種大模型，比如說打立兔，然後以及一系列的這種文本到圖片的生成，然後你可以給一個prompt，然後它可以生成對應的圖片，當然這些生存結果都非常不錯。

但是它還是存在很多問題，就比如說我們現在希望對一幅圖片，進行這種交互式的編輯，比如說我現在希望在這個街景圖裡面，然後增加一輛車，增加一輛車的話，一個辦法是直接在這個輸入的語意圖上面。

你去放一個車的這個mass，但是這裡存在一個問題，就是因為這些車其實是，它是在它不僅只是在圖片空間，它其實是在現實中的話，它其實是在三維空間，就是這種鳥看圖下面的這種車，所以如果你只是在圖片上面。

放一個這樣的一個編輯的話，它其實是沒法很好的，把這個車的離這個相機的這個距離表示出來，所以這裡更直接的一種表徵方式，其實是這種鳥看圖的一個表徵方式，因為這個鳥看圖。

就BEV representation，它其實代表了這些物體在空間裡面的結構，空間裡面的位置，然後它跟這種LIDAR數據比起來的話，就這種純3D的這種典型數據，比起來的話，它又是一個非常簡潔的一種表徵。

因為它直接是也是個2D的，但是它是從上往下看的2D表徵，那這樣就比較方便的，能讓我們進行這個圖片的這個編輯，比如說我們希望在某個位置增加一輛車輛，那麼就可以直接在這個鳥看圖裡面，把這個車放進去。

那麼它就有可能把這個車，在圖片裡面生成出來，然後另外這個鳥看圖的一個，比較有意思的一個特性是，我們也可以用鳥看圖來做一些這種場景仿真，你看這裡播放的這個視頻，就是你看這些車其實是可以互相進行交互。

然後我們有很多的這種自動駕駛的數據庫，它採集到這些車的軌跡，那麼就可以把這個場景的這種動態運動，然後把它重建出來，所以這是鳥看圖，作為一種表徵非常好的一個特性，所以我今天的這個講座會分兩部分進行。

就是首先是給大家分享一下，利用鳥看圖然後進行場景生成，就比如說我們輸入是一個這樣的鳥看圖，然後我們希望生成這種第一視角的，這個這個駕駛的這種圖片，然後第二部分呢，我是想給大家分享一下。

我們基於鳥看圖來進行一個這個場景仿真，因為就這個圖片生成過後，其實你並沒法用它來真正進行交互，那其實是我們是希望把它能放到仿真器，這樣我們就可以把一些物理的表徵也加進去，然後這些碰撞的表徵也加進去。

這樣整個場景就可以真正動起來，然後也可以跟下游的這些，比如說自動駕駛的這些任務，然後聯繫起來，那麼首先我這裡第一個講的一個工作是，這個B-Bay鳥看圖的這個感知，其實鳥看圖的這種感知其實也是。

也不是一個新問題，但是可能是這幾年大家逐漸注意到的一個問題，然後大部分的這個鳥看圖的感知，都是集中在這個感知任務就檢測任務上面，比如說一輛車輸入六個視角，然後它可以生成一個這種從上往下的這個圖片。

然後象徵這些車的位置，然後你有了這個鳥看圖結果過後，然後你就可以進行這種比如說規劃，然後路徑規劃，然後這樣就可以計劃出它未來需要產生的這個軌跡，所以近兩年其實這個這種B-Bay Perception。

其實是相對比較火的一個研究課題，就是在這種物體檢測以及這種智能駕駛裡面，都是很重要的一個研究課題，然後這裡其實有一篇這種中述其實就是在講這種，B-Bay感知的這個工作。

對於這個B-Bay感知其實我們組很早以前就有相應的工作，這個View Particle Network這個VPN這個工作，就是當時我還在MIT的時候快畢業的時候。

帶了一個實習生潘伯文然後做的一個實習的一個，他實習的一個工作，然後這邊工作也挺有意思，就是我們這個工作其實是18年就完成了，但是整個搞到20年才發表出來，然後中間也是歷時了這個兩年。

然後五六次的這種巨搞，就是被各種這種CV的會這個聚了，然後最後發表到這個I-ROS，其實是一個Robotic相關機器人相關的這個會。

所以這個VPN其實是最早做這個B-Bay Perception的一兩個工作之一，我沒法說他是第一個做這個，但是他是至少是前三開始做這個工作，因為我們覺得這個任務也是非常有用。

就是你在機器人情況下面你給他一個第一視角，然後生成他這個從上往下的視角，這樣就可以拿來做很多應用，所以這個其實也是一個工作如果真的好的話，他其實是可以成為一個好的工作。

就現在其實做這個B-Bay Perception的，都會去引用我們這一篇工作，這是一個題外話，然後我們回到我們這裡想做的這個B-Bay任務，就是之前的工作是相當於我們給入輸入的圖片，然後生成這個鳥看圖。

我們這裡希望把做這個的反問題，就相當於我們想做這個B-Bay generation，我們這裡就生成，然後相當於輸入是一個鳥看圖，然後我們輸出是這個圖片。

然後我們就這個就我就帶了一個這邊UCA的一個本科生Alex，然後對這個問題進行了第一次探索，我們之前其實有過一次調研，其實並沒有大家並沒有意識到這是一個有意思的問題，然後我們就把它首次把它提出來做。

然後我們這裡就希望給一個這種鳥看圖，然後生成不同視角第一人稱的這個圖片，你看右邊他其實我們有三個相機，因為在自動駕駛的車上面，其實他是有放了這種一排的相機，就比如說三個相機往前，然後三個相機往後。

這樣就可以對周圍這個有感知，然後我們這裡是希望生成這個前面三個感知，然後這裡技術藍點就是說，我們怎麼把這個鳥看圖的這種作為輸入，把它加入這個圖片生成裡面去，另外一個藍點是我們希望不同視角。

就比如說這個左視角跟正面視角，它有它的一致性，因為這裡我們這裡三張圖，其實是分別從decoder出來的結果，然後我們希望分別出來的這個結果，它之間是有重疊的，你看到它其實是有側面的重疊。

然後我們希望重疊的部分，然後它也能保持一致性，然後我們就把這個BVM進行了相應的一個建模，其實也是用了比較標準的encoder，decoder的一個結構，就用了這種BQBE2，然後我們分別對它的鳥看圖。

以及這個圖片生成進行一個學習，然後再把兩塊聯繫起來，所以這個model其實跟這種Darling2，文本到圖片生成其實有一些類似，就像我們這裡編輯的，就是encode的就不是文本了。

就是我們這裡編輯的其實是encode的，其實是BVV，然後把BVV變成鳥看圖，變成這個特徵相連過後，然後再放到這個decoder裡面去解碼，然後能把這個圖片解出來，所以我們就做了這樣一個類比設計。

然後這裡有一個特殊的設計，就是我們希望這個不同視角，它的注意力其實是有對應的空間關係的，所以就做了這樣一個小的設計，就是在position encoding的時候，把不同視角它的特徵相關性。

然後把它放到self-attention，這樣就可以讓它確保它的一致性，然後我們這裡是出來的一些結果，就左邊是我們放的top-down鳥看圖的輸入，然後右邊是我們分別在六個視角下面，產生的這個圖片。

其實效果還是比較不錯的，我們這裡用的decoder其實就是用的這個BQBE2，然後我們學校也實驗室也沒有資源，可以去run這種擴散模型，所以就直接用了這個BQBE的這個結果。

所以它圖片上面質量其實也還是有很多瑕疵，我們覺得把這個decoder，如果換成更好的decoder的話，這個圖片效果可以進一步提升，不過這並不是這個工作的重點，重點還是我們希望能把這個問題首先提出來。

然後建立一個baseline，這樣大家就可以來做了，然後這是另外一個結果，你看左邊是放這個鳥看圖，然後右邊是它生成的這個結果，然後你看到右邊這個建築，它其實就從前的那個視角裡面，那個建築就在右邊。

然後在右視角它其實也是有，然後右下視角它也是有建築的，所以它這個方法其實它是比較好的，能保證不同視角，從這個decoder出來的這個結果的一致性，然後都能保證，然後我們這裡再給大家看一個這個視頻。

然後這裡左上角是一個鳥看圖的輸入，然後上面一行是我們生成出來的三個視角的結果，然後下面一行是這個ground choose，就是它原始這個相機對應的樣本，你可以看到它其實是一致性對應還是很好的。

因為我們這個鳥看圖，它其實是一個抽象的表徵，它只是告訴了這個位置有車，但是什麼樣的車我們是不知道的，所以它就是有這個隨機的這個效果，你看到這裡我們並沒有這個，持續的這個限制加進去。

所以你可以看到它每一幀其實都是在變化，但是它跟下面這個實際的圖片，它還是能對應上，就是它有車的位置還是應該有車，然後有建築的地方它有建築，然後有些地方它也會把這個建築，換成一個其他風格。

然後以及加一些這種樹進去，在這個鳥看圖裡面，它對周圍這個background，它其實並沒有對它進行這麼細的這個風格，所以你可以看到它其實出來的這個生成結果，還是挺有趣的。

就是它可以把這個場景比較好的這個給重建出來，然後我們這裡在利用這個BVGen，做了一些趣味應用，就是我們這裡鳥看圖，其實是一個比較簡潔的一種表達，我們可以用這個仿生器來產生它這個鳥看圖。

然後這裡兩個結果就是左邊我們是，從我們這個自研的一個駕駛仿生器，叫Metadrive的一個仿生器，然後後面我會給大家再介紹一下這個Metadrive，就是從仿生器裡面拿到的這樣鳥看圖的輸入。

然後再放到我們這個模型裡面去，然後就可以利用這個模型，然後把第一視角的這個圖片給生成出來，這樣其實你可以想像，後面如果我們能把持續的這個一致性能加進去的話，那我們就相當於可以鳥看圖得到這些軌跡。

我們就可以直接把這個場景的這個生成給變出來，這是BVGen的這個工作，然後這裡一個潛在的問題是，我們之前那個工作其實都是在做單純的圖片生成，但是生成出來這種第一視角的這個圖片。

它其實比較難直接用到一個仿生器，或者這種實際的駕駛仿生任務裡面去，我們實際是希望能得到更多3D的信息，因為有越多的3D信息，讓我們就相當於可以越好的把這個場景，放到這個仿生器裡面。

所以我給大家接下來講的一個工作，是我們從這個兩維圖片，然後到3D這種3D的圖片生成，就像是它裡面包，它又不是一個完整的3D，但是它是2。5D相對的一個東西，這樣我們就可以比較好的。

更好的控制這裡面車的這些特性，然後這裡這個工作是我們今年CVPR，發表的一個工作是由我的學生徐英豪同學，在Snap實習做的一個工作叫Discosync，我們這裡想嘗試的是把生成模型給Nerve。

Nerve是神經場模型兩者結合起來，因為Nerve其實原始是拿來做純建的，它其實並沒有生成的能力，所以我們希望但是Nerve模型，它自身帶有很多這種3D的信息，所以我們想把這兩者進行融合。

所以我們這裡就想做一個這樣的一個排序，我們有二維的這種鳥瞰圖過後，然後我們可以生成這種三維的結構圖，然後再從三維的結構圖裡面進行這種神經的，Neural Rendering那種渲染。

然後能把這個場景給渲染出來，所以我們今年這個Discosync，就是這個工作就是在做這樣的一個嘗試，所以我們就提出了這個Discosync這個模型，然後它是一個我們說它跟Nerve最大不同。

就是它是個Generative Nerve這種概念，就是它其實對它採樣，然後它可以生成新的這個場景，所以我們就把這個輸入也加進去了，就是它輸入其實也是一個這種鳥瞰圖的，一個3D的一個抽象表徵。

然後它象徵了物體對應的位置，然後可以放到這個Generator，Object Generator裡面去，然後我們背景也對單獨處理，這樣就可以把前景跟背景兩者都結合起來。

然後再利用了一個Nerve裡面常見的這種Retracing，然後給它Volume Render出來，然後再把圖片這個up sample出來，然後這裡然後我們也結合了一些，GAN的一些東西。

就是把加入了一個這種Discriminator，然後跟實際的圖片真實圖片進行這種分辨，然後我們也對它這個前景，然後進行了一些處理，就是前景然後我們再加了一個單獨的一種，這種GAN的一個區別器。

這樣就可以使得前景圖片，它這個效果也更好一些，然後就像這樣把一個生成模型，GAN的一個模型跟這種Nerve模型，兩者做到了一個結合，然後我們這裡有Local跟局部跟整體的分辨。

利用GAN loss使得這個圖片，它的真實度可以進一步上去，這樣我們就可以對通過改變這個輸入的，這個Layout結構的不同，那麼我們就可以把這個圖片，生成出來的這個圖片也進行這個對應的編輯。

比如說我們想把這個車的位置改變，然後它的朝向改變，這樣就是它可以通過這個神經網絡進行這個渲染出來，然後我們也跟之前的一些方法進行一些對比，就是在Clever這種3D front。

然後Vamo視力庫上面都進行了對比，然後這裡我們選的就是ValueGAN，以及3D然後Giraffe，然後我們在這些場景裡面，效果都是最好的。

對在這種3D 3D aware image generation，這個細分領域的話，就是最好的一個結果目前，然後接下來因為這個生成模型，它其實是跟Nerve結合了。

所以我們Nerve包含的這些有用的特性，我們都可以加進來，因為看到這裡Nerve它其實可以調整這個相機，發射的這個方向，然後我們可以調整這個場景的這個3D結構。

然後這裡圖片它其實是這個Volume Rendering出來的，就把這個圖片，然後我們可以自由地改變這個場景的這個結構，然後，然後這裡我們也可以對物體進行一些這種顯示的一個編輯。

比如說我們希望調整這些家具的這個位置，我們就可以在這個秒看圖這個結構裡面，然後拖動它的這個方框，然後這個圖片，然後它也可以對應的這個位置，也可以把它渲染出來，然後右邊這個對這些物體。

然後我們也可以這種Translation Rotation，然後都可以達到相應的這個編輯的一個結果，然後我們也可以進行一些這種對物體進行增加跟去掉，比如說我們可以把裡面不想要的車。

然後在我們輸入前面給它這個去掉，那麼它對應的這個位置，這輛車然後就被去掉，這樣如果你只是在兩維的圖片上面，通過比如說Diffusion Model進行這種Intending的話。

它其實對3D的理解並不是這麼好的，它結果是並不理想的，所以就這種3D的編輯的任務，就確實是應該是在這種3D的表徵下面進行操作會更好，然後這裡我們也可以做一些這種增加，比如說在這個場景裡面。

我們希望增加更多的車，然後就可以在最前面的輸入，然後把這個車放到它對應的這個3D空間裡面的位置，然後它就可以把在2D空間裡面，把這個圖片給生成出來，所以它就對2D跟3D之間建立了一個比較好的一個聯繫。

然後我們也可以進行一些這種restyling，比如改變這個車的這個圖片，它的顏色然後它的這個shape結構，然後就可以改變，然後這是我們跟之前的一些方法比較，比如說這種1G 3D，然後一些比較。

然後我們對它相機控制都是要更好的，然後以及跟Giraffe，然後也是有一些對比，然後我們比Giraffe能取得更好的這種結合控制，因為我們在我們的情況下面是更顯示的去對這些不同物體進行建模。

然後現在在鳥鋼圖空間進行建模，這樣就可以得到一個非常理想，以及直覺性的這種控制，然後這裡也可以進行一些這種實際場景的一些編輯，比如說我們可以用一個encoder把給定的一個圖片放到這個影空間裡面來。

然後再進行對應的這個編輯，然後接下來我們是想繼續把這個空間擴大，就是之前的工作講的是說我們把一個位置的一個鳥看圖，然後放給這個模型，然後它只能生成相當於一個場景的一個圖片，生成一個結果。

但其實我們在現實生活中裡面其實是有非常大的一個場景，比如說這樣一個地圖，我們其實是希望是能非常大的這種場景結構，那麼這個問題其實是在沒有多少人去探索。

所以我們想把這個之前這個生成模型擴展到大規模這種場景的一個生成，然後這裡一個我們首先想到一個方法是說是不是這種大場景生成，就是跟視頻生成其實是一個概念。

這裡右邊我是給大家展示的是我們下載的一些駕駛數據庫，然後你可以看到這些駕駛數據庫，它其實是展示了在很大一個空間結構裡面進行這種運動，所以我們能感知到它的這個結構。

所以是不是我們把這個問題可以把它變成一個視頻生成的一個問題，我們也對這個問題進行初步的一個建模嘗試，就是我們今年SOR一個工作就是在這個圖片生成，然後我們就去魔改了一個叫StyleGAN-V的一個結構。

然後給它加入了很多這樣的一些鮮豔，比如說這種Alien Spray的一些設計，然後以及對它的Decoder進行一些這種Protruding訓練，然後對它的這個分類器然後也進行一些操作。

然後就可以得到相對視覺上面能看得過去的一個生成結果，然後這裡其實如果你如果換一種思路來想的話，視頻生成如果能把它生成的非常連續的話，那其實你就相當於是在3D空間裡面對這個相機進行運動。

那麼就能把這個場景給重建出來，但是其實效果還是不是這麼理想，我們也跟之前的一些方法進行對比，然後雖然在圖片層次FID上面比之前的StyleGAN-V效果好，但是它其實也沒有真正表示這個空間的一個結構。

那我們就又換了一個思路，就是說如果我們能把整個大場景生成出來，那我們就可以把相機放在這個大場景裡面運動，那麼就可以把這個運動的這種視頻然後給生成出來，所以我們就把這個問題又切換到了一個大場景的一個生成。

然後我們這裡是最近做的一個工作，然後就是想對這種無限場景的一個生成，比如說我們希望在訓練的時候，然後可以給它比較小的這樣的場景，這種鳥瞰圖然後生成圖片，然後我們在測試的時候然後可以擴展它的空間。

比如說如果它是一個這種Translational Environment的一個結構的話，那按照你input你給它放一個更大的這種鳥瞰圖的話，它應該可以在不同位置然後把這個場景都給生成出來。

所以我們這裡對這個問題進行了一個初步的一個嘗試，然後這裡技術細節就不說了，然後因為paper還沒有放到archive，然後我們達到的一個目的是說我們給錄這樣一個變化的一個鳥瞰圖。

然後通過它局部的這個生成，然後我們可以把這個局部生成的結果，然後拉成一個更大空間的一個結果，然後這裡是我們在這個clever數據庫上面做的一個初步的一個結果。

你看到左邊是我們放進去當前位置放進去的鳥瞰圖，然後第二行是這個局部生成的結果，然後最右邊是我們把整個鳥瞰圖給我們平拉，然後再把它整合起來的一個結果，然後這樣就對空間其實進行了一個更大規模的一個重建。

然後我們也可以達到一些這種編輯的一個結果，比如說我們放入鳥瞰圖，然後我們希望比如說推進這個前方這個方塊的這個箱子，我們就在這個圖片裡面這個方塊箱子就可以往前推。

然後我們比如說把這個對應這個某一個位置的這個方塊，然後它的風格然後從方塊變成一個圓球，或者變成一個圓錐，然後都可以達到這樣一個編輯，然後我們可以在這個場景裡面放入更多的物體，然後這個模型都可以生成出來。

然後這裡是一個交互，我們做了一個交互界面，因為這個鳥瞰圖它是一個非常直覺的一個編輯方式，你就可以很好的你在哪一個位置想生成東西，然後就可以讓這個用戶然後在某一個空間裡面點一個東西。

然後把它拖動到對應的位置，然後我們就可以右邊就是這個生成模型，然後把這個結果給生成出來，就對應這個相機放的這個位置，然後就可以把對應的這個物體給它放出來，然後我們用一個神經模型來學了一個這種仿真器。

然後可以對這些物體進行對應的這樣的一些編輯，OK然後這是第一部分的內容，然後第二部分然後我再比較快的給大家講一講，我們基於鳥瞰圖做了這個場景的一個仿真，然後這是我目前實驗室的一個比較大的一個方向。

就是想把這個機器決策的一些東西，然後也跟我們視覺感知，然後兩者能把它整合進來，因為我們這裡發現就是在鳥瞰圖，其實是一個非常簡潔的一個表徵方式，然後很多的這種駕駛數據庫。

它的這個交通場景其實都是通過這種鳥瞰圖來表徵的，然後這裡我們是從Waymo數據庫上面導進，來的一些交通駕駛場景，然後這裡每個車它其實有對應的這些軌跡，然後這是Waymo他們有這種數據採集車。

然後就把周圍的這些車的軌跡都採集到，然後就可以把這樣的真實數據導到一個仿真器裡面，然後通過這種重新播放，然後就可以把這個場景給重建出來，為了更好的使得這樣的研究工作，就是機器決策跟機器感知結合起來。

然後我們實驗室一直在開發這個模擬器，一個叫Meta Drive的駕駛模擬器，我們這裡強調了它的相對於之前的模擬器，比如說Color，它的一個強處就是它是非常有效率，就是在單機的PC上面。

然後我們可以達到這種500幀的訓練效率，然後我們這裡而且保證了它的這個場景，可以從實際數據庫裡面導入一些新的場景，然後我們也可以學一些生成模型來產生新的場景。

然後這個Meta Drive目前已經開源出來了，就感興趣的同學可以去看一下，然後基於這個Meta Drive，我們可以導入這種真實數據，然後這裡我們是導入一個New Things的一個價格數據。

然後最左上角是它實際的RGB場景，然後我們可以在我們的這個仿真器裡面，對它進行重建，就直接重建，然後右下角是它在這個實際空間位置裡面，秒看圖的一個結果，然後右下角兩張圖是我們這個仿真器提供的這種深度圖。

跟這種點雲圖的一個仿真結果，然後這個模擬器它有比較好的這種場景生存能力，然後我們這裡的場景生存，其實跟圖片的場景生存不一樣的是，我們首先定義了很多這種交通的路口，然後一些這種基本的結構。

然後我們通過一個程序化的一個生存方法，就像對這些我們事先定義好的結構進行採樣，然後跟拼樂高一樣，然後把它拼起來，然後再轉換成可以進行交互的一個環境，這樣就可以避免直接用一個神經網絡去生存所有的東西。

然後我們也對這裡面的這些場景，進行了一個相對比較真的一些仿真跟重建，然後我們把這個交通燈然後行人，然後也放到裡面，然後這是一個導入這個微模數據出來的一個結果，然後你看到這個是四個不同的交通場景。

然後裡面有不同的車，然後以及有不同的人在走，然後這都是通過物理仿真器然後做出來的，你可以想像接下來其實我們就希望能把這些工作物理仿真，跟這個實際的這個圖片生存，然後兩者結合起來。

能把裡面的這些3D的這些部件，然後它的真實性進一步提升，然後這裡就是可以導入這個實際，實際駕駛數據的一些結果，一些場景我們可以把這種高清地圖導進去，然後以及它的對應的一些軌跡，然後也可以導進去。

然後這樣就可以相當於建立一個這種數字，數字孿生體的一個東西，然後這裡是我們跟把這個模擬器跟NewSense兩者進行一個同步，然後左邊是NewSense數據庫的一個結果。

然後右邊是我們利用它的這個鳥瞰圖作為一個中間介質，然後把它對應的位置那個車給它放進去，但我們目前並沒有把左邊這裡面車的這些形狀，以及它的這個視覺信息給它同步起來。

然後我們現在其實有些安裝正在進行的一個工作，就希望能把這個在這個實際的駕駛數據裡面，然後能從實際的場景裡面把這些車給拿到這個物理隱形裡面，這樣就可以使得我們的仿真器也變得更真實。

基於這個導入進來的這個數據，然後我們也進行了訓練了一些生成模型，然後這裡生成模型其實是類似於生成軌跡，因為我們發現我們導入進來的軌跡存在一個不好的地方，是它都是非常斷的，因為它是在現實中採集的。

所以它有很多斷的這些軌跡，然後我們就想設計一些生成模型，然後生成好了過後，那麼就可以從這個模型裡面採樣，然後採樣生成不同的這種交通結構，所以我們今年在這個e-Crowd就是這個機器人的這個會議上面。

有一個叫traffic jam的一個工作，然後它其實也是生成模型，但是它跟圖片生成不太一樣，我們是直接生成這個軌跡，所以我們把這個生成過程分成兩個步驟，第一步驟是我們首先放入一個高清地圖。

然後首先是讓這個模型學習怎麼擺車，就像把這個車從鳥旦圖放進去，然後擺好車過後，然後第二步驟是對於每一輛車，然後我們可以生成它的未來軌跡，這樣就可以得到每輛車的實際的一個在場景裡面的軌跡。

然後我們可以這裡這個視頻就在播放這個擺車的生成的過程，熱力圖就象徵了下一步它最有可能性擺車的位置，然後我們擺好車過後，然後可以把它未來軌跡然後生成出來，然後這樣就可以對這個場景進行仿真。

然後我們可以進一步把這個鳥旦圖放到我們的這個Metadrive裡面去，然後就進一步可以交互的這種物理的一個仿真結果，然後這裡是一些結果圖，然後在不同這種不同路段。

然後生成的生成出來的不同的這個交通的這個軌跡，然後也有在同一位置，然後我們可以對它進行一些擾動，模型擾動，然後它可以生成不同的交通結構交通流，然後我們也用到把它用到一些現有的一些編輯。

比如說這種impending，impending我們這裡的impending，就是說可以可以去延伸它斷了的這些軌跡，然後我們可以對現有的這個場景進行一些擴增，比如說原來的這些交通場景裡面並沒有這麼多車。

然後我們可以給它加入更多的這個車，使得這個場景變得更複雜，這樣就給我們更好的可以測試我們的這些這種駕駛系統，然後我們也做了一些實驗，比如說證明這些生成出來的這些數據是有實際的用處的。

就是我們用這些生成數據來訓練了一個這種強化學習的駕駛模型，然後這裡是先是兩個baseline，就是我們如果用之前程序化產生的場景，然後它其實跟真實數據它其實存在很大的很大的這個gap。

所以它這個它的這個成功率是降了非常多的，然後這裡是如果是用我們這個traffic jam的模型，產生的結果的話，然後它其實是可以得到比較好的，跟它真實場景數據訓練出來類似的一個結果。

然後我們這裡更好的一個好處是，我們可以對這些生成數據進行自由的編輯，比如說使得每個場景裡面它的車流的密度更大，這樣就可以生成更有挑戰性的場景，然後這樣就可以進一步改進這個強化學習。

然後我們可以在這個traffic jam裡面，我們可以做一些比較簡單的調整，比如說我們可以把這個traffic jam裡面，產生出來的一些交通場景導入到其他的模擬器裡面。

比如說我們可以把這個交通場景導入到其他的模擬器裡面，然後我們可以做一些比較簡單的調整，比如說我們可以把這個交通場景導入到其他的模擬器裡面，然後我們可以把這個交通場景導入到其他的模擬器裡面。

然後我們可以把這個交通場景導入到其他的模擬器裡面，比如說color或者GTA5這個遊戲，俠盜獵車手的這個遊戲裡面，然後這樣就可以幫助這些模擬器擴展他們的這個場景，然後這裡是一個結果。

就是我們把我們traffic jam產生出來的結果導入到color裡面去，你看到它是有不同的一些車輛的結果，OK 那這裡就是一個比較簡單的調整，然後我們就來做一個比較簡單的調整。

然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整。

然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整。

然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整。

然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整。

然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整。

然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整。

然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整。

然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整。

然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，然後我們就來做一個比較簡單的調整，那麼我們下一個報告是來自於斯坦福大學助力教授吳嘉峻老師。

然後吳嘉峻好，那個你不介意我中文介紹你，這個好好的好的，那吳嘉峻是斯坦福大學計算機科學系的助力教授，然後呢他這個研究方向是機器感知推理與物理世界的交互，從人類的知識中汲取靈感。

然後呢他曾經在加入斯坦福之前，在Google Research當任訪問的教育研究員，然後呢他在MIT獲得博士學位，導師都是大佬，Bill Freeman和Josh Tenenbaum。

然後呢並且在清華大學獲博士學位的時候，在MSA和這個圖倫文老師有非常非常密切的合作，然後呢我們可以看到嘉峻老師吳嘉峻老師他就是各種師從大佬，他本身做的也非常非常優秀。

在3D或者在很多這種物理世界上的這種，類似於基於這種物理的鮮豔去跟環境交互上有非常非常有名的這種工作，然後我們今天呢也是非常榮幸邀請到他來給我們講一講他最新的這個工作。

報告的題目是Understanding the Visual World Through Naturally Supervised Code，好我們歡迎吳教授，現在這樣行嗎，可以的可以的，很好。

好的好的，好謝謝謝謝李老師邀請，是這樣就是我剛才和李老師說，就是說我之前試過，我說我們這個尤其當時是在哪個國內哪個公司，我說我們講一下然後我就用中文講，後來呢或者中英加雜的講吧。

後來我發現這個就是講得很不好，大家也聽不懂我也講得也不順，然後呢就是後來我就前面我就在跟李老師說，我說要不我們就還是用英文來講，然後這樣可能反而方便一些，然後等到之後我們問答環節和這個panel的時候。

我們可以再用中文，好吧所以先跟大家說，這個征得了這個主辦方的同意，所以我用英文來講這個，然後再次感謝這個邀請，非常榮幸能夠在這裡，我叫吳嘉俊，我是斯坦福大學現在的助理教授，好的今天我要說的是。

通過自然系統化的文法來理解視覺世界，視覺世界我認為是很容易理解的，我們生活在這個世界裡，我們用我們的人類視覺來看，所有的圖案、數位、物質、質感，我們每天都在用Python或其他東西來寫文字。

所以我認為我們都明白寫文字的需要，雖然我希望在這個講座的最後，我能夠展示我們可以用文字或標語來解釋，比如說人們說的非人物系統化AI，寫文字或標語或程式，在更廣泛的意義上。

不僅僅是Python或Full Loops，我們可以有更廣泛的解釋，寫文字的意義，同時我們可以知道自然系統化的意義，我會給大家介紹幾個例子，希望我們可以更清楚地解釋，好，問題是我們如何能夠。

真正地利用現實世界的，富有結構、標語、程式，來改善視覺世界的觀感和理解，首先，在這個世界裡有很多富有結構，如果我們看這些畫面，例如廣場或建築物，我們會發現它們並非只是像畫面，雖然這些人物系統化的模型。

總是以畫面來形容，但它們的結構其實比像畫來得更富有，例如我們看這些畫面，我們會發現這裡有飛機，這些畫面是由飛機製造的，這裡有天花板、地面、牆壁，還有一種相等的結構，就是反光的相等，還有一些繁複的結構。

你可以看到上面的燈光，在繁複的結構中繁複地繁衍，如果我們看建築物，我們會發現它們是像窗戶或地面繁複地繁衍，所以我們的問題是，我們能否利用這種結構的資訊，來超越畫面，或是和畫面結合。

以智慧的視覺理解和編輯，來做出更多的改善，我給大家一個例子，這是一個視頻，我們放在這裏，這可能只是Photoshop的GUI，但它實際上是我們的原理，我們想要的東西，或我們能做的東西。

首先我們可以進行互動分配，因為建築物的使用者有一個互動，所以這是標準的，所有人都可以做，你進行互動分配，你會有建築物，你可以在3D中計算消失點，然後我們希望使用者，透過一個更多的互動，那就是建築物。

如何變得更高，使用者可以只用拉動，來使建築物更高，使用一種互動，一種步驟，或者是，如何讓建築物更高，問題看起來很簡單，但實際上，其實並不簡單，因為你需要，真正的了解，建築物的多層次，在最低層。

你需要了解建築物的質感，質感是什麼，建築物的顏色是什麼，所以東西應該看起來相似，在中等層次，你需要了解，建築物的3D位置，建築物是3D的，每個層次都有，它們的層次，所以你需要了解，建築物的層次。

建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次。

建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次。

建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次。

建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次。

建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次。

建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次。

建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次。

建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次，建築物的層次。

建築物的層次，你可能會覺得，這是舊式的機械學的方法，但你可以想像，用GPT-4來替代這些東西，你可以做到相同的效果，如果我們看CPR-2023，就像是VyprGPT，這些都是新的，舊的想法。

被重新實行和實現，用新的工具，但基本上，你可以做相似的東西，例如把這些圖案，分類出來，然後擴大，這是他們在2018年，做的2D線組，而你們可能也注意到，一個明確的限制，就是，假設你擁有。

圖像裡的物件系列，所以在這個情況下，你知道，這個世界只是，圖案和線組，我只想找到這些，線組和線組，來分類圖像，然後我可以搜尋程式，來解釋這些，低次元空間，但是世界並非，是圖案和線組，如果我們想從畫圖中。

從圖案和線組，來整理成自然形，就像我們有一個牛奶瓶，裡面有很多麵包，他們就意識到，有些結構很明顯，可能你還是小孩，我們只是玩這些結構，然後我們做出一個，三角形，但是，這裡有一個物件，所以它是一個結構。

但是你怎麼去代表它，它不僅是線組和線組，而是圖案，所以特別是，我們能否，找到一個方法，來解釋這些，低次元空間的物件，甚至不需要，很多的知識，我們受到，一些經典的電腦思想，在內部學習，或單純的畫面學習。

是由，以色列的梅哈爾·羅尼，他們在這項題目，工作了一年多，其實已經兩年多了，所以，內部學習或單純的畫面學習，他們依賴這個關鍵觀察，就是如果你看到單純的畫面，即使只是單純的畫面，他們意識到。

單純的畫面中的圖案，很可能會重複自然，這種傳統，可以發生在同一個層次，在這些紅色的格子，但也可以發生在不同層次，在這些綠色的格子，所以，為什麼會有這種重複，為什麼會存在，因為，如果我們看一件這樣的場景。

有這麼多格子，他們意識到，基本上，這些格子，是相同類型的物件，所以他們必須看起來相似，因為他們是相同的種類，相同的類別，但只是不同的場景，相同的物件類別，但是，因為現實世界是3D，你看到的物件。

在2D的畫面中，有視覺的影響，你擁有3D的空間，你視覺的影響，在2D中，這就是為什麼，物件看起來比較大，而這種重複相似，可能發生在不同層次，除此之外，也可能發生在同一個層次，好，現在我們有了這個觀察。

我們如何能夠使用它，所以人們所觀察到的，或是試過的，就是把它們結合進去，例如，如果你有一個像這樣的畫面，有些重複的圖案，如果你把這個圖片，傳送到一個預設的網絡，例如，一個預設的Alexa的圖片。

然後你可以拿出這些圖片，這些活動圖片，然後你可以計算，每一個可能的移動，如果你把這些圖片，移動到横向，X輻輻，或是直方，或是Y輻輻，你可以把某個輻輻，移動到某個輻輻，然後這些圖片，有多可能會和自己相符。

這些圖片的自然相符，在不同的空間，然後你可以用ArcMax，找出X和Y，最大化的相符，這些X和Y的意思是什麼呢？，它們大致上只是指出，兩個鄰近的重複物件，最大可能的差別，因為如果你把物件。

傳送到某個圖片，某個輻輻，這些圖片很可能會重複，這意味著，這些傳送的物件，相同的空間物件，可能看起來很相似，這就是為什麼，X和Y很可能是，兩個重複物件之間的距離，我使用這些功能圖，而不是只是RGB像素。

是因為這些認知網絡，是訓練，它們應該是，對自然影像的所有聲音，對於錄影聲音，或是光影的改變，等等，希望它們會更堅固，我們會採取這種方式，但我們會做一些改變，這讓我們可以，不需要做任何訓練。

我們需要做一些訓練，因為我們需要採取認知網絡，但我們不需要訓練，某個數據庫，我們可以採取，這種上下層的方式，然後我們可以測試，一個圖片，從一個圖片中，我們可以辨識，這些重複物件的中心，你可以想像。

你現在正在嘗試，分析自然影像，這對你來說是很瘋狂的事情，我們有這些像素，分析自然影像，但你現在正在嘗試分析，自然影像，在一個，相對較低層次的空間中，這是這些物件的中心，然後，當你擁有了。

這種低層次的空間，你就可以做，像之前一樣，尋找一個程式，來解釋這些中心，現在你可以，尋找一個程式，來解釋物件的位置，但沒有被拍攝的事情是，因為你不再假設，世界是由圓形和橫方構成的，你不再知道。

物件是什麼，你不能說，好，現在這裡有一個線，或者現在這裡有一個橫方，你只知道，現在這裡有一個東西，但那東西是什麼，你怎麼定義那東西，你可以定義，用一條神經網，用一條神經網的生物，所以這是一種方式。

你擁有這種，神經物理的代表性，或生物代表性的圖片，讓你做一些有趣的事情，例如，如果你有，這個圖片，有很多過程，然後有一個失誤的圖片，然後我會問，我怎麼感覺到，失誤的圖片，人類會說，好，這裡有很多過程。

所以我會假設，我可以在那裡放一個圖片，所以這個系統的力量，或代表性，就是這種程式的結構，或碼頭，告訴你，要去看，告訴你，這些是其他物件的中心，這些是你應該去看的圖片，但如何使用這些圖片。

去感受這個失誤的區域，你將它轉移到神經網，神經網，把所有這些參考圖片，用智慧的方式，去做圖片，用智慧的方式去混合，你可以看它，你會發現，哦，這不像是我只是，複製一個隔離圖片，這不像是複製一個圖片。

但其實是，在看我應該在哪裡看，然後用神經網，去放一些低階的圖片，讓出口圖片看起來更真實，也更有靈感，這和我們應該放在那裡的靈感，很相配，然後你可以做雙鏢，你可以有另一個圖片，但不是另一個圖片。

是一個圖片，但是另一個圖片，是一個圖片，是一個自然的圖片，你可以做雙鏢，是自然的圖片，因為我們知道，沒有完美的圖片，是在自然世界，所以每個圖片都必須，該程式告訴你，它們應該在哪裡，但是當然。

它們必須有些小的偏差，你可以去認出這些偏差，然後你可以大大地，大大地調整這些偏差，你可以大大地調整這些偏差，這些物體，從哪裡來的，或者你可以說，你可以大大地調整這些不平等性，這可能會很有用。

在工業生產中，做出偏差探測，我們可以回到，這個牛奶和麵包的例子，你可以找到這些物體的中心，你可以做圖片，或者把麵包丟回去，你可以做雙鏢，你可以加上另一個麵包的列表，你可以做平等性，你可以加上平等性，好。

這看起來很好，但是有一個很大的差別，我們現在是在自然的圖片中，有一個很大的差別，在那張圖片中，在麵包圖片中，和這張廣場圖片中，我早就已經展示過了，在自然圖片中，你可以假設，在自然圖片中，你可以假設。

所有東西都在一個2D平台上，你可以從上面看下來，你可以從下面看下來，但是這並非自然圖片中的一種情況，因為在廣場圖片中，不是每個東西都在一個平台上，明顯地有很多平台，有坐墊，有桌子，有兩面牆。

所以有很多不同的平台，所以現在的問題是，我們能不能將這種，一種結構的表現，從單一平台到多個圖片，這不應該太難，因為我們需要的，當然是鏡頭的定位，我們需要的是圖片的定位，我們需要設定鏡頭的定位。

然後我們需要找到一個方法，把圖片分配到多個平台，然後每個平台，我們需要量度它的波長，6個波長，就是它們的位置和表面正常，當你擁有這些東西，你能夠將圖片的定位調整，因為當你知道圖片的位置，它的表面正常度。

你能夠調整圖片的定位，以便看到圖片的上下方方向，當你擁有這些東西，你就可以從一個問題中，減少到一個問題，一個你已經知道該怎麼解決的問題，你就可以尋找一個方法來解釋，你可以從單一平台到多個平台。

來將圖片的定位調整，這是一個很難的問題，當然我們仍然要，依賴底層的視覺桿，例如我們要量度消失點，消失點的位置，以及圖片的定位，3D圖片的量度，是一個很難的問題，以前我們在2020年，是沒有那麼堅定的。

現在還是沒有那麼堅定，當然現在是有變得更好，所以我們還是用2D圖片，2D圖片是更好的，因為它能給你很多正確的答案，但它也能給你很多噪音，很多錯誤的正確答案，我們稍後再談，但是在我們先前的討論中。

我先想做一個例子，我們總是嘗試，以底層的方式去走，從廣角鏡頭開始，然後我們嘗試去找出，一些低層的視覺桿，然後我們一直走到，高層的結構的程式，繞圈圈，所以這曾經是線圈的例子，人們首先要找出線圈和形狀。

然後才去尋找程式，單一平台圖片，也是這樣，你嘗試去超越線圈和形狀，但你嘗試去找出，這些繞圈圈的線圈，然後你尋找程式去解釋，這裡我們只有一步，這個底層的過程，我們只有一步，我們嘗試用離位點和圓形圖片。

幫助你做平台圖片，你可以做這個類似的例子，嘗試指導你的思考過程，如何做到，但是問題越來越難，我們從精神的圖片，到自然的圖片，到多平台圖片，你會看到問題越來越難，有很多可能解釋，它們會發生什麼事。

比如說在這個特定的圖片，你會說離位點，已經算得很準確，但是平台圖片，有很多錯誤的正確性，根據這些平台圖片，看起來有很多不同的解釋，在哪裡有平台，在哪裡有牆，在哪裡有座位，在哪裡有地，有很多解釋。

現在問題是，哪個是正確的，人類有很多知識，我們說，Candidate 2是正確的，Candidate 2是正確的，因為我們看到了很多廣場，我們看到了很多牆和地，我們知道它們的樣子，但這不是機器人的情況。

尤其是機器人，只有一個圖片，如果機器人只看到了這個圖片，那怎麼會知道，Candidate 2比Candidate 1或Candidate 3好，尤其是它們都滿足了，離位點的限制和平台圖片的限制。

我們可以看到，當我們從更多的複雜圖片中走進來，這些視覺圖片就越來越限制，問題就越來越嚴重，假設空間越來越大，就越來越多的不確定性，那我們怎麼解決這個問題呢，我覺得需要一個基本的理解，我們必須要思考。

為什麼這些結構會存在，為什麼這些物體或平台，會存在，為什麼這些平台會存在，為什麼這些繁體會存在，為什麼這些物體會重複存在，這基本上是因為我們有了人類的喜好，人類喜歡這種喜好，有時候是很明顯的。

有時候是很微妙的，比如說這個通道，因為我們喜歡這種平定，我們喜歡這種結構，我們在建造這個通道的時候，我們先介紹了這個東西，整個環境都需要是相等的，線條都需要犧牲一個系列的範圍。

所以因為這些基本人類的個人喜好，這些結構就存在了，這代表什麼？，這代表要如何幫我們解決其後半程的問題？，這表示如果我能夠實行解決這個低級問題，那麼高級的問題就可以更容易解決，我給你一個具體的例子。

我們先拿一個例子來解釋，我不知道哪個系列的結構最適合我，我會說，好，我真的不知道，但我們假設它們都很好的，然後我們來看會發生什麼，假設我認為一個是正確的，或者兩個或三個是正確的，會發生什麼？。

我們可以前進，我們可以假設它們是正確的，我們可以用表面的數據來量度位置，然後我們可以用表面的數據來量度位置，來改正這些標準，然後我們可以運用我們之前所知的方法，我們可以用程式來解釋它們，然後你會發現。

如果你有正確的標準，那麼標準和解決的標準會非常常見，因為那是人類的個人喜好，因此，如果你想要辨識，尋找一個程式來解釋這些，我剛才說的解決標準，那麼辨識程式或輸入程式，會變得更簡單，你可以看到中間的。

如果你用程式來重建標準，重建會變得更好，另一方面，如果你的標準，標準問題並沒有解決得很好，你得到錯誤的標準，你對表面數據的測量錯誤，然後你重建它，標準就會看起來很奇怪，你不會得到一個非常好的程式。

或者簡單的程式來解釋它們，你得到的程式會變得更複雜，重建會變得更糟糕，所以我認為這裡的基本觀察，是非常深奧的，就是我們常常會想到的，底層問題，例如表面數據，或是標準，或是平面分析，它們並不完全不相關。

這些高級程式的調查問題，但是平面分析和程式分析方法，它們可以，它們應該能夠幫助彼此，因為我們要與它們連接，是人類的要求，是人類的經驗，所以這些底層問題，視覺觀察，或是程式分析和理論問題。

它們應該能夠幫助彼此，在這個特別的情況下，我們可以利用程式分析來告訴我們，最好的平面分析是哪個方式，就是物體的前列腺，然後你會能夠得到，對的平面分析，得到對的程式，然後你會有這種程式性的。

心臟循環循環的分析，然後你可以做的就是，你可以說，如果我往前走會發生什麼，所以，相較於完全自動循環的方式，這是在2020年，所以現在你可能能做得更好，但是，即使是非常長的程式，長期程式分析，你會看到。

你應該保持這個結構，而不是變得越來越模糊，而且再加上，不僅是預測，但還有描述，如果你站在這個廣場，但是你說我現在不往前走，我往後走，你會看到什麼，如果你往後走，你應該期望這個程式告訴你，好的。

這裡應該有光，繼續進來，如果你往後走，另一個光會繼續進來，因為所有的光是自己重複的，所以你期望沒有光進來，而不是在製造一個模糊的畫面，讓現實的畫面看起來更小，再加上，如果你想說，好的，發生什麼事。

我給你一個圖像，但是如果我往後走，如果我往後轉，我會看到什麼，如果我只看到廣場前面，你問我後面是什麼，當然我不知道，有無限的可能性，但是如果我選擇一個，我會說，是的，我可能在製造無限的廣場。

當然世界上沒有無限的廣場，但是如果我選擇一個，我會說，這看起來是我只能有的最有可能的解釋，所以我只能在無限的廣場製造，讓光影和陰影，所有的東西都繼續重複，而不是製造一個非常模糊的東西，好的。

你可以從這個廣場的例子，去到建築的例子，就是，一切都是一樣，除了在廣場中，你可以把它想像成一個盒子，你站在盒子裡，但是如果你在看建築，它是相同的盒子，但是你只是在看盒子，從外面，因此，相比廣場。

你只有一個，你只有一個消失點，如果你看建築的例子，你只有一個消失點，但是其他東西都是一樣，所以你可以做一個分析，就像我們之前所說的，但是，相比Dixon的方法，有些人可以保持結構非常好。

但是他們不太尊重輸入，有些人真的尊重輸入，但是重建看起來不太好，但是現在你可以，用結構方法做到兩者，最後我想說的是，你會說，現在我們有這種，流動方法，它們可以做得更好，這還是一個問題，所以我認為。

你會對這些，層面層面的方法，可以更有效地，融合在結構的表現上，例如，一個常見的問題，現在有很強的，圖形模式，尤其是3D的模式，是一個物體有多個頭，所以因為圖片中的數據，人類很可能會從狗的前面，拍攝照片。

所以每個狗的照片，都很可能會有一個頭，狗的頭朝著你，所以你會產生一個3D的狗，有多個頭，所以有某種結構的知識，希望能幫助你解決這個問題，我應該說，這真的是兩位精彩的學生做的。

雖然現在他們在做很不同的事情，但是我喜歡這個工作，所以我還是會談論這個，還有現在的柴維恩，現在的Ph。D。在MIT，還有在Stanford的Ikai，好，現在我想說一些更新的事情，我們談論了視覺數據的。

相關的問題，我們開始從圖片到自然影像，即是單一的影像學習，然後從單一的圖片，到多個圖片，接下來要做什麼？，你可以想像一下，我們在2D做的所有東西，都是單一的圖片，如果我們有多個圖片，就像是你有2D。

但是每個圖片都有一些面部的，你把它們合起來，所以它們的數字和細節都不一樣，這也不足以說是2。5D，但你還是能夠形成一個圍環，所以我會說這大概是2。5D，我會說是2。5D，所以接下來我們要轉到3D。

我想強調的就是，就像人類的觀感，2D和2。5D，通常都是視覺中心的，所有的場景都是場景，鏡頭總是在眼睛裡，同時，視覺系統也在眼睛裡，這就是視覺系統，但是我們談論3D，它通常有一個大變化。

就是你現在是在視覺中心，而視覺系統是自己的系統，你從完全不同的角度去看待視覺，因為原始的視覺，現在不是在眼睛裡，而是在視覺中心，但在具體的視覺中心，是非常有趣的，因為他們需要，鏡子和視覺。

需要一整個的變化，很多的方法或東西，我們之前談論過的代表性，在概念上，它們應該是可轉換的，但是他們之間有著深刻的差別，我認為這需要進一步研究，但是，在這個特別的情況下，我們可以看3D形狀。

它們通常有一個非常無限的，程式化的結構，這又是因為人類，我們嘗試製作這些物件，尤其是對於人類的元素，我們真的希望它們能夠正常，桌子的上半部分，椅子的缺口，我們希望它們能夠正常。

我們希望它們能夠有效地變化，但是我們也擔心，如果我們把椅子的缺口，做得不夠長，那椅子就不會穩定，我們希望它們能夠穩定，我們希望它們能夠便宜，能夠做得更好，所以我們有很多的考慮，讓我們建議這些形狀。

它們必須有這樣的結構，因為時間的關係，我不能說明我們如何做到，但我可以很快地展示結果，我們嘗試用學習方式，去做形狀，你會有能力進行程式化的形狀表現，我們非常受到啟發，我們在程式和數據圖形上，都在研究。

我們有很多的工作，如何使用程式模型，來做數據圖形，我們也在用新的網絡來進行，然後因為我們有限量的，程式改動，我們也有新的網絡，來做程式的運行，讓我們可以主要做自動應用訓練，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式。

我們也有很多的學習方式，我們也有很多的學習方式，我們也有很多的學習方式，好的，我們謝謝嘉峻的精彩分享，好的，我們謝謝嘉峻的精彩分享，然後我們趙州老師也請您去分享一下您的看法。

然後我們趙州老師也請您去分享一下您的看法，我同意朱老師和嘉峻說的，我同意朱老師和嘉峻說的，現在我們的生成模型可能會有一些生成的不自然的地方，現在我們的生成模型可能會有一些生成的不自然的地方。

現在我們的生成模型可能會有一些生成的不自然的地方，現在我們的生成模型可能會有一些生成的不自然的地方，我們可以來進行檢測，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大。

越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話。

那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，那麼隨著嘉峻說的模型越來越大，越來越逼真的話，謝謝李老師，是這樣子的，今天這個報告呢。

我們主要是分享了一些生成式模型的一些情況，我們主要是分享了一些生成式模型的一些情況，我們主要是分享了一些生成式模型的一些情況，我們是希望做生成式模型，做的它推理速度更快，它的模型size更小。

包括它的表現力更強，包括它的表現力更強，那麼我們看到我們今天可以不僅是生成視覺，那麼我們看到我們今天可以不僅是生成視覺，那麼我們看到我們今天可以不僅是生成視覺，我們也可以生成音頻這種不同的一些模態。

我們也可以生成音頻這種不同的一些模態，那麼從我的角度來進行出發，那麼從我的角度來進行出發，因為我一直是在做多模態，因為我一直是在做多模態，包括是做人機交互，那麼我是思考，那麼我是思考，那麼接下來做的是。

比如說我們現在做的基本上都是以生成式為主，比如說我們現在做的基本上都是以生成式為主，那麼接下來的話會往理解，那麼接下來的話會往理解，這種進行靠，這種進行靠，因為之前的話理解和生成，這兩個問題。

都是分別來解的，就是很多人都要不是解這種多模態的understanding，就是很多人都要不是解這種多模態的understanding，要不是解多模態的這種生成，那麼在這種大模型這種時代的下。

那麼在這種大模型這種時代的下，那麼在這種大模型這種時代的下，我們是希望是把理解，understanding和生成，一起放在一個model裡面，那麼也就是說我們，接觸的理解的模態，比如說以人機交互為例。

比如說以人機交互為例，我們輸入的話有talking face，我們輸入的話有talking face，有它的一些speech，有它的一些speech，等不同的一些模態，那麼在我們的理解上面。

那麼在我們的理解上面，是我們可以更好的做我們的生成，是我們可以更好的做我們的生成，那麼這個的話最後一個願景的話，是通過理解和生成，因為我們是做多模態人機交互，因為我們是做多模態人機交互。

那麼可以做一個更好的一個multimodal dialogue，那麼可以做一個更好的一個multimodal dialogue，那麼可以做一個更好的一個multimodal dialogue。

那麼使得我們的生成的，包括是和人進行交互的，包括是和人進行交互的，那麼更加的一個逼真，而且對人的一個黏性會更加的強，而且對人的一個黏性會更加的強，那麼這個是我覺得是多模態的生成模式。

那麼這個是我覺得是多模態的生成模式，那麼這個是我覺得是多模態的生成模式，在人機交互這麼一個領域，在人機交互這麼一個領域，接下來我們想做的一些事情，接下來我們想做的一些事情，謝謝，好的 謝謝趙州老師。

好的 謝謝趙州老師，好我們這個時間可能也差不多，還有最後最後的一個問題，然後我們這個問題可以請趙州老師先回答，然後我們依次，然後最後請朱老師做個總結的致辭，就是我們怎麼看待這個。

就現在我們繼續發展到現在，大模型啊有各種各樣的這種生成，diffusion的這種技術啊，那麼您覺得下一步就是生成模型，那麼您覺得下一步就是生成模型，如果再突破或者它未來的這個發展，最會讓你激動人心的。

這個點是什麼，雖然我們現在有非常多的一些像李老師說的，雖然我們現在有非常多的一些像李老師說的，雖然我們現在有非常多的一些像李老師說的，比如說是有大模型這個爆發，比如說是有大模型這個爆發。

包括有diffusion做的更加更加的，包括有diffusion做的更加更加的，一個真實這麼一個場景，一個真實這麼一個場景，那麼我們發現就是說，像AIGC的話，也可以在很多程度上，也可以在很多程度上。

是可以變革我們這樣的一個生活，是可以變革我們這樣的一個生活，可以給我們帶來很多場景的一個變化，比如說現在的人機交互，比如說現在的人機交互，那麼基於這種虛擬的人，那麼基於這種虛擬的人。

包括虛擬的世界包括虛擬的環境，這種人和人的一個交互，這種人和人的一個交互，那我們不僅是在物理中做的是已經很真實，那我們不僅是在物理中做的是已經很真實。

那我們在網絡環境中也是可以有一個非常真實的這麼一個真實的一個映射，那我們在網絡環境中也是可以有一個非常真實的這麼一個真實的一個映射，那麼這個我覺得接下來往這個方向去走。

那麼這個我覺得接下來往這個方向去走，那麼這個我覺得接下來往這個方向去走，應該是一個非常激動人心的很多一些問題，應該是一個非常激動人心的很多一些問題，應該是一個非常激動人心的很多一些問題。

但是我們還是要去想辦法去解決，但是我們還是要去想辦法去解決，有很多的一些新的場景可以來進行構思，那我是這麼一個觀點，那我是這麼一個觀點，謝謝趙老師，然後我們佳峻請您分享一下，吳教授請您分享一下您的觀點。

吳教授請您分享一下您的觀點，我只有兩點，一個是concrete，一個是concrete，就是短期內最有exciting是什麼，我覺得接下來很可能就是，我們可以做更多的一些新的工作。

還有就是我們可以做3D，那怎麼能更順好做3D，我們到底是data怎麼來，現在的data還是用video data，我們從prolific dreamer開始，從prolific dreamer開始。

能夠做得更好，做general scene 做dynamic，做articulation，我覺得這是非常exciting，long term來看，我還有一個很大的問題，也不是問題。

其實剛剛趙老師也說到一點，eventually最後，AI真的做得很好之後，它會變成一個HCI problem，就是你要human-computer interaction，我怎麼樣能夠interact。

現在2D已經有這個問題，我就是生成image但很難control，那eventually你要生成，不管是audio也好，cd也好，video也好。

怎麼樣能夠effectively做human interaction，然後去，這裡面需要怎麼是最好的handle，能夠bridge human和machine，我覺得還是有很多的。

當然這也是很多social science的一些問題，但這可能比較long term，好的，謝謝嘉峻，那麼最後請朱勛老師，做一個展望和總結吧，不能算什麼總結吧，我覺得剛才兩位老師講的，我完全同意。

回到這個問題裡，未來這個，學的比較exciting，或者是去做的，剛才兩位老師講的，我都完全同意，然後我想稍微補充一點，就是這個，特別是和嘉峻講的，就是interaction，我們現在是看人和算法。

人和機器之間的interaction，其實還有另外一個，就是未來的，比如我們做的動物模態也好，這種understanding，generation， reasoning，等等這些，這些能力達到一定的。

水平之後，其實它有另外一方向，就是可能會在機器人和實體上結合，我們現在在說，叫巨身的這種，這種形態，事實上它將來，我們看到的不光是一個，一個模型一個算法，它可能是一個實體的一個對象，它可以和環境和人。

和各個方面來進行交互和演進，我們現在在做的，就是在這個，巨身裡面當然有很多討論，包括在我們這個conference裡面，也有專門的session，會關於這個相關的，這一塊其實也有，近期的進展也比較多。

像這個，比如說關注像PAMI，這些進展等等，我覺得這可能是，未來，除了我們現在的這個，在一些模態或多個模態上生成之後，進一步的可能產生更大的，這種影響，如果一旦這個技術達到某種，能力之後，可能這個。

對我們整個的變化也比較大，這個我可能覺得是未來比較有意思，然後，剛才重新說讓我再說兩句，我就是，還是接著再感謝一下，感謝一下我們所有的嘉賓，因為這個我也算這個conference的。

program co-chair，也感謝重新和這個，建培教育，這個生動生動模型這個session，這個討論，這整個的這個，雖然沒有全部都聽完，但是我聽了這幾個報告裡面，都是非常的insight。

其實也是和我們這個，BI conference的一貫宗旨，是非常符合的，我們說要做這個，內行的專業的而且是高水平的，這種conference，所以其實要達到這個目標的話，就是依賴這個。

我們這些專家的精彩的報告，所以最後一句話，也感謝大家也感謝，在現場的這些觀眾，謝謝大家的支持，好謝謝朱老師，然後也謝謝各位，謝謝趙朱老師，還有嘉靖老師，我們今天這個論壇就到這兒，我們感謝各位觀眾的支持。

感谢观看，字幕由 Amara。org 社群提供。