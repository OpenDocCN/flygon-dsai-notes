# [2023北京智源大会]视觉与多模态大模型 - P1：[2023智源]视觉与多模态大模型 - Mercurialzs - BV1WN411k7T4

欢迎各位来到我们今天下午的视觉与动模态大模型论坛，然后我们今天论坛一共有四个报告，然后首先第一个报告呢是来自新加坡南洋理工大学的潘兴刚教授，然后在报告开始之前我先对潘教授做个简单介绍。

那个兴刚的他是隶属于MMLab NTU和SLab，他的研究方向是生成式人工智能和神经渲染，主要工作包括有Drag Gun和Deep Generative Prayer，Gun to Shape等。

在加入南洋理工之前的话，他是曾经是这个马克思普朗克计算机科学研究所，Christine教授组的博士和研究员，他在香港中文大学MMLab获得博士学位，是从这个汤晓鸥教授并在清华大学获得学士学位。

他今天带来的报告也是目前特别火的一个工作，然后叫Drag Your Gun，大家欢迎兴刚，[音乐]，谢谢主持人的介绍，各位来宾老师同学们大家好，很高兴在这里和大家分享我们近期的工作。

Drag Your Gun Interactive Point-Based Manipulation on the Generative Image Manifold，也就是拖拽你的Gun。

在生成图像流行上实现基于关键点的图像编辑，那么我是来自南洋理工大学的潘兴刚，这个工作也是和麻布索MIT 宾大 谷歌的合作者共同完成的，好 那这里我们关注的问题是图像编辑。

图像编辑在计算机视觉和图形学中都是一个很经典的问题了，那么近年来由于生成模型的发展，有一系列的图像编辑的方法被提出，例如基于监督学习的算法，如果用户标注了标签的话。

那么根据标签我们可以将图像向指定的属性的方向编辑，其次有基于语意图的编辑，那么用户可以通过重新绘制语意图来实现对图像轮廓的编辑，以及基于人体鲜艳的编辑。

那么通过一个dance pose或者skeleton作为输入，可以生成一个人的不同的姿态，近期由于扩散模型还有大语言模型的发展，基于文字的编辑逐渐成为了主流，那么这些编辑的方法。

他们在特定任务上都取得了非常大的进展，但是当我们在生成一个图片的时候，或者说编辑一个图片的时候，我们不仅希望编辑它的语意 它的内容，我们很多时候其实用户需要对它的空间属性进行编辑。

例如这个图片的轮廓 不同物体的位置，物体的姿态 朝向 物体的大小等等，那么对于空间属性的编辑，这些方法都受到不同程度的局限性，那么它们没有办法兼顾灵活性 准确性以及通用性，例如如果基于语意图编辑的话。

如果我想对其中人的姿态或者说车的朝向进行一个变化，那就意味着用户需要重新绘制这个语意图，那么这不是一件非常直观和容易的事情，再例如基于人的编辑没有办法拓展到其他类别的物体，然后基于文字的编辑虽然强大。

但是它对于空间属性的精确编辑仍然不是特别方便，比如说这只猫如果我希望它的头向左偏移十个像素的位置，那么基于文字的模型不太方便做到这一点，所以怎样的一个对空间属性的编辑是我们希望实现的呢。

那么如果我们观察人与物理世界的交互的话，如果一个桌子你希望去改变它的空间属性，那最直接的就是你直接去移动茶杯的位置或者移动物体的位置，那么我们和手机交互也是我们直接通过手指去拖拽手机。

那么另一个通过这种直接交互的方式来实现，图像编辑的一个例子就是皮影戏，那么在皮影戏中操纵者对于这个图像中的物体是有完全的控制权的，你只要去指定它的关键点移动到哪个位置。

那么根据这个物体本身的结构它就会发生相应的动作，那么我们有没有可能像控制皮影戏这样去控制图片呢，相关的一些应用的话，一个是Photoshop中的液化功能，它也可以让用户实现一些拖拽的编辑。

但是可以看到这里它的拖拽只是一些简单的2D形变，并不考虑物体本身的结构，并且它没有办法生成新的内容，没有办法将被遮挡的部分生成出来，或者将嘴巴张开，那么类似的基于拖拽的编辑。

其实在图形学中有个经典的问题就是，Shape deformation或者说Geometry processing，那么这里展示的是一个经典的方法。

As rigid as possible shape manipulation，它是把物体网格化，然后假设这个物体具有一致的刚度，然后来实现形变，那么可以看到这种方法，它同样，假设物体有一致的刚度。

那就不符合物体本身的结构了，因为很多时候物体是有骨架的，以及它仍然没有办法生成新的内容，那么理想情况下，我们希望这样的编辑符合物体的结构，并且能够想象出被遮挡的内容，怎样可以做到这一点呢。

那么能够对物体结构有感知力，并且能够产生新内容的模型，自然就是生成模型，那么在之前一个经典的生成模型，就是对抗生成网络，Generative Adversarial Networks，或者说GAN。

那么在训练完成后，GAN做的事情其实很简洁，它是将一个符合高斯分布的，512维的影像量，映射到一个高维的图片，那么通过对影像量进行一些扰动或者变化，就可以实现对图像中内容的变化，那么在这个工作中。

我们想做的事情，就是将这种拖拽式的编辑，基于GAN来实现，那么对于一个GAN所生成的图像，我们希望用户只要指定关键点的移动，我们就可以让图片中的内容，发生对应的变化，那么为了实现这种效果。

一个核心的问题就是，我们如何去改变GAN的影像量，来实现所需要的这种，基于关键点的变化，那么在介绍方法前，我们可以简单看一下，最终所呈现的效果，那么用户只需要指定，红色抓取点和蓝色目标点。

算法就会将抓取点朝目标点移动，实现最后比较自然的生成效果，那么可以看到这个变化，是符合物体自身的结构，并且可以想象出被遮挡部分的内容的，那么为了实现这一点，我们设计了一个迭代式的算法框架。

这里展示的是一个GAN的生成器，将影像量W映射成为一张图片，那么这里是一个狮子的图片，然后用户只需要输入，红色的抓取点和蓝色的目标点，那么为了将红色抓取点，移动向蓝色目标点，我们需要给它施加一个力。

将它朝蓝点推，那么这样一个力就可以，formulate成一个Motion Supervision Loss，也就是运动监督损失函数，那么通过这样的运动监督损失函数，将红点推向蓝点。

这个损失函数用来优化GAN的影像量W，在优化一步后我们得到了一个新的影像量，WEP，那么新的影像量会产生一个新的图片，那么这个新的图片已经按照我们，推动关键点的方式去移动了一下，那么在它移动之后。

其实我们还暂时不知道它移动到了哪里，所以下一步我们需要做点追踪，也就是去更新红色的抓取点的位置，让它跟随图像一起移动，比如说这里这个红点最开始是在，上面红点在鼻子的位置。

那我应该要让它一直跟随着鼻子的位置一起走，只有这样最终才能准确地将它，移动到蓝点的位置，那么在这之后我们再重复以上的步骤，进行运动监督以及点跟踪，直到最终所有的抓取点，都移动到了它对应的目标点。

那么接下来我们就介绍，这其中的两个关键的子问题，一个是运动监督，一个是点追踪，那么其实在设计最终的方法之前，我们进行了一些不一样的初步的探索，那么我们最初的想法，其实和最终的实现方式不太一样。

最初为了实现对运动的监督，我们想到的是使用光流模型，因为光流是最直观的对于运动的汲取，那么我们的做法是这样，对于一个干索生成的图片，我们先复制这张图片作为一个参考图，那么将这两张图片。

另外我们还有用户输入的关键点，将两张图片送给光流模型，去预测两张图片之间的光流，那么由于这里两张图片是一样的，那么它的光流自然就是零了，那为了实现对关键点的运动的驱动，我们其实目标是让它。

这个关键点对应位置的光流不是零，所以我们就用目标的光流，去监督这个关键点对应位置的光流，比如说我希望它是朝左移动，那么它理应输出的光流应该是负一零，所以我们把负一零作为这个ground truth。

去监督这个关键点的光流，ground truth去监督光流所预测的结果，那么这样就得到一个运动监督损失函数，我们用它去优化GAN的影像量，那么这种方法是可行的，通过优化我们确实可以让。

光流所输出的结果符合我们的预期，也就是说关键点的位置朝左移动了一点，那么但是这样做的一个问题是，我们所用到的光流模型是一个经典的模型叫RAFT，那么它是一个迭代计算的模型，计算开销比较大。

如果说我们在编辑的过程中，一直使用这样一个光流模型的话，会大大降低运算的效率，那么我们作为一个交互式编辑的方法，希望它能够给用户及时地反馈，所以我们在想能不能让速度更快一点，那我们大致的想法是。

这里之所以要用到光流模型，是因为我为了驱动运动，需要提取出对图像空间位置敏感的特征，那么对于GAN来说，它在生成一张图片的过程中，它生成的不仅仅是这一张图片，还有在生成这个图片过程中。

这个生成器内部的一系列特征，那么之前的一些方法其实分析过，GAN它的中间特征其实对于空间位置，有很强的判别力或者说很敏感，那么一些算法展示了，它可以用来做Fill Shot Segmentation。

但是其实我们这里想说的是，它和图像内容的Dense Correspondence，密集对应关系有很强的偶合性，因此运动监督可以直接，基于GAN的特征来实现，那么这里介绍我们最终的方法，其实非常简洁。

那么这里这个方块就是，GAN生成图像过程中的特征了，为了将红点移向蓝点，我们的想法是观察这个红点附近的，一个小范围的Patch，有这个小的红色的圆，为了将它移到蓝点，我们希望它先走一小步。

走到蓝色的Patch的位置，有蓝色的小圆圈的位置，那么这就可以formulate成一个简单的Loss，也就是说我去优化蓝色位置的feature，让它的值接近红色feature的值。

也就是以红色feature为Ground Truth，来优化蓝色feature，那么当蓝色位置的数值，变成红色位置的时候，其实就是红色的圈圈，移动到了蓝色圈圈的位置，那么这里要注意的是。

为了使这种拖拽是单向的，所以我们需要将红色圈圈，从反向传播的计算图中分离出来，也就是一个Detach的操作，那么这样的运动监督的损失函数，可以用来优化影像量，其实这里我们还让用户。

可以选择性地输入一个Mask，因为用户在编辑图片的时候，可能有一些区域是他想要动的，有一些区域他希望保持不动，比如说背景的部分，那这里这个图片中可以看到一个，白色的圆圈，这个就是用户指定的可动区域。

那么为了限制背景区域不动，我们在设计了一个，在特征空间的重建Loss，也就是这个特征中的阴影部分，让它与最初的特征保持一致，好，那通过这两个Loss进行优化之后，我们得到了一个新的影像量。

新的影像量生成了新的图片和新的特征，那么下一步问题就是如何更新，抓取点的位置来跟踪图像的内容，关于关键点的跟踪，其实也有一些相关的研究了，那么我们其实之前也尝试了一些，不一样的方法。

并且有一些专门做关键点跟踪的方法，但是在这个工作中，其实我们发现，并不需要引入额外的神经网络，因为Gantt Feature就像刚才提到的，它对于空间位置有很强的敏感性，因此关键点跟踪也可以直观地通过。

Feature Matching，这个特征匹配的方式来实现，具体来说在这个新的Feature中，我们去关注这个红点附近的一小块范围，这个红色方框的范围，我们在其中去寻找那个。

和最初的抓取点所对应的特征值，最近的那个像素的位置，也就是一个Nearest Neighbor Search，那么这样找到的一个关键点，就是对应于最初的那个关键点，所对应的语义的那个点。

也就是说它实现了一个跟踪的功能，那么最初的关键点是在鼻子上的，那么通过Feature Matching找到的关键点，也会是在这个狮子的鼻子上，所以通过这两个简单的设计，我们实现了运动监督和点跟踪。

那么再结合前面所介绍的，迭代式的算法框架，Drag-Gaunt算法就实现了，那么通过这种交互的方式，我们可以对物体的很多不同的，空间属性进行编辑，例如张开嘴或者将耳朵提起来，或者让这个狗坐下来。

或者让它的头转一下，或者重新设计一辆车子的外形，可能汽车外形设计者会喜欢这个功能，或者编辑各种各样的其他的动物，或者让一个猫只睁一只眼睛，那么这是在训练数据之外的一种结果，或者对人脸的编辑。

那么瘦脸或者改变头发，都可以比较直观地实现，或者对模特的衣服的编辑，或者是任何奇怪的物体，比如微生物，对风景的编辑，以及让太阳升起来等等，那么我们做的过程中，其实出现了一个同期的工作。

叫User-Controllable LTE，或者User-Controllable Latent Transformer，那么它所实现的目标其实非常相似，但是这里它有两个重要的局限。

一个是它对于位置的精确控制，其实不够精确，因为它只能粗略地移动物体，并没有办法把关键点，精确地移动到目标点的位置，其次是它对于多点的支持并不好，另外它也没有办法去，只编辑图片的一部分区域。

而保持背景不变，那么我们的方法可以，更好地保持背景不变，以及实现更精确的控制，这里展示了更多对比，那么第一行是输入的图片，还有用户的编辑，那么下面两行是，User-Controllable LTE。

和我们方法的结果，那么其实从第一个例子，马的例子就可以看出，它们对于多点的编辑并不好，那么我们方法可以，对于马尾巴 腿 还有头，都移动到对应的位置，我们方法同样可以，编辑更加密集的关键点。

比如说对于人脸的，比较密集的关键点的编辑，这里每一个例子里面，左边两个展示的是，输入图片和目标图片，那么这里的目的就是，把输入图片这个人脸的关键点，挪动到和目标图片的关键点，保持一致，所以可以看到这个。

输入图的表情，还有脸的轮廓，都变成了和目标点差不多，那么这里我们进行了，一个定量实验，我们计算我们的方法，所编辑达到的目标点的位置，以及目标图像本身的，目标点的位置之间的误差，那么可以看到。

我们方法的误差显著，由于基准的方法，UserController。LT，我们也和其他的，点跟踪的方法进行了对比，那么可以看到，基于RAFT的跟踪，以及基于PEEPS的跟踪方法，都是弱于我们的方法。

另外我们进一步进行了一个，匹配图像重建的定量实验，那么对于一个GAN所生成的图片A，我们对于影像量进行扰动，随机得到另一个图片B，那么B相比较A发生了一点变化，我们计算它们之间的光流。

在光流图上抽取32个稀疏的点，然后将图A和这个稀疏的光流，作为我们的Point Based Manipulation方法的输入，那么这里可以用来测试我们的方法，或者其他的方法，得到一个对B的估计。

那么这样的话，我们通过比较对B图的估计，以及真实的B图，就可以得到一个误差，这个误差可以用于衡量方法的好坏，那么同样我们的方法，也是比其他的方法取得了明显的提升，那么这里对比了点跟踪的结果。

可以看到左边是输入的图片，还有用户的编辑，那么右边展示了编辑过程中，点跟踪的过程，第一列展示了我们的方法，那可以看到这个红点，抓取点是一直在狮子的，鼻子上方一点点的位置，但是其他的点跟踪的方法。

它们可能在编辑过程中，逐渐出现了一些偏移，那么一旦出现了偏移，它最终就没有办法准确地将，抓取点移动到目标点的位置，所以对于生成模型，我们也提供了一种，进行点跟踪的一个新的思路，那么对于以上展示的结果。

其实基本上都是GAN所生成的，这个合成的图片的编辑，但是在实际应用中，其实我们关注的是，对真实图片的编辑，那用GAN编辑真实图片，就需要进行一个额外的操作，也就是GAN重建。

我们去优化这个GAN的影像量，来实现对一个用户输入的图片的重建，这样就可以进行后续的编辑，那这里展示的是一个人脸的编辑效果，可以改变表情 朝向，或者头发，再多一点，或者让眼睛睁大一点，诸如此类。

那么这里展示了更多真实图片编辑的结果，那么这里基于关键点拖拽的编辑，很多时候是存在多解的，为了让狗实现头朝右移动一点，可以有不同的方式，狗可以整体移动，狗可以身体旋转，狗可以直改变头，那么在这个算法中。

如果不施加额外的限制，它会寻找实现这种目标最近的节，那么在这里右侧的结果就是，狗的一个旋转的效果，那么为了减少这种歧义，用户也可以进行一些额外的限制，比如输入这个Mask Yema，来指定可动的区域。

这样就可以只让狗的头的部分动，此外我们也展示了一些，超出数据级范围之外的一些编辑效果，比如说让这个狮子张开一个血盆大口，或者让车的轮胎大到夸张，那么也可以实现一些符合预期的效果。

但是与之伴随的也会带来一些畸变等问题，那么对于GAN来说，它的影空间有两种选择，两种不同的选择，一种是W，一种是W+，那么W+它的空间会更大一些，也就是说图像的编辑会允许更多的灵活性。

那么在W+中间编辑的话，可以更好地实现一些，超出训练数据之外的效果，例如说这种睁一只眼闭一只眼的情况，那么最后来分析一下方法的局限性吧，那么前面起了一些超出训练数据分布之外的一些编辑。

那么当这种超出的范围过大的时候，会很容易产生一些失真的效果，比如说这里这个人体的模型是在模特的数据上训练的，那么它很少会有一些夸张的动作，比如说手或者腿张得非常大，那么如果我们进行这种编辑的话。

会容易产生一些畸变或者失真模糊，此外对关键点的选取也有一点讲究，那么它和这个关键点的纹理的丰富度有关，如果说是在非常平滑的部分选的关键点，例如这个车的车门的位置的话，那么在拖动的过程中。

它会更容易发生一些偏移，就点跟踪的偏移，但是如果选在纹理丰富的位置，比如说车窗，那么它就可以更好的实现跟踪，但是我觉得最重要的一个局限性，其实是对真实物体的编辑，这也是可能需要澄清的一点吧。

因为可能之前这个方法的视频在网上流传的时候，大家可能觉得类似的效果，它输入任意图片就可以做到，但是目前还不是这样，因为我们所展示的真实图片编辑，更多的是有一个明确的主体物体，并且背景不是那么的复杂。

但是如果你是这样的一个有非常多物体，或者背景非常复杂的图片的话，但重建就很难保证质量，那么编辑的质量也会大大降低，这也是后续研究的一个重要的拓展方向，那么最后总结一下。

我们提出了一种能够像控制皮影戏那样，去控制图片的方法，那么通过关键点的目标位置的指定，就可以实现对物体空间属性的编辑，那么为了实现这一点，我们提出了两个模块，运动监督和点跟踪，那么它们都是基于。

GAN本身对空间属性具有判别力的特征而实现的，那么最后可能大家会想，现在基于文字的编辑这么火，那么这种拖拽式的编辑它的地位怎样，它是哪一种编辑会成为主流呢，那其实我觉得这两者是互补的。

因为你通过文字很难直观地控制这些空间属性，那么空间属性也没有办法做文字能做的一些对内容的编辑，所以我觉得最终的AI生成图片的框架，应该是将文字还有这种对空间拖拽的编辑，都囊括在其中的一个系统。

那么所以往后的话，在这个方向还有很多可以拓展的方向，如果说有同学对这个方向感兴趣的话，其实我们最近也在招生，欢迎大家来申请，好的 我的介绍就到这里，欢迎大家提问，(掌声)，好问题。

这个问题其实被问到很多次了，首先因为这个拖拽式编辑的问题之前研究非常少，但是当你发现这个问题的时候，它非常符合直觉 非常重要，所以我觉得路是一步一步走的，这是为什么我们最开始选择了基于GAN来进行研究。

因为GAN它的这种影空间对于图像内容容易编辑的这种属性，使得GAN会作为这个问题的一个很好的开始的研究对象，但是显然Diffusion的上限是更高的，所以下一步就是将这个方法迁移到Diffusion上。

那由于Diffusion和GAN它在生成图像的机制非常不同，所以你没有办法将DragGAN的方法，通过简单直接的迁移就在Diffusion上实现，但是其中的一些思想可能是可以借鉴的。

所以我认为这件事情是可行的，但是不直接需要学者们接下来的努力，谢谢，(掌声)，(音乐)，好 其实就像你说的把嘴巴张大，它其实有两种不同的解法，一个解法是张开嘴，另一个解法是放大这个图像。

其实你也可以实现一个张大效果，这就是我刚才说的编辑的奇异性问题，因为你很多时候是有多解的，那首先第一是这个模型会按照最容易实现，这种编辑效果的方式来达到这个目的。

那么最容易实现这个效果的方式是取决于什么呢，其实很多时候取决于数据的分布，因为可能数据里面它张大嘴的情况，比它这个脸靠镜头非常近的情况会多一些，那它就会选择这种方法去解，但是如果你想进一步限制它。

是通过张大嘴而不是通过放大的方式来实现的话，你可以引入更多约束，比如说你让它身体的部分固定住，或者背景的部分固定住，那这时候它就不会通过放大的方式来实现，而是通过把嘴张开的方式实现，对，你好，大体量。

你好 谢谢，就是GAN的训练往往来说都是我们知道是不太稳定的，然后在这种情况下，我们体量这么大还有模型这么大的情况下，我们有没有对它的不稳定性训练的问题做一些优化吗。

我觉得近期应该有一些工作去研究这个问题了，就是朱军燕他们团队包括其他的团队，研究了一些基于语言模型的比较大的GAN，在很多类别上的物体训练，所以他们对于稳定的训练提出了一些新的改良。

我觉得这个是另一个问题了，谢谢，就在MASK和MASK的边界上的东西，很多时候会产生一些比较违和的一个情况，我想一下这个工作里面会有没有出现过这个情况呢，或者说在那个狮子张口的时候。

有没有可能在这个区域出现一些让人觉得比较奇怪的噪点呢，就是在边界的柔和方面的问题，你是说MASK的边界处可能出现一些不自然的过渡吗，对对对，因为你那个做的那个Loss。

是针对MASK区间做了一个Loss，然后边界和边界外可能会有一些不柔和的过渡，有没有可能会出现这样的情况呢，在我们的实验中没有看到太多这种情况，因为对于GAN模型来说。

图片的整体都是由一个compact的影像量来控制的，所以它会倾向于生成整体比较和谐的结果，那我觉得这里其实更重要的一个问题，反而是编辑的过程中MASK之外的背景也会发生一点小的改变，对。

那我觉得这是GAN模型设计上的一个问题了，你可以通过其他的方式去让背景固定的，更加固定更不容易变化，比如说通过一些特征融合的方式来实现，但可能这是一个tradeoff吧。

特征融合可能更容易引入一些不自然的过渡，还有一个问题就是，我之前在做defusion影空间的一个操作的时候，它很有可能会出现一些奇怪的造成，因为它这个影空间可能之前训练的时候没有见过，然后您刚才也提到。

在数据集之外可能会出现一个比较明显的造成，那就是在正常的情况下，有没有可能会出现一些比较奇怪的，就是棋盘状这样的造成呢，有没有可能会出现这样的一个情况，对于GAN来说，我们没有观察到很多棋盘状的问题。

对，这也是可能GAN相对于defusion，做这一个任务的优越性体现吗，你可以这么说吧，谢谢，是的，因为defusion它其实还是一个tradeoff，虽然说它不容易受到这种artifacts的影响。

但是它的性能比defusion还是差了一些，谢谢，潘老师好，这个工作非常有意思，就是我们看到这边工作主要的内容是，可以在图像上通过关键点的拖拽来改变图像的外形，那我们有没有考虑除了在二维平面上。

就是在三维空间上做一些拖拽呢，比如说我们对图像获得它的深度信息，那么我拖在这个点的时候，我可以不局限在这个平面上，我可以在这个图像的纵向，这一种方向去拖动它，有做过这方面的一些可视化的结果吗，对。

这是一个很好的点，那么首先我们的baseline方法，user-controllable-lt，它在算法设计的时候，其实引入了这样的机制，纵向的深度的编辑，它可以放大或者缩小，那么其次。

网上已经有把dragGAN和3DGAN结合的一些第三方的代码了，那么，它可以对3D形状进行一些编辑，谢谢庞老师，因为时间问题我们就差不多了，然后的话，就是我们一会儿所有嘉宾讲完之后。

我们还会有一个panel环节，我们会有些议题讨论，然后当时候也会邀请一些同学来，老师或者来提问，好吧，谢谢这个新刚的报告，我在下面听也是非常的酷炫，非常酷炫，然后讲的也非常的清晰，然后我们下一个报告呢。

是一个线上的报告，然后是来自多伦多大学的高俊，高俊呢，他是多伦多大学的PhD，然后以及NVIDIA的Research Scientist，他的研究方向的是三维计算，三维计算机视觉以及图形学。

主要关注的是机器学习，在LargeScale 3D上的生成方向的一个应用，他的代表工作有很多，有这个GED 3D，Magic 3D，DeepThread等等，然后其中很多已经被集成在。

这个NVIDIA的产品当中，包括NVIDIA的Picasso，GAN V3D，Neural Drive Sim，以及Toronto Annotation Suite。

他也是今年NeurIPS2023的领域主席，然后这个相信高俊已经在线上了，对吧，OK，好 那我们就欢迎高俊的报告，他的报告题目是。

Machine Learning for 3D Content Creation，OK 行，谢谢各位老师，然后也欢迎大家来听我的报告，然后因为不好意思，因为我现在人在加拿大。

所以就只能在远程上给大家给talk，所以非常抱歉，如果大家有什么问题呢，都欢迎大家在那个给完talk之后，或者是给talk中间给提问，然后今天我要报告的，因为今天的这个论坛主要是探究的事情。

是视觉与多模态大模型，那我就跟大家来介绍一下，大模型在这种三维内容生存当中的一些应用场景，以及我们group所做的一些相关的工作，首先第一个问题我们需要去回答的事情是。

为什么我们要做一个三维视觉上的一个model，在我看来一个最简单的一个回答，就是说我们人类生活在一个三维的世界，而创造一个三维的数字世界，或者说一个三维的虚拟世界，不但可以帮助我们更好地去理解这个世界。

同时也可以帮助我们去解决，很多现实生活当中所无法解决的问题，在一个三维的虚拟世界当中呢，我们可以进行交流或者互动，即便说在现实生活当中，比如说人与人之间相隔千里，比如说像现在我们可能是。

只能通过zoom去meeting，但如果说有一个虚拟世界，我们就可以不用通过zoom了，创建虚拟世界同时也能够带来，很多的其他的应用场景，比如说是robotics或者是self-driving。

我们经常很难能够在现实生活当中，去训练一个机器人或者是无人车，并且现实生活当中的long-tail distribution，很难通过去采集数据去capture，而创建一个三维的虚拟世界。

可以帮助我们搭建一个更好的simulator，从而让我们训练机器人或者是无人车，尤其是在一些跟安全相关的，一些重要的场景上面，在其他应用场景当中，创建三维虚拟世界，也经过了几个世纪的发展，比如说是电影。

在一个虚拟世界当中，创作人可以去创作很多，他们想创建，但是现实生活当中又很难去创建的场景，比如说像我们看的《游荡地球》里面，电影里面月球的场景，在游戏里面3D的虚拟世界。

可以帮助大家去在游戏当中进行娱乐，同时就在这个星期，苹果所发布的vision pro，也可以通过虚拟现实或者是增强现实，来帮助人们进行更好的工作，以及交流，那么在所有的这些应用场景里面。

其中最重要的一个component，在我们看来是一个三维的内容的生成，具体来讲的话，如果说我们需要去搭建一个大范围的，一个比如说三维的虚拟场景，比如说是无人车的虚拟场景，那么我们就需要有很多。

我们在现实生活当中，所能够看到的行人，房子 车子 花草 树木等等，等等普通的物种，这更具体来讲的话，我们其实希望的三维内容生成，在三个方面进行scale up，这首先第一个方面需要做的scale up。

是我们希望在，希望能够生成大量的不同种的物体，比如说是像大家现在在的北京，在北京的早高峰的时候，马路上有成千上万辆不同的车，他们high level而言都是车，但是细节上。

在细节上每辆车又有它自己的不同，第二个点的话是多样性，我们希望能够生成不一样的物体，比如说是动物，这个地球上有成千上万种，不同的动物的物种，而我们除了有动物之外，还有植物，还有很多我们人类。

自己所创造出来的物体，第三个需要scale up的事情是质量，就是说我们期待有，能够有一个高质量的生成，这边高质量，包括说是我们需要有，高质量的物体的几何信息，以及高质量的纹理信息。

但同时相反咱们来看一看，现在工业界里面，大家常见的一个三维内容，创建是一个什么样的一个工作流程，然后这个视频其实就是一个，非常简单的一个工作流程的，一个视频，我们可以发现。

就是说在现在的这样的一个工业界里面，创作一个三维的内容，是需要消耗大量的能力物理，去做这么一件事的，而且它并不是每个人都能做这件事，它需要你对，比如说你对这个软件的理解，并且你自己得要有一些。

这种艺术建模的能力，而这个并不是每个人，都能够拥有的一些能力，因此，我们仅仅靠人工去进行三维内容创作，是很难去scale up的，同时咱们也再看一看，比如说最近这几年，机器学习的发展，尤其我们可以发现。

机器学习在language，以及2D image上，具有非常迅猛的发展，比如说language，大家都听说知道很多，不是都用过的GPT或者是GPT-4，然后在二维图片生成领域。

我们同时也看到了像imageGen，Dali-2 stable diffusion，Middle-early等等这样的工作，他们其实就能够允许人们，去通过一些text prompts。

或者是一些sketch mask等等，这些input去生成非常高质量的，2D的图片，但是咱们也同样看看，这种机器学习在三维内容创建当中的，一些进展的话，然后这个结果是Dream Fusion。

如果熟悉三维视觉的话，其实会知道Dream Fusion，其实是最近这一年非常颠覆性的，一个发展，而且它也能够拿到了，今年IKEA的best paper，我们可以发现它的效果，其实已经很不错了。

比如说像在左边，大家看到的这个结果，但是如果说咱们把它拉近一点，咱们看看它所能够生成的具体细节，我们就可以发现在细节上，就是现在三维内容生成的，这种performance还是远不如。

前面所给大家展示的image的图片生成，或者是language上的GPT，然后这边主要差的两个点，就是geometry和texture的quality，并不是特别好，因此我自己的PhD期间的主要工作。

其实是探讨的问题，就是说我们如何去利用机器学习，来进行帮助我们去更好的生成三维的内容，并且我们希望能够生成高质量的三维的shape，他希望我们期待有高质量的几何信息，同时也能够尽可能的有逼真的纹理信息。

比如说像这个slides的左边所展示的，所有的物体都是我们AI model所生成的，而并不是人工去创建的一个三维的model，最后我们也希望我们所生成的三维的模型。

能够被直接运用到一些graphics tool里面，就是一些图形学的软件里面，比如说是blender， maya等等，因为这样子就可以直接帮助到大家，直接去进行创作，OK，这个问题是一个很难的问题。

就是用机器学习来做三维内容生成，那么我们该怎么样去探索这么一个问题呢，我们的思路是这样子的，在我们去探索这个问题之前，咱们先zoom out一下，就是从一个第三者的角度上来看。

这个三维内容生成它的pipeline是什么，因为当我们理解了这个pipeline，它可以帮助我们去design一些，或者是它就会告诉我们，到底需要去解决哪些问题，一个通常来讲的话。

一个三维内容生成的pipeline，大概是这样子的，我们可以有些input，这个input可以是从一个product distribution里面，sample出来的一个layer code。

比如说你是一个生成式模型，它也可以是一个single image，或者是multi-view image，比如说你要做single view 3D contraction。

或者是multi-view 3D contraction，然后你也可以是一个text prompt，比如说你想做什么text 3D generation。

然后我们有一个machine learning model，这个machine learning model，它会encode大家所提供的这个input，然后会decode出来一些三维的shape。

它可以是什么表示方式，它可以是match，可以是point cloud，也可以是implicit function，然后我们会把这种生成的结果，用到一些application里面去。

比如说像simulation，或者是robotics啊，或者是游戏当中，那么我们来看一下，这样的一个pipeline的话，它其实告诉我们，有这么几个比较大的一个挑战，首先第一个挑战。

在我们看来是一个三维的表示，具体来讲，它其实就是说，我们该如何去表示，我们所生成出来的，这样的一个三维的物体，为什么它是一个比较难的问题呢，是因为它不但影响着，我们如何去设计我们的这个网络结构。

该长成什么样子，同时呢，它也影响着我们如何去，把我们生成的结果，运用到一些下游的场景当中，因此一个比较好的三维表示，就应该，首先呢，它得非常适合机器学习，因为我们想用机器学习。

去生成这样的一个三维的物体，然后第二点呢，是它得非常，它同时也得非常在，适合我们在下游的任务当中，去进行一些应用，因为这样子的话，我们的输出，就可以直接被放到一些，下游场景里面去了，同时呢。

它也希望能够支持，不同的topology，因为每个shapes，它的topology可能不一样，然后呢，它也希望能够支持，比如texture和以及materials，因为呢，我们为什么想做这两点呢。

因为我们希望，我们生成的shape，是能够有高质量的shape，如果说你的topology，只是一个fixed topology，那么你的质量就很难去上升上去了，OK，这个是在一个三维表上。

我们需要达到的这么一个，一个要求，同时呢，另外一个挑战是，在我们看来是在一个，在算法层面，首先这边包括这么几个问题，首先呢，我们就应该去思考，如何去搭建一个三维生成模型，并且能够高效的训练它。

使得它能够生成，高质量的三维的内容，这是我们需要思考的第一个问题，同时呢第二个问题是说，我们来仔细的想一想，现在其实SWEDY有一个，很大很大的一个问题，是说SWEDY data非常非常少。

我们可能比如说像ShapeNet，它就只有五万个shape，然后最新release，Optiverse，它就已经很大了，但它只有八十万万个shape，但是相比于RD的data，比如说像Lion。

Lion有五个billion，就是五十亿个，五十亿个data，这其实是有一个，是上千倍的数量级之间的一个差别，那么其实我们也应该思考一个问题，就是说当我们在训练三维生成模型。

尤其是我们需要scale up的时候，我们该如何去运用到那些，更加常见的RD的data，来帮助我们去逃出，一个在三维生成模型里，这种数据的一个约束，然后第三个问题，其实跟潘老师之前讲的，也有点关系。

就是说当我们有了一个三维生成模型之后，我们该如何去控制它，我们该如何去，让这个三维生成模型来生成，我们自己真正想要的一个，三维的内容，然后我自己Pagetree，之前的所有的工作。

基本上都是在cover这两个，这两个challenge，然后今天由于时间关系，我就给大家主要介绍三篇工作，就比较简约的介绍三篇工作，首先第一篇工作是DMPath，它其实主要解决的是三维表示的问题。

然后该3D它所解决的问题是，它所回答的问题是，我们如何去design并且train一个，一个比较efficient的三维生成模型，然后Magic3其实想告诉我们，是我们如何去leverage一个。

RD的data帮助我们去生成三维的shape，好 我们先看一个，第一个问题就是3D三维表示的一个问题，在最近这几年当中，基于inplicit function，就是隐含数的方式来表示三维的物体。

其实有非常好的进展，比如说像deep SDF，occupancy network或者是Nerf，它们非常非常适合进行学习，因为它们是在一个三维的field里面，去连续的定义了一个场。

然后你用一个神经网络去逼近这个场，其实你就会非常容易的去训练，这样的一个神经网络去逼近它，因为它没有任何的那种，比如说像discrete operation，就是那种离散化的操作。

然后同时它可以表示非常非常复杂的geometry，比如说像Nerf里面，它可以表示一个非常非常高精度的一个卡车，同时它也允许我们做这种tabological change，就是说在在拓扣上的一些改变。

比如说我想从一个球变成一个茶壶，它都是可以做的，它非常非常nice，同时呢咱们也看一看一些，在一些下游的应用场景吧，比如说是像实时渲染，real-time rendering。

shape deformation，或者是物理模拟这些场景里面，在这些场景里面呢，其实Mesh其实是被大家所用的非常，用的比较广的，原因事情是因为Mesh其实可以给我们，带来很多很多的benefits。

如果说我们用Mesh来做一个表示方式的话，这里面benefits主要有这么几点，首先呢Mesh它非常适合去做rendering，它render非常非常快。

比如说我如果想给Mesh做Rasterization based rendering，其实它这个已经是实时的，比如说像那个Blender的Rasterization based rendering。

如果说你想做Retracing based rendering，for Mesh它也可以做到实时的，比如说像最新的很多GPU，它们就支持real-time rendering。

real-time retracing，同时呢Mesh它也，方便我们去定义很多在Surface上的properties，比如说是像Surface上的BRDF的properties。

这样你就可以render，带Material的Mesh的，比如说我如果换一束光，我把光改变了之后，我的Mesh就会有很多的这种光影的效果，同时呢在Mesh上，我们也可以方便去定义很多的。

类似于Normal Map呀，或者是Deformation Map等等这样的一个，Mesh表面的属性，同时呢Mesh也更加intuitive，帮助我们去做editing或者是deformation。

比如说像As Rigid As Possible里面，所做的那些Shape Editing呀，或者是做Physical Simulation里面，比较常用的Neon Hooking Solid。

去用一个Tetrahedron Mesh来表示一个，三维的物体，但是如果说，但是呢一个基于Neuron Field的表示方法，是很难去运用到这项的，就是来自于Mesh的这项的。

这么多的Advantages，就是它的优势，这边的原因事情是这样子的，当我们想把一个Implicit Function，跟Mesh来建立联系的时候，我们其实经常用的一个操作，相信大家经常用的一个操作。

是叫做Machine Cube，但是呢Machine Cube本身是一个，Machine Cube本身是一个不可导的一个操作，因为它里面有很多的，很多的离散的操作，这个不可导其实就意味着说。

如果说我们在这样的一些Downstream Application里面，它有一些Loss Function，或者说我们觉得这个生成的效果不好。

我们从Downstream Application告诉我们说，我们生成这个Shape在这个角度，它看着不对，那我该怎么Backward Slew。

这样的一个Non-Differentiable Operation，到Implicit Function里面，这个是很难做的，因此呢它就导致了，这个Implicit Function就有这么一些。

比较大的一些缺点，比如说它很难做Reality，你需要去重新Reformulate这些Random Equation，它也很难做Shadow，它也很难做Editing。

并且大多数它所表示的是一个Static Scene，而并不是一个Dynamic Scene，OK 但是我们其实就像我最开始讲的，Implicit Function它有很多很多它自己的优势。

它其实是很适合机器学习的，那么我们该怎么把Mesh的优势，以及那个Implicit Function的优势，给它结合到一块呢，并且解决这样一个Non-Differentiable的问题呢。

我们的一个核心的观点是这样子的，我们需要做的事情是，一个Differentiable Isosurfacing的一个操作，这个Differentiable Isosurfacing的意思是这样子的。

就是我们可以首先有一个Implicit Function，然后我们得想办法Differentiable，就是可谓的，把这样一个Implicit Function，转化成一个Explicit。

就是一个显示的Mesh，并且这样一个显示的Mesh，它就是我们这样的，它这个显示Mesh，就是我们Implicit Function里面，所对应的那个Isosurface，因此它就可以带来。

给我们带来很多很多好处，首先它的第一个好处就是，它可以非常自洽地，嵌入到机器学习里面去，因为它的Backend，它的内核 它的核心，还是一个基于Implicit Function的表示。

来表示一个三维的Shape，因此它就可以很适合的做机器学习，它也可以帮助我们做很多的，不同的Topology，同时呢因为它的生成的这个Output，它也是一个Mesh，那么它就会和很多的。

这种Graphics Engine，做一个非常好的契合，并且它也可以允许我们去，从一些Downstream Application里面，去Backward这个Gradient，然后在这里的话。

我就不具体讲我们怎么做一个，Differentiable Isosurface，然后感兴趣的同学，可以去看我们的Paper，然后在这个地方，我就给大家展示一个。

我们把Differentiable Isosurface，的一个应用场景吧，就是我们把它和Differender，做一个结合，这样子的话我们就可以，从Multiview Image里面去重建出来。

三维的Geometry，三维的材质以及Lighting，这个问题本身就是在Graphics里面，探索可能有12年的一个问题了，然后呢之前呢大家都是基于很多。

像Mesh啊或者是像Structured Flow Motion的，一些方法，他们就每一步都是Discrete，然后每一步之间都不能去Backward Gradient，最新呢大家做过很多Nerf。

但Nerf呢它的随地Geometry可能又不是，特别的好，它可能主要Focus上是一个，Novel View Synthesis，但我们其实更奇怪的事情是，如何去得到一个随地Geometry。

然后把我们的方法跟这种，这种这种这个传统，这种这个Task结合呢，就是用我们的方法去解决传统的，这种Task其实也非常简单，就是说我们可以首先有一个，Neural Implicit Function。

就是一个Implicit SDF之类的，我们可以首先通过我们的这种，Differential Isosurfacing去Extract出来，这个Implicit Function所对应的这个。

这个Surface长成什么样子，当我们有了一个Triangle Mesh，我们就可以非常容易地把它，Render成一个二维的图片，因为Render是非常非常简单。

Render Mesh这个相信Graphics里面，也研究了可能十来年了，然后并且速度会非常非常快，我们就可以通过Differential，把我们的一个三维的Mesh，Render成一个2D的图片。

然后因为这是一个，Multi-View 3D Reconstruction Task，因此呢我们就有一个，2D上的一个Ground Truth，就是说我们有2D我们有，有这个二维图片的Camera。

就它的Camera Pose，以及它的二维图片到底长什么样子，我们就直接对比我们所Render出来的，就是这个二维图片，以及Ground Truth的二维图片的，这种Difference。

然后由于这个Render是可导的，并且我们Extract这个三维Mesh的，这个操作也是可导的，那么我们就可以从，从一个2D的Loss Function，Backwards Gradient到。

我们的3D Implicit Field，这样可以帮助我们去优化，我们的3D Geometry，同时也可以帮助我们去优化，它的Material以及Lighting，然后这个地方呢，给大家展示一个。

一个小的Demo吧，就是说我们可以，首先它的Input，就是一些二维图片，但我们可以重现出来，像这张视频里面所展示出来的，这样的一个三维的Shape，然后比较好的事情是什么呢。

就是因为我们现在生成的是一个Mesh，那么我们就可以做很多很多的Editing，像这个Slide里面所展示的，就是我们要去Model它的材质，我们要去更改它的材质，同时我们也可以做的。

另外一个Editing是，我们可以做Physical Simulation，就是说像这个地方，我们做的事情，因为我们有它的Geometry，我就可以在上面放一个，比如说别的一个物体，然后把它自由落体。

让它掉下来，然后呢，同时呢因为我们也，因为我们重建出来它的材质，以及它的那个，它的Lighting，那么我们就可以做另外一个Task，就是Relighting，就是我想换一个环境光。

去渲染这样的一个Object，这些东西都是可以做的，而之所以能够做这个，就是因为我们的，生成的结果是一个Mesh，OK，好，现在，OK，下一个Slide，然后呢现在呢，我们有了一个比较好的。

一个比较适合机器学习的，一个三维表示方法，那么接下来，我们想探索的问题是说，我们该如何，当我们有了这样一个三维表示，我们该如何去探索，得到一个比较好的，Train出来一个比较高效的，一个三维的生成模型。

然后呢这个就是，我们的工作Gathering，Gathering它的核心的，Idea其实非常非常非常简单，如果跟大家讲的话，其实就是这么一句话，就可以Summarize它，就是说我们在2D的GAN里面。

在2D的这种图片生成，我们看到很多非常非常，好的Performance，它非常成功，那么我们其实应该，思考的一个问题是，我们如何把2DGAN上的成功，把它带到3D上面去，那么具体来讲的话。

2DGAN上的成功，它包括两点，首先第一点是说，2DGAN它花了很多时间去Develop，该如何去去去约束，我们的2D的Discriminator，并且呢如何什么样，并且并且呢也提供了很多。

这种2DDiscriminator的，这种Architecture，比如说像StyleGAN，比如说像那个PatchGAN，等等等等，很多的这种2D上，如何去比较Efficient。

如何去不让2DGAN去Explode，这种方法，他们都研究了非常久，第二个第二个点呢，第二个成功的点是说，2DGAN呢它也有很多，很多二维的生成的Generator，它可以允许我们去。

有这样一个Capacity，去生成非常高质量的2D的图片，我们就是希望能够把这两点，带到3D里面去，首先第一点的话，就是如何Supervise在2D，那么就像我们前面探讨的，前面给大家展示的。

我们可以很自然的，把Differential Render，做一个结合，就是说我们生成了一个3D Shape，我们通过Differential Render。

把生成的Shape Render成一张2D的图片，那么我就可以在2D上，去通过2D的Discriminator来Supervise，然后因为Render是可导的。

那么我就可以把这种Supervision，Backward到我们的，3D Generation里面去了，然后Generate在2D呢，它的其实用法也非常简单。

就是我们有一个Triplane Based的，Representation，然后我会在接下来的Talk里面，继续谈谈什么是Triplane Based的，Representation，而我们想做的呢。

就是把这种Differential Render，以及Triplane和DMTest，就是我们的Differential Iso-Surfacing，做一个结合，而这种结合的一个。

最大出发点是为了Efficiency，而为什么我们这个地方，特别强调Efficiency呢，是因为我们的Supervision，永远来源于2D的图片，这意味着。

如果说我只能Render一张非常小的2D图片，假设我的2D图片，它就是比如说128x128的Resolution，那么我的Training Signal就会非常非常Weak。

但是呢如果说我能够非常Efficient的，Render成一张非常High Resolution的Image，比如说是1000x1000，1024x1024的这种2D Image。

那么呢我就可以有一个更好的这种Supervision，帮助我们去生成高质量的3D的Shape，然后这个地方我们就需要，比较注意的一点是，我们如何去能够Efficient的Render出来。

一张非常High Resolution的2D的图片，我们来看一看我们的一个Pipeline，Pipeline上我们首先有两个Prior Distribution。

一个是Geometry的Prior Distribution，另外一个是Texture Prior Distribution，它每个都是一个Gaussian，我们首先有一个随意Generator。

然后下一个Slide会告诉大家，我们用的随意Generator是什么，来生成一个带Texture的Mesh，然后这个是我们的Inference的Pipeline，当我们在Training的时候呢。

我们首先从Training Set里面去Sample出来一些Camera，因为我们假设我们在Training Set里面，知道每一个Data的Camera的Pose。

然后我们可以通过Differender，把我们生成的这个MeshRender成二维的图片，在这地方我们Render两张图片，一张是RGB Image，一张是二维的Silhouette。

然后我们会有两个不同的Discriminator，然后去分别去区分每张Image是真实的Image，还是虚假的Image，然后由于这个Render是可导的。

那么我们就可以把Discriminator的这种Training Signal，去Backward到我们的随意Generator，而且这个地方需要提的一点事情是。

因为RenderMesh是非常非常Efficient的，所以这边Bottleneck的话其实是在Discriminator，然后Render是一个非常Cheap的操作。

然后我们来看看我们该怎么样去Design一个随意Generator，我们的用法是一个Triplane Representation，所谓的Triplane就是说我们用三个Plane Feature。

然后每个Plane所对应的事情是在三维上的不同的Protection，就类似于大家所看到的三视图一样的东西，然后我们首先可以有一个XY Plane或者YZ Plane或者是XZ Plane。

有了这样一个随意的Feature Plane的话，我们对于每个随意上的每个点，我们可以Project把它Project投影到，另一个Plane里面去得到它这个顶点的Feature。

然后当我们得到这个顶点的Feature，我们就可以Predict出来一个Implicit Field，然后DMTab就是我们前面的Differential Isosurfacing。

它就可以帮助我们从这样的一个Implicit Field里面，去提取出来一个三维的Mesh，当我们有了这样一个三维的Mesh的时候，怎么去Texturize它呢，就是给它赋予它的纹理信息呢。

这个做法也非常简单，就是说我们对于三维的Mesh上的，三维Mesh面片上的每一个点，就是并不仅仅是面片的顶点，我们也可以是面上的任意的一个点，我们可以知道它的三维的坐标，当我们有这样一个三维的坐标之后。

我们可以把它放到这样一个Triplane里面去，去提取出来它的Triplane的Feature，然后就可以做一个，就把它给一个，把它放到一个MLP里面去Predict出来，它的那个RGB的Color。

好 我们有了那样的一个，然后这个地方给大家展示的就是，我们Gatsby所能够生成出来的，Object的一个结果，我们可以生成非常高质量的Shape，比如说像摩托车里面这些把手啊，像这个动物里面的脚啊。

尾巴啊，嘴啊什么的都可以生成出来，然后呢，而且它也具有比较高的Diversity，OK，这个大概就是我们所现在，Gatsby所能够达到的一个生成的效果，好，但是有了一个这样的一个三维的生成模型的时候。

其实有些时候我们最care的事情是，我们如何去生成一个没有见过的Data，这个其实是大家需要那个，Go beyond the training dataset，然后呢，因为我们有两个Data。

我们有两个Latent Code，一个是Geometry Latent Code，另外一个是Texture Latent Code，因此呢可以做一个非常有，非常有意思的事情。

就是说我们先Sample两个Shape，然后呢，我们对这两个Shape做一个插值，像这个地方呢，每一个每一个纵向，我们是固定住它的Geometry Latent Code。

然后只插值它的Texture Latent Code，我们就可以发现，从纵向来看，每一个每一个列这个地方，它的Geometry是一样的，但是它的Texture会不断的会慢慢的。

从一个白色车到一个黄色的车，然后每一个横向呢，我们固定它的Texture Latent Code，然后插值它的Geometry Latent Code，我们就可以发现，它从一个一个Sports car。

然后然后，它从一个小轿车插值到一个大的SUV，这个Size就不跟他讲，然后另外一个其实就是潘老师，潘老师刚刚提到的，然后因为Dragon非常好，然后呢有一个学生。

他其实把Dragon和Gasoline做一个结合，然后呢，然后我觉得非常有意思，所以也在这里说给大家，然后这个地方我们可以看到，比如像这个地方，像这个动物，我们也可以通过只抓个两个点。

然后去改变这个动物的尾巴的位置，然后像中间这个地方呢，我们也可以通过只抓个两个点，去改变这样的一个椅子座位的这样一个位置，然后我们也期待这样子的效果，能够帮助大家去，比如说像去做一些艺术的创造啊。

这等等等等，非常感谢这个潘老师的工作，抓得非常好，对，OK，然后我先我就先skip这几个slides，由于时间关系，我们先讲讲，我们我们讲讲最后一个一个一个工作吧。

就是说我们该如何去去运用到很多在RD上的data，帮助我们去做这种三维的内容生成，然后这个地方是我们的一个一个Magic3D，做的一个工作，在Magic3D里面的话。

它的它的它的setting有一点点的不一样，然后呢这个setting是说我们给一个text prompt，就比如说一个Binding CT on top of Pancake。

就是一个一个一个一个兔子在一个面包饼上，做在一个面包饼上，然后呢我们希望的事情是用户给一个这样的text prompt，我们希望能够生成出来所对应的3D的shape。

然后像这一页slide所展示的上面一页结果，就是我们model的所产生的结果，我们的核心idea呢也非常非常简单，就是说我们看到很多的RD的generating model。

就是RD的diffusion model，它们非常非常有优越性，然后呢在我们看来RD的diffusion model，其实是给了我们一个score functions。

然后这个score functions其实就可以帮助我们，去告诉我们说我们该如何去优化我的RD的图片，使得我优化所得到的RD的图片，能够更加的像一张真实的图片，那么我们其实就应该想办法。

把这样的一个RD上的score function，把它distilled到一个三维的世界当中，那么这种distillation其实想，其实一个非常简单的操作就是differential render。

因为就像我们像前面那个slide所展示，我们可以把我们所生成的3D的shape，把它render成一张RD的图片，然后呢在RD上我们用diffusion model。

告诉我们score function该长成什么样子，然后把它distilled到我们的这种三维的内容创建当中，这个就是一个非常high level的告诉大家，就是RD的diffusion。

和这个如何去用一个RD的image diffusion model，这个idea并不是我们第一个所提出来的，其实在Dream Fusion是第一次，第一个所提出来这样的一个idea的。

但是呢在Dream Fusion里面，在这样一个idea的一个大的setting下，最大的一个challenge是什么呢，是efficiency，为什么它是一个很大的问题呢，是因为当我们想运用到一个。

在二维图片上的一个score function的话，我们一般都需要render出来一张full image，就是说我们需要把整张图片给。

给那个score diffusion model去做denoising，但是呢像大家如果正常trainers，其实你并不是render整张图片，你其实只会sub-sample一些pixels。

而之所以你要你会sub-sample一些pixels，是因为你如果想render一张全图，你是会是一个非常efficient的操作的，就是说如果说你想render一张，512x512的一张图片。

这个在nerf上是很难去做到的，因此呢像Dream Fusion，它其实就只能render一张，64x64的一个image resolution，并且即便是在这样一个非常低resolution的情况下。

他们也需要消耗一个半小时才能够得到一个shade，然后呢同时呢也因为，他们只能render出来一张非常，低resolution的图片，那么这就意味着他们的training signal。

是非常非常limited，它的diffusion model也只能用一个，非常low resolution的diffusion model，而既然你的low resolution model。

你就很难从这里面去capture出来，很多high frequency的details，比如像这里面的这个冰激凌，你就只能是一个非常rough shape，就是一个非常粗糙的一个3D shape。

你就很难得到一个非常高精度的，一个比如说冰激凌或者是一个汉堡包，然后我们的一个思路是这样子的，我们的一个思路是说我们除了，我们可以用一个close to fine，就是一个两阶段一个从一个粗糙到精细的。

一个两阶段的一个过程，首先在第一个stage，在第一个close stage的话，我们其实我们用的是一个instant NGP，去帮助我们得到一个initial geometry。

告诉我们说我们先得到一个rough，比较globally就是全局上看起来还比较对的，一个geometry是什么，然后这个地方呢我们就不需要render出来，非常高精度的图片。

我们可以直接用一个非常低精度的diffusion model，非常快速的就可以把它train出来，然后为了去用到那个高精度的diffusion model，帮助我们得到recovery出来。

更多的高精度的信息呢，我们其实又还是回到了我们最开始，所跟大家谈到的DM text，就是一个differential isosurfacing。

因为render mesh是一个非常非常efficient的操作的，因此我们可以通过differential isosurfacing，从instant NGP里面去得到一个3D的mesh。

而当我们有了这样一个3D的mesh之后呢，我们就可以把这个3D mesh，render成一张2D的图片，而这个地方呢我们就可以render一个，非常高精度的2D图片。

在我们的例子里面是一个512x512的一个，二维的图片，然后把这个二维的图片，这个高精度的二维图片给stable diffusion。

因为stable diffusion resolution是512x512，我们就可以得到一个，high resolution diffusion prior，然后呢我们可以在这张图片看一个对比。

就是说右边这个是经过了，high resolution diffusion prior所生成的结果，左边这个呢是只有，low resolution diffusion prior所得到的结果。

我们可以很明显可以看到，当我们有了，当我们有能力去运用到，高精度diffusion prior的时候呢，我们所生成的shape是有更高更高质量的，然后我们也可以发现这个。

这个magic dream fusion这个事情，就是说我们对比我们的结果和dream fusion的结果，我们可以发现我们的geometry和texture，都有更多显著的提高，然后我们有这么。

然后这个就不讲，就是如何去control final generation，这些关系就不讲，我们先谈谈给大家show几个demo吧，首先是，既然只能说这个那就只说这个。

这个事情呢是我们NVIDIA一个Picasso，这个事情是说我们让artist通过给一些prompt，然后呢我们去给这我们，在这些给定的prompt里去生成所有的3D shape。

像这里面这个视频里面所展示的，所有的三维的objects，都是我们通过magic 3D所生成出来的，然后我们也期待这样的一个framework，能够帮助艺术家或者是各种创作者。

去创造这样的一个三维的虚拟世界，OK，然后在我们稍微的做一个小的总结吧，我们这个这个pipeline的话，其实是首先讲了三个事情，第一个事情是一个三维的表式。

我们其实提出的是一个differentiable isosurfacing的一个，一个方向，它可以帮助我们从把gradient从match backward。

到一个invasive function里面去，然后呢讲的第二个点是是gas 3D，它其实是develop了一个比较，它其实它的最核心的事情。

是它把differentiable isosurfacing，differentiable render，以及2DGAN里面的这些benefits，都把它融入到这种3D generation里面去了。

这样就可以帮助我们去生成一个高质量的3D mesh，以及materials，然后最后一个magic 3D，它其实用的是一个cost to find的一个training paradigm。

然后并且它能够帮助我们去做一些text to 3D generation，然后最后的几分钟，给大家谈谈一些我们觉得，比较make sense的一些future work吧。

在我看来其实future work的总结，其实就一个非常简单的就是scale up，我们需要scale up，那scale up它有这么几个方面，首先第一个方面的scale up是说。

我们要从single category generation，到一个universal generation，像我们的gas 3D一个很大的limitation是说，我们只能生。

我们我们要在一个一个category里面去train一个3D generation，这个是非常非常inefficient，而且不scale up的，我们其他的事情是。

我们能不能有一个3D generation model，它可以generate很多很多，我们在设计生物当中，所能够看到的很多种不同的object，这个事情是需要探索的第一个方向，第二个方向是说。

我们该如何从一个object generation，就是只单单的生成一个单个的物体，到一个生成一个一个三维的一个大的场景，然后像这个地方，我们其实希望能够比如说把compositionality。

因为这个世界是compositional的，把这种compositionality把它嵌入到这种三维生成模型里面，比如说像那个neuronsynclath，或者是giraffe所对应的这些观点。

然后呢最后一个需要scale up的事情，在我们看来是说，我们之前所生成的很多的物体都是一个静态的物体，但我们所有人都是生活在一个动态的一个三维的世界当中，我们人是会走路的对吧，车子是会开的。

那么我们该如何去生成一个会动的物体，然后我们做了一个非常非常简单的一个尝试，在一篇即将要release的一篇siggraph paper里面，我们去我们我们想办法重建三维的物体，以及这个物体的物理属性。

像我们重建的事情是这个物体的密度，就是它的质量，但是这还仅仅是一个非常非常初步的，一个一个一个一个实验结果，然后呢我们也期待把生成的三维的物体，把它变成一个dynamic object。

然后呢最后这就是我的一个最后一个slides，就是非常感谢大家来听我的presentation，然后我要谢谢我的很多的合作者，对然后我们有什么问题也欢迎大家提问，谢谢大家，OK现在是那个QA时间。

有五分钟的时间，您好那个因为我之前是认真的读过咱们这个，GET3D和Magic3D这两篇paper，然后您刚才也提到了，就是我们想要从GET3D这种很有局限的这种生成。

变向Magic3D这种能够更多样性的这种生成，是我们其实可以看到这个效果是在直线下降的，所以其实大家都知道，现在最大的问题就是它缺乏高质量的，Magic3D的训练数据集，就除了像英伟达这种。

可能有这样的实力去储备一些这样的数据集，就可能对于其他的公司或者我们这些学生，学术来说可能这个门槛是很高的，所以我想请问您觉得未来这个问题，可能会有一个什么样的解决方案。

大家是不是需要组织一个开源的这种集体，来去对这个数据集做一些努力，谢谢，我非常感谢这个同学提问，非常make sense，其实现在学生已经在做这么一件事了，比如说像Optiverse。

就是Allen AI2他们在做的这个事情，他们其实就是一个开源的集体，他们也在想办法把这种二维的数据，三维的数据去clack更多的三维的数据，并且具有更高质量的三维数据。

而且都是free release给大家去用的，其实我觉得更多的事情是这样子，在我看来是这样子，其实你如果看现在三维生成的模型的performance，我们即便在ShapeNet上。

我们的performance都还是不是很好，就如果说你能够对比我们生成的结果，和ShapeNet自身的结果来看，它的其实还是有很大很大的gap的，这就意味着我们就是在算法上面，还需要有很大很大的改进。

来得到非常高质量的生成，当然data上面也是一个需要去improve的地方，但是我觉得算法也是需要一个去improve，我们可以在非常small scale上。

比如说像如果仅仅focus on ShapeNet一个category，我能不能把这种生成的质量，像真的ShapeNet的object那么高精度，这个现象大家其实是做不到的。

对所以可能算法上也需要很多提升，然后呢然后你说的这种开源的，开源data set有很多人在做这一件事，你好老师我想问一下您这个到了一定阶段，您这个准备什么时候开源，以及您是以什么形式开源。

还有以后您的软件是以插件的形式，还是一个单独的软件系统的形式，首先Gatsby已经开源了，就是Gatsby所有开源的所有的code，training inference code都开源。

然后我们也release了一些pre-trained model，然后Magic3这个事情我也不是特别清楚，因为这个事情是NVIDIA他们那边主要在推的，OK还有其他人有问题吗。

老师你好非常感谢您的精彩演讲，我有一个问题就是现在我们知道很多的工作，就是text to 3D的生成的这样的工作，比如说Dream Fusion然后Magic3D。

然后还有近期的这个Prolific Dreamer这样的工作，他们其实还有一个很重要的问题，应该是没有被解决，就是你用text去生成3Dobject会有多连的问题。

我们一般叫做multiple phase，它产生的一个可能很重要的原因，就是你的预训链的deferred model，可能它你如果通过text prompt去传进去position。

就camera position的一些信息，它可能是很弱的，所以尤其是Slippery Deferred这种model，它对这种多连信息是很难去get到的，所以我想问一下老师。

您对这一方面有什么解决方案的建议吗，就是怎么去避免multiple phase这样的problem，谢谢，对multiple phase的problem大家非常常见。

我们也碰到很多multiple phase的issue，其实在我看来是这样子的，我的个人理解是说，我们得想办法把3D product给运用到这个里面去。

因为3D product它没有multiple phase，比如像Gas 3它所能产生的shape，它没有任何的multiple phase，因为我们data告诉你说我人就一张脸。

我的动物也就只有一张脸，我没有很多张脸，然后我们之所以，如果说仅仅是靠一个2D的product，因为就像你说的，Stable Diffusion它可能就不具备这样的一个，view dependent。

view conditioned diffusion model的这种generation，所以它是很难仅仅通过2D的product去，得到一个三维的。

得到这种没有multiple phase的issue，就是第一个可以指的，在我看来值得尝试的就是，如何把3D product和2D product做一个结合，另外一个比较有意思的工作就是，如果大家关注。

比如像那个0123，就是传比尔他们做的一篇工作，他其实是说他find new diffusion model，他用3D去find new diffusion model。

这样子的话你的diffusion model，就是把3D inject到你的2D product里面去，这样子你的2D product就有了这种camera的。

somehow你有些camera的一些information，这样子你就可以说，你的这种2D的product，它可能就没有一些multiple phase的issue了。

这时候你从0123你去distill你的3D shape出来，可能就会要比从一个就是正常的，或者是没有fine tune过的stable diffusion要好一点，对老师那我能再跟着问一下吗。

就是是0123是有一个二阶段的fine tune的过程，然后让他去capture到一些camera pose相关的position，但是这样的话就是这种感觉，这种就一二阶段分别处理的这种方式。

可能是不是就是就因为我还是比较好奇，就有没有一个更unified的方式，直接从源头去解决，就是避免这种可能太有点就是过于依赖调参，这种的二阶段的方式，从源头上解决，我个人还是更倾向于像我最开始说的那个。

把3D product做一个结合，就是说如何把该3D和Magic 3D做一个结合，这也是我自己特别想探索的一个方向，对因为该3D它是没有任何的一个这种multiple phase issue。

所以如果能把该3D和Magic 3D这种2D，就是该3D的3D product和Magic 3D的2D product做一个结合，我觉得可能是需要去，当然这个如何做结合，这个事情我也不是很清楚。

这个事情我们也在探索，然后呢也需要很多的科研工作者去研究这个事情，好的好的谢谢老师，OK那个因为时间问题，然后我们的QA到此结束，然后如果有更多的问题，大家可以等到panel环节继续问这个高博士。

然后我们再次感谢高博士的报告，谢谢大家，然后我们接下来第三个报告呢，是来自志愿的王新龙，然后我简单介绍一下，新龙的话目前是志愿研究院视觉模型研究中心研究员，然后博士毕业于这个澳大利亚阿德莱特大学。

研究领域为计算机视觉和基础模型，近年来然后也有很多很非常不错的工作，包括SOLO， SOLOV2， DASCL， EVA， PANTER， 以及SAG-GPT，然后也获得了Google Ph。D。

 fellowship，以及阿德莱德大学的这个doctoral research mantle，然后新龙的这个报告题目是通用视觉模型初探，我们欢迎新龙，谢谢各位老师介绍，然后我是来自志愿研究院的王新龙。

然后刚刚两位老师都秀了很多炫酷的这个效果，我这边没有那么炫酷的效果，就是主要讲怎么把视觉模型做大做强，以及我们在多莫泰的一些进展，首先我们的研究目标就是去做，为了实现通用的视觉智能。

然后这个问题其实很直接但是也很大很困难，所以我们在研究的过程中把它分成了两个部分，一个就是视觉表征，就怎么去学到更通用的视觉表征，它能够应对各种各样的视觉的信号情况，然后我们要处理的任务。

所以就是包括我们之前做的预训练，其实都算在视觉表征的这个里面，然后第二个部分就是怎么去学视觉的一个通才模型，在以前我们主要还是更focus在一些单一的任务，比如说分类 检测 分割。

我们去刷一些单一的benchmark，比如说我们在Invisa上提高了0。5个点，或者Coco上提高了0。5AP，但是最近几年大家慢慢地开始往，怎么样去做更通用的模型，这个模型它可以像GDP3一样。

能够训完之后，你能解决各种各样的视觉任务，你可能甚至不需要微调了，或者可以做一些新的视觉任务，这个是我们在第二个部分，就是视觉通才模型这个部分，希望去研究和解决的一个问题，然后我下面就先针对这两个部分。

介绍我们最近的一些工作，首先是第一个，就是怎么去学更大的 更强的通用的视觉表征，我们提出了一个叫EVA的视觉模型，然后这个想法也是很简单，就是我们希望回答第一个问题是，什么样是一个好的视觉表征。

首先这个好的视觉表征，它必须是能够适应各种各样的下游的任务，你所需要的应用的情况，然后我们分析下来，其实分成两部分，一部分是它需要很high level的语义，另一部分它是需要一些结构的空间的清晰。

一个最简单的想法，就是直接把clip和mask image modeling做一个结合，然后这个方法呢，一训练方法其实很简单，就是我们这个图里面所示，我们有一个clip模型。

clip模型就是通过大规模的图文队，训练出来的这么样一个模型，然后我们有中间一个EVA，我们给输入的图片的时候会mask一部分，然后去重建这个被mask部分的clip的特征。

然后也不需要tokenizer，然后用回归的特征，这么一个方式去做训练，这个方法非常简单，但是我们觉得最重要的是说，怎么样把一个统一的unified和简单的方式，把它给做，就把它scale up。

把它变大，也就是我们首先是把它变大到了，10亿参数的一个规模，所以这个工作一句话介绍，就是clip加mm加eb的参数，然后具体来说呢，就是我们在30M的数据上预训练之后，然后它这个模型可以迁移到。

各种各样的下游的任务上，就经过微调，比如说做infinite分类，做video的分类，做object检测分割等等，那这些其实我们只是做了，并不是我们希望的结果，就是我们把这个，大家都说要做大模型。

要做这个scale up，那scale up究竟的目的是什么，我们认为这个模型，至少它能够满足三个方面的一个能力，也是我下面介绍这个表格里面的一个情况，一个是首先，在一些大家经典的任务上。

我们希望它能够取得一些更好的性能，就是新的突破，也就是我们这个表里面所示的，像infinite cocoa，然后video的这种分类，取得更好的一个性能，这个是一个基本的第一步，然后第二步是说。

我们以前有一些很困难的任务，比如说常委的视觉识别，就是比如Albis的实力分割，它可能有一千个类别，那大部分其实没怎么见过的类别，怎么把这些任务能做好，然后我们发现，这个模型变大之后，在这些任务上。

能够带来一个质的提升，也就是这个红框里面的，这个5。8AP的一个提升，这个是我们觉得，比较欣慰的一个结果，因为它能够真正地带来，在一些我们原来觉得很困难的任务上，有一个大幅的突破，然后进一步的是说。

我们这个模型是从clip来的，那clip本身大家知道，是一个很通用的基础模型，但是它的问题是很难训，有没有可能，我们能够帮助clip训得更好，也就是我们发现第三个，也是最重要的一个点。

就是因为能够帮助clip，更好的连接视觉和语言，具体来说我们在训clip的时候，因为需要很大的versize，然后动辄32K 64K，然后这个大规模的数据，400M 2B，然后很难训。

那我们发现用EVA作为这个图像，Image Encoder的这个初始化之后，整个训练会变得非常的丝滑，就是很顺利地能把这个模型给训出来，所以在基于1B就10E参数的，这个EVA基础上。

我们把它在clip上做训练，然后可以训到，这是之前开源的，最强的clip模型，就是比之前openclip的模型会好很多，然后基于这个发现，我们觉得这个其实是最重要的一点。

那我们怎么把它进一步做scale up，也就是我们叫EVA clip这个工作，就是重点放在，怎么去把clip训得更好，更大更强，因为就像刚刚说的clip很难训，然后也很耗资源。

怎么样更高效的更稳定的训练clip，其实是一个非常重要的问题，然后在这个工作里面，我们发现了三点比较重要的，这个报告里面吧，我们发现三个比较重要的一个技巧，一个是最重要的是。

把EVA作为这个Image Encoder的初始化，能够大幅地优化clip的训练，然后第二个是用LAM优化器，去训得更好，包括用Flip，就是去提升这个效率，Flip就是说我们在。

虚拟图文对比学习的时候，会扔掉，我们会扔掉一半的这个Image Patch，来做这个对比学习，然后有了这些这个技巧，我们把EVA和EVA clip，进一步scale up到了，4B和5B。

这里的5B就是50亿参数，然后它可以在ImageNet的零样本，分类上取得一个现在最高的，82%的性能，就是比之前的像OpenClip，OpenClip比之前最高是80%。

然后可以看到这个图里面的一个效果，也是现在开源的最强的clip模型，然后刚刚说到的，包括1B链，这个4B链5B链，包括我们有0。3B链，这是一系列模型都已经开源了，然后也被很多后面的动模态的工作。

像Bleed2这些来采用，所以也欢迎大家去GitHub上，自己去试用，然后刚刚主要是讲了第一部分，就是怎么学到更好的，更通用的更大的视觉模型和表征，那第二部分是，就是我们希望怎么去训一个模型。

像GPD3一样去解决各种各样的视觉任务，然后你可以训完之后，你可以有test time的一些方法，去激活它的能力，去做一些之前做不了的事情，第一个是介绍我们，也是CPU-R的一个工作叫Panter。

Panter的动机有以下几点，首先一个是我们做通用的视觉模型，第一点就是怎么去做通用的，统一的视觉的建模，这一点跟语言不太一样，语言来说，你所有的任务，语言它都是统一的形式，但对于视觉来说。

你每一种任务都是一种不同的形式，以前大家分别做分类 检测 分割，深度估计 关键点检测等等，那怎么样用一种统一的形式，把所有任务统一起来，并且把它们的数据都能够利用起来，是这里的一大问题。

但是统一本身并不是最终的目的，我们不是为了统一把它放到一起，而是希望看到统一之后，它能够出现一些新的能力，第二个动机是，我们希望去探索一种新的，视觉感知的方式，以前大家一直都是，我们先训完。

然后在每个任务上去Funtune，然后去得到一个点数，那有没有可能我们探索一个新的方式，它不需要Funtune了，你这个模型可以自动的去完成对应的任务，第三点是，我们希望去探索一种通用的接口。

就像语言一样，语言可以作为一个接口，去做各种各样的事情，那视觉来说，什么是视觉的接口，之前有一些工作，像包括大家知道的，像Pixel to SIG之类的，大家会把视觉信号做一个离散化。

然后像语言一样去处理，那我们这里问的一个问题是，有没有其他的方式，或者说更适合视觉的方式，去解决视觉问题，所以带着这些动机，我们提出了这个叫Panter。

然后这个名字叫做Images Speaking Images，就是图像说图像的语言，然后把图像作为一个通用的接口，具体来说就跟这个图里面所展示的，就这里每一行，左边是Prompt，就是提示，就是例子。

给一个图片，跟它对应的这个，比如说分割，或者深度估计，或者关键点检测，作为例子，那送到这个模型里面，它就知道，你要完成对应的这个任务，然后这里最关键的一点，就是刚刚说的，怎么去统一不同的视觉的任务。

因为视觉任务太散，然后这个很，之前其实很难把它们，以一种简单的方式统一起来，那这里我们的想法是，把所有视觉任务的输出，都统一成图片，就是当我们人在标图片的时候，其实你就是在画画，那我们假如说所见即所得。

就去预测你对应的这个视觉任务的图片，也是我们的一个动机，所以具体来说，就把比如说第一章语意分割，那以前每一个像素，可能是一个80个类的类别，那现在我们把所有的每一个类别，当做一种颜色，然后这样的话。

它的label其实都是，就是一张图，然后也其实也就是，大家看到的这个样子，然后包括实力分割，那实力分割有另一个问题，就是怎么去定义实力的类别，然后所以我们这里，其实是沿用了Solo的思想。

用它的位置来定义它的颜色，同样的来把它变成图片，然后第三个是深度估计，就是更直接一点，就是把它的深度值，变成这个图像的空间，包括关键点检测，然后一些low level的任务。

low level任务更直接了，因为它本身的输入输出，就是图片的形式，所以有了刚刚这个统一的形式，就是我们现在所有这几个任务，它的数据都是图片了，那我们应该怎么去学，是我们现在要解决的问题。

所以我们提出了这个generalist painter，然后这个方法，其实就是做impending，或者做mass image modeling，就是我们把这个图片拼起来，就像这个图里所示。

左上角是所有的任务的，各种各样的数据，我们把它变成图像队，然后我们现在的过程中，随机地去sample，然后把它拼在一起，去做图像研码，也就是mask一部分，然后模型希望去，预测被mask的部分。

然后这个架构，就是一个简单的vision transformer，然后损失函数，就是一个回归损失，具体就是smooth L1，整个非常简单，就是没有针对任务的特殊设计，就这么一个简单的结构。

我们把刚刚统一的数据喂进来，选好之后，我们在做预测的过程中，其实在做的事情是，去母权，就是去impend，或者说去恢复被mask的那张图片，也就是我们希望的target，然后以这种形式。

就能把这些任务给统一的训练起来，然后训练完之后，这个模型就具有了，in context visual learning的能力，就是上校文视觉学习，上校文学习。

in context learning是GV3，它核心的一个思想，有了它你可以做，各种各样的语言的任务，那对于视觉来说，我们是探索了，上校文的视觉学习，就是你在训练完之后，我可以通过给一些例子。

让它去自动完成对应的任务，比如说这里分割关键点，包括一些你没见过的任务和例子，然后当然这个通用模型，也是得跟之前的各种专用模型，去做一个对比跟评测，所以我们在常见的这几个视觉任务上，包括深度估计。

语音分割，全景分割，关键点检测，denoising，deraining，然后还是low level的任务上，去做了一个评测，就包括了，这个任务包括了，high level的视觉理解。

和一些low level的图像的处理，整体的话，就是比之前的一些通用的方法，像pixel-to-sig，v2，UVM，UnifyO，在很多指标上会有大幅的提升，然后另一个也是去比较。

当我们把这些数据都一起训练之后，一个问题就是，它比分别训练有什么优势，那这里做了一个分析，就是当我们把这些任务单独训练，跟一起训练，结论就是，你统一的训练，在大部分任务上，都有一个很大的提升。

在个别任务上会有冲突，这样一个结论，然后这里很有意思的，就是进一步去探索，它更多的任务中间的一些具体的关系，然后这个也是，这个是我们暂时没有触及到的，这个是更多的一些可视化。

就是我们这个方法很简单很统一，那它具体在这些任务上表现的怎么样，这里秀了一些像语音分割，深度估计，实力分割，然后关键点检测，这样的一些例子，可以看到都还是，就是这种方式是，能够得到很准确的预测的。

然后就有了Penta这个，很简单的架构框架之后，相当于你只要喂数据就行了，然后你后面的就是去，去过一个transformer，然后去回归这个像素的，这个RGB值，那整个简单的框架，我们想说它还能做什么。

就是能够进一步探索它的空间，所以我们提出了SAC GPT，就是叫Sem everything with a Generous Penta，就是用Penta去做任意的分割，也就是我们说的分割一切，然后。

大家可能都知道Sem，我们其实23月份就做好了，然后当时一直在完善，看到他们当天放出来，我们当天晚上就上线了，然后，虽然大家的角度不一样，但是目标其实一样，都是怎么去探索通用的分割模型。

为什么大家都突然会，聚焦在分割这个问题上，可能不了解分割的同学，可能会比较奇怪，因为分割是一个，分割里面最基础最重要的任务，或者说分割也可以表达，各种各样的视觉任务，就像聊天，SAC GPT还能够聊天。

表达各种各样的视觉任务一样，分割也是同样的，然后Sem来说，它是去做交互式分割，就是你给点给一些框，它去把当前的物体分割出来，我们其实走的另一条路线，其实可以理解成少量的分割，就是我们给一些例子。

你能完成任意的分割的任务，具体怎么做的呢，就是刚刚介绍了Panther，那其实从Panther到SAC GPT，需要的是我们把所有的，分割的数据汇集起来，把他们比如说语音分割，实力分割 全景分割等等。

前景分割，各种领域的分割汇合起来，然后把它们统一成上下文的samples，就是上下文的例子，比如说对于左边这些人，场景录，他们其实是共享一些context，上下文，然后这样的话，你就可以在他们之间。

做上下文学习，包括他可能是同一个实力，同一个类别等等，构造完这样的上下文的例子之后，后面要做的事情就跟Panther一样，就把它们拼起来，然后去做mass image modeling。

就是去挖掉一部分，然后去预测被挖掉的那部分的像素值，然后这样就直接完成了，这个整个的训练，也是非常简单，然后训完之后呢，它就可以做，你给任何的例子，它能够照猫画虎的按着你的例子，去做对应的分割的任务。

比如说，这里我们想要它分割影子，然后之前肯定是没有见过影子的，我们把影子给它提示出来，然后这个模型就知道，我要把影子给分割出来，包括轮廓，轮廓也是没有训练过的，然后它也知道，你要把轮廓给分割出来。

包括右边是更多的一些diverse的，拍脑袋想的一些例子，比如说，左边的lost spot，就是损失函数，没有人会为了损失函数去训练一个分割模型，然后我们给了它一个例子，告诉它我要分割这个尖峰。

然后它就知道，把其他图像的尖峰给分割出来，包括左下角这个码，因为我们给的是图像，我们这个prome它其实是一个图像，那你这个图像可以是一种颜色，就是分割一个目标，也可以多种颜色，可以分割多个目标。

所以我们把这个码给涂成三种颜色，表示头身子和脚，这个模型就知道，把其他的码，给同样的头身子和脚给分割出来，包括右下角的这个是图文caption，就是图文描述，把所有的描述，它也同样的给分割出来。

然后刚刚是，就是给图像的例子，我们把第一针做为例子的话，它同样可以完成这个视频上的，任意的分割，然后在性能上，也是跟Drawing的方法，有一个comparable的性能，然后这里我没有细讲的是。

大家如果感兴趣可以看论文的，是我觉得很重要的一点，对于我们，大家在做toy模型的时候，最重要的一点是希望，你能够它有很多的，在训完之后，可以通过test time的一些方法，去激发出它新的能力。

这里我们重点探索了，比如说它可以任意多的prompt，它可以这个，你可以用一些微调的prompt，去做定制化的一些事情，然后都是可以在一个，简单的框架下去做实现，然后刚刚主要从这两个方面，就是视觉表征。

然后visual genres，就是视觉通彩模型，介绍了我们最近的一些工作，分别是EVA， EVA clip， Pantery， SAC GPT，然后具体，其实大家感兴趣的，可以去看我们的代码和论文。

这里所有的模型，都是已经开源了，然后大家可以在网上找到，让我们总结来看，就是我们的这个研究思路，就是为什么我们会做，这么一件事情，或者怎么做的，我觉得总结来说，其实是一个很简单的公式。

就是unified learning，和scalable data，和large models，就是这个公式，可以套到大家想到的，最近的可能所有的大模型上面，因为这个本质上来说，我们首先是要找到一个。

统一的学习方法，不管是clip的图文对比学习，还是EVA的这种训练的方式，还是Pantery这样的，这个impeding的方式，你找到这个方式之后，你最重要的是找到。

对应你想要的scalable data，就是比如说是，400M的图文对呢，还是这个分割呢，还是这个纯图片呢，然后找到之后，你把模型做大，整个合起来，你就可以做到，我们想要的。

这个scaling的这个效果，然后这里面，这三个里面最难的，其实是中间这个，就是比如说在clip，他们之前，大家其实没有想到，用这样比如说几百M，甚至更多的图文对，训出来的模型，用简单的对比学习。

训出来的模型，能有这么强的能力，那这里的核心，其实这个数据，直到Lion把这个数据，给浮现出来，大家才有能力，去训练这样的clip模型，所以那在图文对之外呢，大家进一步，应该探索什么样的数据。

包括是纯视觉呢，还是这个多姆态呢，这些其实是新的一些问题，那带着这些问题，所以我们进一步探索了，怎么去做，进一步去做多姆态大模型，或者说做，能具有上下文学系能力的，通用的多姆态大模型。

然后这里也是跟大家分享，我们最近的一个工作，然后也是马上要放出来的，叫，对叫Emu，然后这个是一个，能够接受多姆态输入，产生多姆态输出的，一个大模型，然后，对GPT-4来说，它是接受多姆态的输入。

产生文本的输出，然后这里我们是希望，能做一个统一的，多姆态的上下文学系，具体来说，就是刚刚说的数据，是最重要的，所以我们最重要的事情，其实都花在了数据上面，就是把各种各样的，多姆态的序列给找到。

把它处理好，比如说有图像文本，有交错的图文，就我们在浏览网页的时候，其实都在看交错的图文，就像Flamingo用的那些数据，然后有视频交错的图文，就是比如说，你的YouTube的视频。

然后各种各样的视频，它其实都可以处理成，交错的视频文本的形式，包括纯视频等等，这些数据以统一的形式，就是多姆态序列的形式，处理好之后，我们去做统一的，多姆态上下文学系，训练完这个模型。

它就能够感知 推理和生成，各种各样的模态的数据，也就是它能够，你给它的prompt，可以是多姆态的，给它的输出，也可以是多姆态的，具体来说它能做什么呢，就是这里给了一些例子，就是左上角是一个。

它能够有精准的，一个世界的知识，然后比如说它能够，准确知道这是，莫奈的日出印象，相比其他的一些多姆态模型，有比较大的一个特点，然后包括它有少量本的，像Flamingo一样的能力，就是我给它一个图片。

大熊猫，它们在中国非常受欢迎，然后包括羊驼，它们在南美洲，然后给一个皮卡丘，它就知道你要，首先知道这是什么，同时它们在日本非常受欢迎，然后包括多姆态的一些对话，就是不仅是针对一张图片，你可是多张图片。

你可是视频，去做这样的一个chat，还有视频，左下角是视频的一个例子，然后同时刚刚说，它也是多姆态的输出，就是你的输出，可以是任意模态的，比如说这里，展示了一些生成的例子，右上角你可以任意的文生图。

你也可以图生图，因为你的Prom，可以是任意模态的，我可以两张图或者多张图，然后它比如说这里，老虎跟狮子的例子，我把它们作为Prom的时候，它就知道你要去做，它做了一个，虎头狮身的这么一个动物。

然后包括右下角是，文图的多姆态的上下文生成，刚刚说的左边熊猫，它是图文的多姆态理解，上下文理解，右下角是少样本的生成，这些都是新的一些能力，通过这样统一的训练出来的模型，它能够具有的。

下面有更多的一些例子，比如说这是一个聊天的例子，就是针对图片或者视频，去展开一二三四的一个聊天，然后它知道这个狐很危险，要带什么之类的，然后这是少样本的。

In Context Learning的一些能力，就是它能够在上下文中去推理，比如说两只狗两只羊驼，它知道这里你要数数，是五个气球，然后下面是一个比较有意思的，就是你给它几个手的图片。

然后比如说左边是二加三，五根手指，然后第二个例子是二加一，因为三根手指，那你给第三张图，它就知道它要输出二加二，因为你这是两根手指加两根手指，等于四根手指，然后可以看到，它其实是有一些这样推理的能力。

然后这是刚刚的对比的，其他模型的一些，更具有世界知识的一个能力，同时它也能去做很细节的描述，比如说这是美国黄石公园的喷泉，然后能给很多细节的信息，然后刚刚我们这个模型，很快会开源给大家能够用起来。

然后包括刚刚说的，EVA PENTA ZGP，这个EVA CLIP这些模型，都是现在在网上是开源的，大家可以在这个地址上去找到，我们所有的模型和代码，都会放在这个下面，然后也欢迎大家来交流。

然后最重要的是，我们非常感谢，我们团队的这些小伙伴们，然后以及支援其他团队，在Infra Data上面的支撑，因为我们开始，我们视觉团队的工作，也就差不多刚好一年，然后我们现在也在大力地招聘。

然后希望大家有兴趣的，可以来联系我们，然后这是我的邮箱，然后我们觉得，未来能做的事情还非常多，并不是没有什么东西可以做的，我觉得大家如果感兴趣的话，我们可以一起讨论，有什么更有意思的事情。

能够一起合作的，好的 然后谢谢大家，好的 然后谢谢辛龙的报告，然后我们 啊 已经有同学举手了，老师你好，我有个问题想请问一下，就是你那个多模大模行李，尤其是视觉对文本的这样一个任务。

就是OCR一般是一个比较难的问题吗，就比如说这个图片里有多少人头，然后这里的文字是什么，想不想你们对这个问题，有什么独特的理解或者说处理吗，对对 这个我觉得是个非常关键的问题。

就是我们说在训练多模大模行李的时候，大家都会用图文数据，就图像描述的数据，那我们一般说这个图像里，有一只狗是一种描述，那其实更抽象的描述就是文本，就是它其实是最抽象的符号。

你能够理解文本是能够带来很多新的应用的，这是为什么之前大家如果看到，像谷歌他们的那个这个PolyX，他们其实重点强调了，怎么样去用大规模的OCR的数据，然后所以这个，这是我们现在正在努力做的事情。

就是怎么去得到更Diverse，就之前OCR更，不是那么宽泛场景，也不是跟多模大模型结合的，那我们现在想怎么把它跟OCR模型，跟多模大模型更紧密的结合，其实非常关键的，最重要的就是怎么处理这部分的数据。

然后这个应该是能带来很多新的应用的形式，现在有没有做一些尝试，然后我们其实里面有一小部分的OCR的数据，然后但是这个数量其实远不够，就是这个还是有很大的提升的空间，谢谢，王老师你好。

我先就是我对你那个Pinter的工作非常感兴趣，然后我读你那个Pinter工作，我发现就是我对有一块不太了，就是不太理解，就是您为什么会把，在训练的时候会把两张图片concate起来。

然后作为模型的输入，就是说这样做是有什么用意，或者说是有什么好处吗，对这里就要回到我们的出发点，是做希望去探索上下文数学学习，那上下文你得有上下文，所以我们把两张图片concate起来，它就有上下文了。

所以把它们concate起来，做后面的mass image modeling，就自然就是一个上下文学习，然后我发现另外一个问题，就是我比较好奇的另外一个点是，您在做Pinter的时候。

确实是就是说做了很多试验任务，然后验证它的一个就是general的一个能力，但是我觉得有一点挺奇怪的是，我发现有一个主流的一个试验任务，object detection目标检测任务。

好像在您的paper里面没有被提到，然后是目前就是这样的一个模型，对这种body box它的一个，就是一个建模能力还是不太好，还能说有些，是这样我分两个，这个问题很关键，我分两个方面回答吧。

就一个是说body box这个任务，它本身其实是有问题的，我一直认为，就是这是我之前做实力分割过程中，一直的一个想法吧，就是因为以前大家用body box，是因为以前提传统特征。

很多这个用body box，那对于现在来说，首先你仍不用body box，然后你如果有了分割，其实自然而然就有了body box，这是一个问题，第二个是Penta怎么去做像body box。

这样的这个方式，一个是因为我们做的关键点检测，它其实一样的，你body box也是一种，也可以看成一种关键点检测，然后另一种是把它看成分割，就是你box你可以把它看成一种mask。

然后也能同样的去做这个事情，但是我们确实是没有在论文里，做这个事情，谢谢王老师，你好，我想问一个问题，就是一个，我做incognito learning。

所以我做incognito learning的时候，我想同时输入多张图片，那这和Sum everything里面，他们说想干的这个事情，我只给点，但是就说，你如果incognito learning。

我首先要先做segmentation给他的话，那这个segmentation从哪来，这个事情其实用起来，可能交互上要稍微麻烦一点，并且另外一方面的话，就是我输入的话，我把stack起来会好吗。

比方说stack起来有多个通道，然后我前面再去预处理，然后再分patch什么，像这样会好吗，对就是，首先刚刚说的，我们跟Sum解决的是不同的问题，他们其实是互补的，就是我们在公开的demo上。

这个包括最新的demo上，其实都做这个事情，就是这个Sum，你是给一个点，他能够给一个mask，那我们是给一个mask，能够给批量的mask，各种各样的mask，所以他们完全是能够接起来的。

就是一个出发点的不一样，然后您第二个问题是，我把我俩我俩互相stack起来，就比起concrete的话，stack起来的话，我完全的，对，但是我们还是希望，他在空间上。

能够做这样in context的推理，你把它concrete起来，他很难在空间上，去做这样的推理，好，那另一方面我还多问一句话，但是我同学们都不同意，在实话上我可能，认识过很多人都这么快，我都不快。

但是对于不像我来讲的话，往往不像我，我想做的事情，想做的很快，他可能比较不快，那您觉得就比方说，我现在大部分的经济专家，我上不了一些政府的话，我可能比较多，当初的有派出的牛，我把自己的工厂，发明了办法。

所以针对性的发展，有差异的空间吗，有啊，我觉得您刚刚，这是很好的一个proposal，对，这其实对我们一直在想象外国问题，有这一回事，好，那个新刚老师您好，就是您的那边panther的工作。

确实非常fancy，您提到这个unified learning的这种思路，也是给我很大的启发，但是提到您那边，非常优秀的panther这边工作，就不得不提他，这个同期工作的SAM，然后众所周知。

那SAM它也是一个，就是一个非常非常棒的，一个分割的模型，但是他的思路，就是说我通过给一个点的prompt，我可以去迭代的，迭代式的去生成一个，更好的mask，这个方法，它的一个非常棒的思路。

就在于它可以通过，这种迭代式的优化，可以产生一个，很强大的data engine，它可以快速的扩张自己的数据，同时能够得到一个，更好的训练，训练的一个效果，但是咱们这边panther，是不是没有这样的。

这种迭代式优化这种功能，之后您是否也会在后续，想要加入这种迭代式这种想法呢，对，我觉得，首先它能够交互，是它本身的一个很大的，最大的亮点嘛，也是就是被大家这个，这个最欣赏的一个点，然后对于我们来说。

我们不是那么直接的，就你肯定不能，这个把它完整的mask给画出来，还是得像刚刚说的结合起来，同时另一点是说，我们是希望它能够，在批量的上去做这样的交互的，这个自动分割，也是我们现在正在做的一个。

半自动分割的一个工作，然后希望未来能够有机会，这个放出来，好的谢谢王老师，还有其他人有问题吗，没有是吧，好然后我们再次感谢辛龙，我们接下来的报告，也是一个线上的报告，然后主要包含了两个speaker。

然后我简单先介绍一下，第一个是，Karsten Kais，是一位研究科学家，在NVIDIA的Toronto AI Lab，在加入NVIDIA之前，他在Deep Wave System上。

研究了深层生成模式，并业务组织了一家，典型的AI，一家研究生物的研究生物，使用深层生成模式，研究药物的发现，在转换到深层学习之前，Karsten在Massive Data，研究了数据知识。

在Max Planck Institute，研究了光的感觉，他在Max Planck Institute，研究了数据和数学的研究，研究了分子研究，目前Karsten的研究，集中在研发新的，生成学习方法。

预测浮动模式，以及在AI中，应用深层生成模式，来解决问题，例如数据视觉，图像和数据艺术，另外一个讲者是林欢，林欢也是NVIDIA，Toronto AI Lab的人工智能科学家，他也是多伦多的PhD。

他发表了有十多篇，顶讳的文章，并拥有多项专利，他的研究方向，也是主攻大规模图像，视频生成模型，和生成模型，在计算机视觉的应用，他有很多很知名的代表作了，包括PolyRCN++。

Data Science GAN，Editing GAN，以及近期的Video LDM，让我们欢迎来到Karsten和欢林的演讲，大家好，我是Karsten，我是NVIDIA的研究科学家。

和欢林的合作人，我们将谈论图像视频和3D内容，生成模型，这将探讨NVIDIA在Toronto AI Lab，在这一课上的不同工作，让我们开始吧，什么是图像模型，也叫做传输型模型，图像模型。

可能是由深入传输学习，开始在2021年，这份文章的名称，说明图像模型，可以在图像设置上，打击对战的网络，所以它们可以生成，非常高解析度的图像，非常高质量的图像，而且现在已经在其他应用上，使用过。

特别是这些最新的，图像生成系统，引起了很多热情，最近，我们也看到了，一个这样的图像，是由Edify生成的，是NVIDIA的，大型图像生成系统，我会在稍后，再说明更多的，所以，你能做什么，用图像生成系统。

你可以用它，来创造数据内容，这些是非常有用的，例如，你可以用图像生成，3D生成，视频生成，3D形状生成，三次元的视频生成，图像生成系统，已经用过，各种不同的工作，现在我想给大家，一些简单的介绍。

关于这些应用，我们在NVIDIA已经在工作，我先说一下Edify，Edify是一种图像图像生成系统，与教授Denoise的团队，在NVIDIA的项目中，Edify是一种，大型图像生成系统，涉及三种模式。

第一种是基础图像模式，生成64x64的图像，解析度，第二种是超高解析度模式，解析度模式，解析度模式，解析度模式，这是文字指导，我们使用了，T5和CLIP的文字复制，我们使用了，T5是一个语言模式。

CLIP是一个，画面语言的，对比模式，我们使用了这种语言复制，另外我们还有，一个选项的CLIP画面复制，我们可以给予，给产品，我们可以用来，用来做造型，我会再说明，整个模式有约，9。1亿个格数。

这是Edify，Edify，是什么特别的，比起其他图像模式，例如Dali 2，Stable Diffusion，Edify，使用了不同的专家，在不同阶段的，解析过程中，从左到右，我们会有一个阶段。

左边是声音，右边是图像，而Standard Diffusion，在这一阶段，我们会有一个阶段，在这一个阶段，我们会有一个阶段，在这个阶段，我们会有一个阶段，在这个阶段，我们会有一个阶段，在这个阶段。

我们会有一个阶段，在这个阶段，在这个阶段，我们会有一个阶段，在这个阶段，我们会有一个阶段，在这个阶段，在这个阶段，我们会有一个阶段，在这个阶段，我们会有一个阶段，在这个阶段，我们会有一个阶段。

在这个阶段，在这个阶段，我们会有一个阶段，在这个阶段，在这个阶段，在这个阶段，我们会有一个阶段，在这个阶段，我们会有一个阶段，在这个阶段，我们会有一个阶段，在这个阶段，在这个阶段，我们会有一个阶段。

在这个阶段，在这个阶段，我们会有一个阶段，在这个阶段，在这个阶段，在这个阶段，綜合資料的分析模式變化，作為實踐性的模組，因此是我開始使用自動模組生成新的模組，改造出成功的新模組。

我希望在講解後我們可以用多一點的訊息，來展示我們的實體形象，我們可以用許多不同的模組來做分析，實在非常適合在設計中我們可以進行實踐性的模組，我們可以用多一點的訊息來展示我們的實體形象。

這是我們試驗的結果，我們的數學結果確實證明了，我們能夠達到更高的表現，在實踐時我們實行的方式是，我們先訓練一個新的模組，然後我們分成不同的模組，然後再分成不同的世代，還有一個有趣的優點。

當我們將這個模組生成，並且使用專家的研究，我們需要做的整體的生成步驟，並沒有改變，相比於標準的流向模式，我們稱呼的每個單位模組，也沒有更多的規格，比如標準流向模式和單位模式，從這個角度來看。

影響並沒有變得更加貴，我們只需要稱呼不同的模組，但不僅如此，價錢也一樣，所以我們可以說，我們可以免費增加這個優勢，這並不讓我們更加慢，所以這很重要，好，我們來看看一些結果，這是一個對比。

穩定流向模組的DALI，或是接下來的文章，作為指引，一張4K DSLR的照片，一艘小船坐在漁船中，它穿著一件海外衣服和一條頭巾，它在讀書中，背後有幾片葉子，我會說我們的模型，拍攝得很不錯，我們有葉子。

頭巾，船，書，海獺，而穩定流向模組，它混合了船和書，這不是一條頭巾，這裡有些問題，在文章的理解中，穩定流向模組，還有DALI，它們的效果，是比較好的，但這不是很明顯的，所以我們相信，我們的模型。

做得不錯，我還提到，我們可以利用，拍攝模型的定義，來作為指引，或是指引，我們在做生產，在這裡，文章中有張照片，一群蟲子在世界上走，如果我們沒有，提供拍攝模型定義，或是指引，我們會產生這個。

一群蟲子在世界上走，但如果我們，提供一個，像這張藝術畫像的，標示模型，我們會產生，這樣的一個模型，我們還會有蟲子在世界上走，但現在，它們是標示模型的形狀，我們可以利用，這個標示模型定義。

來形容我們的模型，這裡有一個，很有趣的事情，Edify使用了，標示模型和T5文件輸入器，有人可能會問，為什麼我們使用它們，我們來看看這個例子，這是一張蟲子的照片，穿著黃色的衣服，穿著運動服和帽子。

它們是抱著一隻蟲子，如果我們只給它們，標示模型定義，這就是會發生的，我們有一個，蟲子的照片，蟲子的概念，是非常美麗的，但是它們沒有運動服，也沒有帽子，如果我們只給它們，T5文件輸入器。

這就是我們會得到的，我們可以看到，它們有一個東西，有一個帽子，這就是它們的樣子，T5文件輸入器，確實有更好的文字理解，比起標示模型，這已經被證明了，在文字輸入器中，現在如果我們給它們。

標示模型和T5文件輸入器，我們會得到這個圖像，我會說，這比起標示模型，更好，我們可以看到，我們現在有一個，非常特殊的蟲子臉色，所以蟲子的視覺概念，應該是，在這裡比這裡更好拍攝，所以標示模型。

基本上幫助了我們，在T5文件輸入器上，增加了一些視覺質素，這就是我們發現的，所以在這個意義上，標示模型和T5文件輸入器，是彼此相配的，我不認為這是完全驚訝的，因為標示模型，是用了標示語言。

而T5文件輸入器，是用了標示語言訓練，所以標示模型，看起來幫助了，視覺質素，其他我們還做了，在Edify上，我們有一個，用文字來塗漆的功能，為了解釋這個，我們來看看這個例子，這裡有一個，標示語言的圖案。

一個塗抹的塑膠布，和一個藍色的塑膠布，在一塊板子上對抗，現在我們想要，把塑膠布和塑膠布，放在特定的位置，我們可以這樣做，我們可以在圖像裡面，塗上一些位置，塗上塑膠布的位置，等等，這樣我們可以。

生成一個類似這個圖像的圖像，所以這個圖像，是怎麼做的呢？，我們在做的是，我們在改變了，圖像的對焦點，像是圖像的對焦點，是在文字的前面，我們改變了這個圖像，在這些位置，我們對焦點，更加強烈地對焦。

這裡我們對焦點，更加強烈地對焦，像是塑膠布等等，因為這個原因，模型就能夠，對焦點，和不同的概念，在這些位置，產生不同的概念，所以這裡有一個圖像，像是蝙蝠使用火球，對焦點，對 一樣的概念，好。

所以這是Edify，這是Text-to-Image Generation，但現在我們來轉到Text-to-3D Generation，這是Magic3D，高解像度Text-to-3D內容創作。

CVPR 2023的文件，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式。

和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的。

我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到。

這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，這個文件的文字，是由3D形式，和3D圖像，對焦的，我們可以看到，你的算力，目前我們是跟不上的，我們也知道。

OpenAI拿1000塊A100，訓練那個Chai GPT，都花了差不多，一個月到兩個月的時間，那想像一下，如果現在我們要，真有人說，我給你10億美金，你做這個事，你都不一定有，有這個資源。

去把整個的算力集群，以及它整個上游，分布系統給搭起來，當然另外一個點，就是數據本身，也是一個問題，當然我們會說，視覺數據也好，圖片數據也好，有非常多，但是實際上，它的冗餘度非常高，所以在這個情況之下。

我們是否真的，要像語言模型一樣，把整個互聯網，爬下來去做學習，這個我是持保留態度的，所以這塊更多的，可能我們需要，從一個叫做passive learning，就是你完全是數據過來，為數據。

就像樂坤今天講的，就是說我可能會有一個，world model，然後這個world model，它會主動的去和這個世界，做一些交互，然後通過一些prediction，action之間的這種。

joint modeling，它可能能夠，稍微減緩一下，對於這個信息密度也好，對於這個模型學習的，一個建模，所以這是一個物理的分析，我自己個人還是說，繼續去贊同，剛剛高威博士的一個想法。

就是說一個unified模型，所以因為我們要去，學習這個東西，需要一個unified task，這個task本身，它可能是一個很抽象的，但是在建模模型本身，我們現在一般說，比如transformer。

它最大的一個能力，是attention，attention目前僅僅是在，embedding或者feature，為主做attention，那我們能否把它擴展一下，我能否在task。

這個為主做attention，就是你最終實現的是一個，所謂的叫transformer，transformers，然後你在task，driven這一層，你其實可以去針對，你的這個模型的結構，做一些這種。

你可以理解為，就像那個Google有一個叫，這個dual，是叫dual path吧，還是那個，忘了這個什麼網路了，就是它能夠做一些，這個類似的一個，specification，或者重新做一個。

reconfiguration，就像我希望我們將來的，網路設計是否能夠做一個，task driven的一個，architecture reconfiguration，in real time。

然後這樣的話，你在influence的時候，是走一條這種，所謂的sparse的路徑，它的這個推理的成本，會比較低，但是你在訓練的時候，它的多任務之間，多模態之間，其實它很多，它的這個。

這些neurons是可以share的，就像打一個很簡單的比方，就說我們在學習，這個伽利略這個概念的時候，它是有兩個身份，物理學家跟一個，歷史上科學革命的，一個奠基人之一，那我如果現在。

我先給一個task說，我今天考歷史，然後我問伽利略，我就給這樣一個prompt，大家腦海裡面，可能會反映出，他在這個歷史上，怎麼樣被這個宗教給迫害，怎麼樣這個堅持真理，然後怎麼怎麼的，那一系列東西。

如果我現在，我先給一個prompt說，我的物理，我再給伽利略，大家腦海裡面，反映出來是慣性定律，對吧，那個力學定律，所以為什麼，人的大腦可以在這麼短的時間，只需要一個簡單的prompt，就可以。

你可以就好像，把你的整個的knowledge graph，做了一個，一個重新的一個調度，所以這一塊，是我覺得比較，非常神奇，但是我目前並找不到，任何解決辦法的一個，一個方向，所以將來可能。

在視覺大模型這一塊，當我們有了一個，比較統一的unify一個模型，我們有了這樣一個類似，比如說，有一定的吸收性，有一定的這個，task driven，attention的機制的模型的時候。

我們其實可以去，學到這樣一個通用的一個模型，然後通用模型，跟小模型之間，其實很簡單就是，你的通用模型，會在雲端隨著你數據的不斷增強，你的能力通用性不斷增強，然後同時你的這個，當然這個前提就是非遺忘性。

可能後面我們會討論這個話題，那我非遺忘性的，學習了一個大模型之後，我需要把它，通流的方式，fine tune的方式，把這個知識給傳遞到一個，小的模型上面去，或者專用模型上面去，那這個地方其實像我們人類。

有一個互聯網，這麼一個公用的一個老師，每個人都從這個老師裡面，根據自己的需求，根據自己的task，去學到一個自己的小模型，就是我們所謂的，我們每一個人的一個知識的，一個work model。

所以這個是我可能認為，就是說最後你這個，通用的task，對應的這個通用結構，和這些小模型的結構之間的，一個關係，它是一個在時間維度，跟空間維度，不到往前演進，但同時之間有一定的兼容性。

跟這個所謂的非遺忘性，連續性的，這麼一個過程，講得可能有點抽象了，沒有，一會兒下個問題，可以多說一點，然後行，謝謝高博士，好，我就補充兩個小點吧，我覺得就是如果，如何讓通用視覺大模型出圈的。

可能或者是能做的方向，因為我不是特別懂，所以可能說的有點不大對，我覺得可能有兩個方向值得做，第一個方式，如何拓展到一個open world，就是說我們之前做Semantic，至少我理解Semantic。

可能就是你可能有一些，比較固定的一些category，可能Sem它現在能夠做到，就是open vocabulary，其實我想說的事情是，我們如何能夠說，並不像之前一樣做task。

我們只做這種固定類別的task，而是說我們這個世界當中，有很多很多種不同的物體類，我們可不可以所有的類別，都做一種segmentation，或者是detection，或者是whatever的這種。

2D的視覺，然後第二個事情，我可能覺得，我自己特別感興趣的，或者我覺得需要，是一個long tail distribution的問題，就是我們在現實生活當中，很多很多東西。

其實都是一個long tail，不管是segmentation也好，還是detection，甚至是像我們做3D generation，我們也有long tail distribution。

比如說像北美這邊，他們特別懂的是，一輛車上面搭著一個船，因為大家都喜歡開車，搭船出去玩，或者一輛汽車上面，有一個自行車，這也是一個很奇怪的物體，它就是一些，很奇怪的long tail的區別。

那麼我們該如何去adapt，我們的生成模型，或者是我們的這種檢測模型，到一些long tail distribution，我覺得是一個，非常可以值得探索的一些問題，好好謝謝謝謝高博士。

然後剛才夏威也提到了，這個遺忘性，然後我們下個問題其實就是，就是這麼一個問題，就是說面向這種，模型演化的這種連續學習，就是近幾年也開始受到關注，科技部也立了相關的項目，來支持這個研究方向。

傳統的連續學習任務，一般會讓模型從零開始，不斷積累知識，但是在這種，有了這種視覺，或多模態大模型之後，模型本身它已經囊括了，互聯網上，非常非常多的知識了，那在這個背景下，以大模型為基礎的模型演化。

有哪些值得研究的方向，我問這個問題，是我本人是有些私心的，因為本身在21年，科技部一共立了，四個連續學習的項目，然後我本人也正好，參與了其中一個，也現在也正在做這方面，相關研究，也想聽聽各位專家的建議。

那夏威那不從你開始，OK行，對這個問題，我可能是有一個，比較深刻的一個體會，可能大家不做產品，很多時候意識不到這個問題，我給大家舉一個簡單例子，就比如在AWS，它有我之前前東家。

它有一個應用叫做Prime Photos，它大概有幾億用戶，然後上面大概有，300億到500億張照片，現在可能不止了，然後每次我們要做一個，比如視覺的一個分類模型，然後給它打標籤，然後比如說我好不容易。

搞了一個模型V3，然後說我跟產品人說，我這個提高到85%性能，然後他們第一反應就是，我上哪去搞這麼多機器，把這300億張照片，重新跑一遍，對吧，這個地方其實就存在一個點，就是說你的模型出來之後。

它很多時候，在不同的版本之間，就在時空的這個維度裡面，它很多時候它不是連續的，這個不連續主要體現在，幾個維度，一個就是說，它的最終的針對某一個任務，它的結果的不連續，比如說如果是一個分類任務。

假設我是一個特斯拉的，一個自動檢測的一個算法，我今天model 1，我把這個人的識別準確度是90%，然後明天我降到85%，我把stop sign識別成一個什麼，停牌識別成走牌，對吧，那車禍的都有多少。

所以這個地方就很多時候，這個性能回測，就是我們希望模型，在更新的過程之中，能夠保證它已有的，某一個golden的一個子集的，性能的同時，它的收益是淨收益，所以針對這個問題，我們在還有另外一個點。

就是當你在embedding這個維度，你的feature space，我怎麼樣保證，我兩個classifier學出來的feature，是它可能是不是不完全一致，它的feature的一個space。

不完全align，但是它的feature之間是可以，你可以算內積的，可以算距離的，那這樣的話它一定程度上，它是一個compatible的，它的一個好處，比如說現在假設有這310張照片。

然後我已經提好了310個vector，然後我現在新版本上線，我其實只需要針對新的，這一部分的照片提feature，然後這新版本的，比如v2的feature，它是可以跟v1的feature，直接做一個。

比如說做一個內積，這樣的話我就省掉了，我每一次新模型上線，要把我的舊模型，這300萬張照片，全部都重新提一遍的，這樣一個痛苦，從具體的執行層面，可能要花幾個月的時間，去把它做一個。

所謂的叫backfill，這樣所謂的，整個的重新跑一遍，然後從成本的角度，這一跑一遍，這是500萬到1000萬美元的成本，所以我們之前有一個小的案例，就是說我一個實習生，花了20萬美元的。

AWS的計算資源，就跑這個算法，然後被我VP發信息說，你怎麼花了這麼多錢，我說這玩意跑完，可以幫你一年省1000萬，你跑不跑，所以他們後來說，那跑吧20萬可以花，所以就是這一個很。

這一個商業界的一個痛點，當你的問題，到了這麼一個程度之後，我們其實就需要去解決，這個模型的一個，連續性的問題，不管是從這個，所謂的embedding的層面。

還是從classification的score，或者detection的binding box，的一個score的層面，其實整體上就是有兩個維度，一個就是時空的維度，就是我在代際之間，我的模型。

當我的weight更新，當我的數據更新，乃至我的architecture更新的時候，我也能保證它的這個embedding，一定程度上是兼容的，這樣的話，它其實學到的knowledge。

也是可以去被繼承的，然後另外一個點，在空間的維度，就很多時候大家現在，整天談所謂的雲端協同，好吧，比如說我雲端有一個大模型，我要做一個模型壓縮，把它壓到一個小模型裡面去，這個時候我小模型。

最後提出來的feature，跟我雲端大模型是不兼容的，這個時候，現在我還是要把，我雲端的處理的一些RGB的，視頻也好，圖片也好，再傳一份到雲端去，然後這個時候，當雲端跟中端結果不一致。

你還得要搞一個額外的邏輯，去把它們合併到一起，所以這個就很麻煩，就很難得到一個，比如說你的隱私性，和你的效率的一個，綜合的保證，所以這塊我雖然就是說，一直在創業圈跟工業界，但是我們在AWS。

也針對這個痛點，也發了幾篇論文，2020年的時候，我們大概發了一篇，應該是這個領域第一篇論文，就叫Towards CPR的，2020年的一個oral。

Towards Compatible Learning，它其實就是想解決一個，就是我模型更新之後，我的embedding，怎麼樣跟我舊的embedding，是保證一個兼容，這樣我不需要做一個。

冷的冷重啟，然後後來在2011年的時候，我們又發了一篇，就是把這個問題引申到，叫classification，就叫regression free的，一個model update。

in classification，然後再把它引到空間的維度，當我從雲端，把一個模型通過NAS，通過這個壓縮，給它壓縮到終端的時候，我也希望能夠實現一個，跨平台的一個，異構的一個搜索兼容。

那個paper應該叫，Heterogeneous Visual Search，CPR的2021，然後2022年又再考慮，因為你雲端跟終端，是一個空間的維度，那不同的終端之間，比如說特斯拉的車。

跟理想的車之間，這是兩個不同的終端，他們之間的模型，我一套模型部署上去，能否實現一個兼容，所以就有一個，叫cross platform的一個，regression free的一個NAS，所以那個是。

ECCP2022的一個paper，然後再後來就說，那我們把大模型，除了我做這種，weight sharing之外，我需要做這個，結構的架構的一個sharing，所以我在做這個。

knowledge list deal，和NAS的時候，我需要去繼承，它的一些特性，所以應該是我目前，職業生涯最後一篇paper，就是這個archive的paper，就做這個所謂的。

knowledge list deal層面，怎麼樣去解決，這樣的一個連續性的問題，當然這都是在，大模型出來之前，所以大模型出來之後，一個好處，大模型它本身就把，具有一個很強大的knowledge。

所以它很多時候，它的這個連續性和繼承性，會比以前的模型，要好很多很多，但是仍然會遇到這樣的問題，所以這也是大模型，目前在應用的，一個很大的一個局限，就是它不可靠，它就是一個，大家經常評論說。

所謂的一本正經，胡說八道，那我怎麼樣控制，你這個胡說八道的，一個頻率或者範圍，然後我讓你一直一本正經，不要胡說八道，所以這個是我們，我覺得是一個，也很大的一個趨勢，當然我們之前的一些研究。

主要是一個拋磚引玉，但現在大模型時代，我就沒有太多的話語權，我想把話筒，交給其他的提問老師，好 要不下一位誰先，星剛，我覺得星龍，可能比我更清楚一些，不過我可以談點淺見，因為其實這個。

也不是我的主要研究方向，就談一點點想法，首先關於剛剛提到的，一個general embedding的問題，我覺得這個絕對是，通用性一個重要的問題，但其實我在，我的concern是。

一個通用的embedding，要多大程度上，依賴於數據的多樣性，那當我見到的新的場景，越來越多的時候，那之前的embedding space，是否需要微調。

關於learning without forgetting，這個問題可能最初的方法，就是微調模型，然後在微調的過程中，加上模型之前的記憶，之前的輸出來保證，它不遺忘之前的東西，那我覺得可能如何微調。

或者如何重新設計模型，這裡有很多可以探索的空間，比如說你是不是可以，除了微調整個模型，你去記憶一個，模型參數的變化方式，這種變化可能，你只需要一些，比較low rank的space。

就可以在不同的場景下，讓模型參數，按照不同的方式去調整，那另一點我的想法是，我覺得可能這個問題，本質上還是想在，有限的模型容器大小內，塞更多的信息，那當然這裡有很多，可以探索的。

但是可能最終upper bound，還是限制在了，模型本身的大小上，比如說之前CNN時代，大家已經開始研究，learning without forgetting了，但是再怎麼研究，你也不可能比得過。

transformer這種更新，或者說算力和模型大小的更新，帶來的影響要大，所以可能更重要的，還是算力和模型大小的迭代，好 謝謝鑫哥，那信龍，我也沒做過連續學習，我就發表一點個人的看法。

然後就剛剛夏威老師說，那個400億圖片重新遜，我倒是有想法，就是現在有一些做數據徵流，就是你可以把400億數據，徵到1萬張裡面，然後去遜，想要遜到類似的效果，這樣可能是一個可能的路徑。

然後說到剛剛那個大模型的話，因為我們想要進一步的，去優化大模型裡面，這些知識啊 信息啊，本身是很困難的，所以我覺得有兩個方向吧，一個是模塊化的大模型，就當這個大模型，它是模塊化之後，你去更新其中的。

去做一些連續學習，可能會更容易，然後另一個是去給它打補丁，就是做adapter，也是現在其實很多人，重點關注的一個地方，可以以比較小的一個cost，去實現一些新的能力，然後應該也屬於連續學習的一種。

OK 然後我先追問一個問題，就是說在大模型，就是說剛才這個興剛也提到，就是說我們有足夠大，我們有足夠大，可以容納特別多的知識的時候，是不是就是說，我們的東西，前面的都已經學好了，然後當我每次。

想學新的東西的時候，我只需要讓它找一個路徑，通向這個我所要的，這個東西是不是就可以了，而不需要再去很大程度上，對這個特徵層面上進去進行修正，對 我這個也是剛才夏威夷老師，提到的一個概念吧，就是路徑選取。

對 我覺得這個是有道理的，並且是一個挺有意思的研究方向，那現在的大模型裡面，好像並沒有做，並沒有太多這樣的設計，比如說Transformer，它其實還是比較密集的連接的，有可能它這個連接裡面。

已經建模了一些習俗性了，它對某些地方的Attention更強，那其他地方你可以不要，所以這裡肯定是有更多優化空間的，對，好的 那這個高博士，我其實就想補充一小點，就是其實像那個你說的，這個增量學。

其實大家已經在，比如說像是Lola，就是Lola其實是一個，最近大家用的比較多的，一個對Diffusion Model去Funtune，但是Lola的一個問題是，它這種Design非常Specific。

for the Stable Diffusion，就是這一系列基於CNN的，這一套的這種Funtune，但是如何說，你說像我如何去Funtune，一個Diffusion Model。

現在大家可能做的就是，我拿它Pretrained Waves，然後再微調一下，但是有沒有像Lola這種，比較Elegant這種方式，去Funtune Transformer也好。

這也是可能需要大家去探索的，好 謝謝，然後最後一個問題，就是比較輕鬆的一個，但是我相信可能是，在座的研究人員，特別是高校的老師，以及現在正在讀PhD的學生，最關心的一個問題，就是現在就是我們這個大模型。

對計算資源的消耗也越來越大，其實大多數實驗室和高校，計算資源其實已經沒有辦法，支撐這個大模型的研究，你比如像我，我覺得我實驗室能買到，就是我Scope到100張卡，我就已經很多了，再多不是錢的問題。

我都沒地方放，就會有這樣的問題，就是說那麼就是說，在這個環境下，就剛剛辛龍也講到，就是說剛剛就是說，比如說我們的過去的，傳統的比如說Detection也好，Segmentation已經是舊時代了。

那麼在新時代，就是說未來幾年吧，就是說學術界特別是，大部分高校的老師，大部分高校老師還要吃飯的，學生也是要畢業的，他們的研究重心應該是什麼，對 學術界，我不是學術界，那我先說一下。

我覺得可能我想到有三點吧，首先我覺得最重要的一點可能是，作為學術研究來說，你的研究，你的方法證明要work，你不一定需要在大模型上證明work，因為一個work的方法，在大部分情況下。

它是general的，它是通用的，你可能在你能負擔得起的，計算資源上，也可以證明你這個東西，它是work的，其實很多時候，學術界提出的方法，符合這樣的特性，那你就可以在有限的資源上去驗證，第二點是。

有很多任務，也並不是依賴大模型的，並不是所有的問題，都需要你從頭開始訓練一個模型，比如說drag gun，或者說你把drag gun的這種功能，想去拓展到其他的場景上，你可能並不需要從頭訓練一個模型。

那麼其實類似的問題，有很多可以去探索的，然後最後就是，可能將來校企合作，也越來越多的，會成為一種，大家做研究的方式，謝謝，夏威，你也是工業界的，然後有沒有對學術界的一些指導建議，其實沒有指導談不上。

只是讓我回憶起，這個很古老的時候，就上一次我對算力有，有強大的需求，而得不到滿足的時候，還是我當時在矽谷創業的時候，我們當時是靠著老黃，給我們贊助的幾塊免費的卡，開始創業，自己鋸管子做服務器。

所以那個時候因為整，所以有時候很，一方面很羨慕現在年輕人，但一方面覺得，現在年輕人做研究確實不容易，所以我剛才其實很贊同，潘老師的幾個點，就是研究不一定，需要那麼大的算力，當然有了算力。

你的研究會更加的簡單，所以怎麼樣解決這個問題，我的想法或者我的建議就是說，是開源和節流，節流的話其實就是說，我怎麼樣從系統的維度，怎麼樣從算法的維度，來減少我的一些模型也好，我的一些算法也好。

對於算力的一個需求，這裡面比如說，你像分布式訓練也好，分布式推理也好，比如像deep speed，這些開源的一些框架，其實很offload，zero這些東西，它很大程度上可以，減緩我們在一定的。

比如說顯存也好，算力也好，它的限制條件之下，我能訓練的最大規模的，模型的一個上限，所以這些技術，我是鼓勵大家，也去積極的，不管去嘗試也好，還是去貢獻也好，就把它集成到，你的研究裡面來，這是第一個點。

再往上走一層，在算法層，很多時候有一些運算，它都是冗餘的，我們再分析一下，比如你做一個profiling之後，這個CNN也好，transform也好，是不是有很多算子也好，很多kernel也好。

它本質上是一個冗餘計算，我能否通過一定的方式，去減少這種冗餘計算，比如說我們大概，2021年發過一篇paper，叫做stochastic gradient propagation。

當你在做back propagation的時候，很多時候你的weight也好，你的loss也好，不是所有的信息，都要全盤的往前做一個傳輸的，你只需要把那些，有點sparsity的思想。

你把那些最重要的影響，對後面的，你做一個差分的話，對它的那個差分，或者它的gradient影響，最大的那部分，把它傳過去，或者在分布式的時候，把它做一個reduce，做一個集成，其實這樣的話。

會一定程度上減少你整個模型，對於內存的一個，或是顯存的一個使用，那個當然是一個非常小的一個點，對這是在節流，開源這一塊，當然就是說，因為我今天可能會打個小廣告，大家可能都是用NV，這是我第二次創業。

做芯片，當然也希望大家以後，多支持一下國產的一個，芯片的這麼一個生態，然後我們目前其實已經就是說，能夠去支持和兼容，大部分的CUDA的一些算法，包括大模型，LAMA SD什麼Chadge。

我們都已經都適配成功了，所以這一塊，但是也迫切的需要各位的，這個developer，能夠像10年前，大家去幫NV debug的一個心態，來幫助國產的這個生態，去做一些適用，提供一些反饋。

幫助我們共同成長，對所以如果有這方面的，這個想法的，也歡迎和我線下聯繫，我也免費送你們幾張卡，你幫我們折騰折騰好吧，對對對也也也也希望這個，對剛才夏威在下面聯繫過，跟我們聊過，就是說也希望你們能對我們。

實驗室對吧，再多贊助一些，我們幫你們多開發一些東西，對吧，幫你們做調試，然後最後一個高博士，高博士應該是我們中間，最不缺卡的一個了，请您从你的角度指导一下我们缺卡的人应该做什么。，我们也缺卡。。

我认为学术圈该如何运作，因为我本身是一个学术圈的人。，我们需要以战略性思考来解决一些大问题， 比如我自己。，我们需要想办法解决一些可以解决的问题，，比如我们需要解决什么是一个比较有效的计算机，。

什么是一个比较好的可微讯，什么是一个比较好的三维表。，然后当我有了一些三维表时，我怎么去发展一些比较好的三维训练模式。，现在在3D里面一个很大的问题是大家都不知道什么模型是对的。，这个跟2D不一样。

跟语言不一样。，语言自从Transformer出现后，所有人都流入了Transformer。，但是现在Vision里面没有一个大家都觉得模型建设是有意义的。，即使是Vision里面。

2D里面可能比较难去做，，但是3D里面即使是ShapeNet这样一个非常小的计算机，，大家也没有探索得非常完美。，所以我想说的是，我们得想办法把一个大问题，分解成很多很多我们可以解决的小问题。。

然后这些小问题其实是学术学院或者是researcher，或者是这些资源不大够的，，或者是有限的资源是可以去想办法去解决，，因为他们都是一些small-scale problem，。

他们都是一些很小的task，很小的problem。，这个是第一点。，然后第二点的话，其实我很同意潘老师说的，，就是这个世界上有很多，它有大模型，它有大模型的问题，，但是有很多问题是不用大模型，。

我们也需要去探索的。，潘老师举例子像Jargon，，但我还想举一个例子是说，，比如说我们如果想做physics，，我们想learning 3D with physics，，因为我们生成一个三维世界，。

我们不可能生成一个静态的世界，，我们需要让这个人动起来，，我们需要让这些object动起来，，但是怎么去表示physics，怎么去学习physics？，我如果说现在即便大家坐的是学术学院的位置，。

即便说我就是给一串视频，，然后我想立刻创造出来这段视频里面，，physics长什么样，就比如说大家看我在这给talk，，那么我是一个3D的，我的嘴巴在张开，我的手在动，。

但是怎么样去look and solve一个会动的人，，一个会说话的人，这本身它就是一个small problem，，但是这种层面是算法上大家都没有一个比较完善的算法，。

它不像large language model，，算法都在那，除了对齐的算法可能不知道，，但是你train large language model，，你如果是train transformer，。

这个self-attention，这个东西都在那，，大家develop好，你就需要把算力堆上去，，你需要把data堆上去，你就可以做了，，其实很多问题你是连算法你都不清楚，。

那么这个时候你就应该去在一个small scale，，你甚至应该把问题的不断的简化，，简化成一个你可以解决的问题，，你可以在一个很simplified problem里面去把算法先develop出来，。

这样子的话你就可以帮助他们去做之后的大模型的发展，，你可以，就是另外一种角度去contribute into the large model，，这是第二点。

另外一点的话其实可能比较偏engineering，，是对学生的一个小的建议，，就像夏威老师说，，其实coding或者是efficiency是非常非常重要的，，这个事情其实我希望是从一个，。

从做小事件开始就需要去培养的事情，，就是我们需要非常非常注重你的coding的efficiency，，当然这个只是对一些学生的建议，，可能researcher可能并不需要，。

老师可能并不需要care这个事，，因为在我们真的要去train大模型的时候，，我们可以很明显的知道efficiency是非常非常重要，，我们的资源是有的，，不管，即便是openai，。

他们的算力对他们而言也都是一个有限的算力，，而他们自己内部的codebase，，虽然我没有看过他的code，，但我能猜出来他们应该是非常非常highly optimized code。

for training GPT或GPT4什么的，，所以其实这意味着如果是我们如果自己要develop，一些自己开源的code，，或者是我们自己做自己的实验，。

也都需要很注重这个coding efficiency，，这样子的话，一个事情是你的资源有限，，那么你自己本身你的实验的这种迭代速度也会变得更快，，如果因为很多时候你把coding你improve一下，。

你的速度可能能快个几倍，，这都是很正常的一个提升速度的一个东西，，所以我大概就想说这么三个吧，，然后觉得学术学还是有很多可以值得探索的问题的。，好，谢谢高博士，，因为时间原因，，就是我们刚才的QA环节。

，其实大家已经问了很多问题了，，我们这里就不再安排这个QA了，，然后我们所有的speaker，，就是首先从这个各自研究的领域，，跟我们分享了很多就是这种很前沿的一些想法，，包括很前沿的一些自己的工作，。

在刚才的讨论中也给我们分享了很多很前沿的一些想法，，我相信也给大家可能也带来了很多未来想做什么，，应该有了一些新的思路，，好，我们最后再感谢我们所有的speaker。，好。

我们这次视觉与多模态大模型的论坛到此结束，，好，谢谢各位。，谢谢大家收看，咱们下期再见。